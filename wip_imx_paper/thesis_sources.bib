@misc{10LeadingLanguage2021,
  title = {10 {{Leading Language Models For NLP In}} 2021},
  year = {2021},
  month = may,
  journal = {TOPBOTS},
  urldate = {2022-01-24},
  abstract = {The introduction of transfer learning and pretrained language models in natural language processing (NLP) pushed forward the limits of language understanding and generation. Transfer learning and applying transformers to different downstream NLP tasks have become the main trend of the latest research advances. At the same time, there is a controversy in the NLP community [\ldots ]},
  howpublished = {https://www.topbots.com/leading-nlp-language-models-2020/},
  langid = {american},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\DL728JD2\\leading-nlp-language-models-2020.html}
}

@misc{10LeadingLanguage2021a,
  title = {10 {{Leading Language Models For NLP In}} 2021},
  year = {2021},
  month = may,
  journal = {TOPBOTS},
  urldate = {2022-01-25},
  abstract = {The introduction of transfer learning and pretrained language models in natural language processing (NLP) pushed forward the limits of language understanding and generation. Transfer learning and applying transformers to different downstream NLP tasks have become the main trend of the latest research advances. At the same time, there is a controversy in the NLP community [\ldots ]},
  howpublished = {https://www.topbots.com/leading-nlp-language-models-2020/},
  langid = {american}
}

@misc{ActiverVoiceOverEntrainer2023,
  title = {{Activer VoiceOver et s'entra\^iner \`a utiliser les gestes sur l'iPhone}},
  year = {2023},
  journal = {Apple Support},
  urldate = {2023-04-04},
  abstract = {Si vous ne pouvez pas voir l'\'ecran de l'iPhone, activez VoiceOver, un lecteur d'\'ecran contr\^ol\'e par des gestes.},
  howpublished = {https://support.apple.com/fr-fr/guide/iphone/iph3e2e415f/ios},
  langid = {french},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\9F7ZQ3IB\\ios.html}
}

@misc{AIAccessibilityGrants,
  title = {{{AI}} for {{Accessibility Grants}} - {{Microsoft AI}}},
  journal = {Microsoft},
  urldate = {2022-05-20},
  abstract = {Discover Microsoft AI for Accessibility grants. Our AI grants provide funding for those looking to improve, empower and help solve global needs.},
  howpublished = {https://www.microsoft.com/en-us/ai/ai-for-accessibility-grants},
  langid = {american}
}

@article{aishwaryaguptaIntroductionAIChatbots2020,
  title = {Introduction to {{AI Chatbots}}},
  author = {{Aishwarya Gupta} and {Dayananda Sagar College of Engineering}},
  year = {2020},
  month = jul,
  journal = {International Journal of Engineering Research and},
  volume = {V9},
  number = {07},
  pages = {IJERTV9IS070143},
  issn = {2278-0181},
  doi = {10.17577/IJERTV9IS070143},
  urldate = {2022-02-07},
  abstract = {The modern era of technology has a tremendous impact on the society. With the creation of the ultimate virtual assistants, chatbots have become a popular entity in the conversational services. Chatbots are software programs that use natural language understanding and processing. Chatbots are not just restricted to help the user to complete his tasks such as booking a movie ticket or finding the nearest restaurant, but they also provide a source of entertainment, play a major role in home automation projects, give business strategy tips and help in other ways. In this paper, we will provide an insight into what a chatbot is and the types of chatbots. We also propose a classification based on the current market trends, ease of usability and requirements.},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\YG9G8CQH\\Aishwarya Gupta et Dayananda Sagar College of Engineering - 2020 - Introduction to AI Chatbots.pdf}
}

@article{aishwaryaguptaIntroductionAIChatbots2020a,
  title = {Introduction to {{AI Chatbots}}},
  author = {{Aishwarya Gupta} and {Dayananda Sagar College of Engineering}},
  year = {2020},
  month = jul,
  journal = {International Journal of Engineering Research and},
  volume = {V9},
  number = {07},
  pages = {IJERTV9IS070143},
  issn = {2278-0181},
  doi = {10.17577/IJERTV9IS070143},
  urldate = {2022-02-10},
  abstract = {The modern era of technology has a tremendous impact on the society. With the creation of the ultimate virtual assistants, chatbots have become a popular entity in the conversational services. Chatbots are software programs that use natural language understanding and processing. Chatbots are not just restricted to help the user to complete his tasks such as booking a movie ticket or finding the nearest restaurant, but they also provide a source of entertainment, play a major role in home automation projects, give business strategy tips and help in other ways. In this paper, we will provide an insight into what a chatbot is and the types of chatbots. We also propose a classification based on the current market trends, ease of usability and requirements.},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\39TB74LY\\Aishwarya Gupta et Dayananda Sagar College of Engineering - 2020 - Introduction to AI Chatbots.pdf}
}

@misc{alammarIllustratedTransformer,
  title = {The {{Illustrated Transformer}}},
  author = {Alammar, Jay},
  urldate = {2022-11-25},
  abstract = {Discussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments) Translations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Japanese, Korean, Persian, Russian, Spanish, Vietnamese Watch: MIT's Deep Learning State of the Art lecture referencing this post In the previous post, we looked at Attention \textendash{} a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer \textendash{} a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud's recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let's try to break the model apart and look at how it functions. The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard's NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter. 2020 Update: I've created a ``Narrated Transformer'' video which is a gentler approach to the topic: A High-Level Look Let's begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.},
  howpublished = {http://jalammar.github.io/illustrated-transformer/},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\EQZIL5QE\\illustrated-transformer.html}
}

@misc{AllenNLPELMoAllen,
  title = {{{AllenNLP}} - {{ELMo}} \textemdash{} {{Allen Institute}} for {{AI}}},
  urldate = {2022-09-02},
  abstract = {ELMo is a deep contextualized word representation that models complex word use.},
  howpublished = {https://allenai.org/allennlp/software/elmo},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\EFIGEQW5\\elmo.html}
}

@misc{alliekmillerIntroNaturalLanguage2021,
  title = {Intro to {{Natural Language Processing}} ({{NLP}})},
  author = {{Allie K Miller}},
  year = {2021},
  urldate = {2021-09-06},
  abstract = {Welcome to your daily dose of AI education. In today's episode, I break down 3 fundamental machine learning categories involving natural language \textemdash{} NLP, NLG, \& NLU (the same technology that powers smart assistants like Siri, Alexa, and Google Assistant).  This content is beginner-friendly and covers exciting applications of how this tech can shape communication in the digital world.  Have any questions or suggestions for future videos? Let me know down below. Tags: artificial intelligence, machine learning, AI, ML, AI/ML, natural language processing, NLP, natural language understanding, NLU, natural language generation, NLG, named entity recognition, NER, part of speech tagging, lemmatization, sentiment analysis, relation extraction, semantic parsing, summarization, email support, augmented creative writing, syntax, semantics, pragmatics, technology}
}

@misc{amamouInvoiceAutolabelingUsing2022,
  title = {Invoice {{Auto-labeling}} Using {{LayoutLM}}},
  author = {Amamou, Walid},
  year = {2022},
  month = oct,
  journal = {MLearning.ai},
  urldate = {2023-02-06},
  abstract = {For more efficient invoice annotation},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\4A52Q3EA\\invoice-auto-labeling-using-layoutlm-8b0e5cffc837.html}
}

@misc{androutsopoulosNaturalLanguageInteraction2005,
  title = {Natural {{Language Interaction}}},
  author = {Androutsopoulos, Ion and Aretoulaki, Maria},
  year = {2005},
  month = jan,
  journal = {The Oxford Handbook of Computational Linguistics},
  doi = {10.1093/oxfordhb/9780199276349.013.0035},
  urldate = {2022-02-10},
  abstract = {"Natural Language Interaction" published on  by Oxford University Press.},
  howpublished = {https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199276349.001.0001/oxfordhb-9780199276349-e-35},
  isbn = {9780199276349},
  langid = {english}
}

@article{androutsopoulosNaturalLanguageInterfaces1995,
  title = {Natural {{Language Interfaces}} to {{Databases}} - {{An Introduction}}},
  author = {Androutsopoulos, I. and Ritchie, G. D. and Thanisch, P.},
  year = {1995},
  month = mar,
  journal = {arXiv:cmp-lg/9503016},
  eprint = {cmp-lg/9503016},
  urldate = {2022-02-10},
  abstract = {This paper is an introduction to natural language interfaces to databases (NLIDBs). A brief overview of the history of NLIDBs is first given. Some advantages and disadvantages of NLIDBs are then discussed, comparing NLIDBs to formal query languages, form-based interfaces, and graphical interfaces. An introduction to some of the linguistic problems NLIDBs have to confront follows, for the benefit of readers less familiar with computational linguistics. The discussion then moves on to NLIDB architectures, portability issues, restricted natural language input systems (including menu-based NLIDBs), and NLIDBs with reasoning capabilities. Some less explored areas of NLIDB research are then presented, namely database updates, meta-knowledge questions, temporal questions, and multi-modal NLIDBs. The paper ends with reflections on the current state of the art.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: 50 pages, uuencoded compressed tar file, containing LaTeX code and .eps figures. Uses a4wide.sty. (No changes in the text. Fixed problem with epsf macro. Use the epsf.sty included in the tar file, not the epsf.sty of the cmp-lg server.)},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\P9EJ7FRN\\Androutsopoulos et al. - 1995 - Natural Language Interfaces to Databases - An Intr.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\MHDFHXJF\\9503016.html}
}

@misc{ATISDatasetClean,
  title = {{{ATIS Dataset Clean Resplit}}},
  urldate = {2022-10-29},
  abstract = {cleaned and balanced split of the ATIS Dataset},
  howpublished = {https://www.kaggle.com/datasets/siddhadev/atis-dataset-clean},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\JB33PSCL\\atis-dataset-clean.html}
}

@misc{ATISDataSetMS,
  title = {{{ATIS DataSet}} from {{MS CNTK}}},
  urldate = {2022-10-29},
  abstract = {Explore and run machine learning code with Kaggle Notebooks | Using data from ATIS from MS CNTK},
  howpublished = {https://kaggle.com/code/siddhadev/atis-dataset-from-ms-cntk},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\K49E2D2D\\atis-dataset-from-ms-cntk.html}
}

@misc{Aveugle2022,
  title = {{aveugle}},
  year = {2022},
  month = jun,
  journal = {Wiktionnaire},
  urldate = {2022-09-01},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {french},
  annotation = {Page Version ID: 30590238},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\YDWHDH4A\\aveugle.html}
}

@article{baevskiData2vecGeneralFramework2022,
  title = {Data2vec: {{A General Framework}} for {{Self-supervised Learning}} in {{Speech}}, {{Vision}} and {{Language}}},
  shorttitle = {Data2vec},
  author = {Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.03555 [cs]},
  eprint = {2202.03555},
  primaryclass = {cs},
  urldate = {2022-04-01},
  abstract = {While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\Q79ZZN64\\Baevski et al. - 2022 - data2vec A General Framework for Self-supervised .pdf;C\:\\Users\\Vincent\\Zotero\\storage\\5CZIV56Z\\2202.html}
}

@misc{BERT2022,
  title = {{{BERT}}},
  year = {2022},
  month = sep,
  urldate = {2022-09-02},
  abstract = {TensorFlow code and pre-trained models for BERT},
  copyright = {Apache-2.0},
  howpublished = {Google Research},
  keywords = {google,natural-language-processing,natural-language-understanding,nlp,tensorflow}
}

@misc{bhavsarVarietyEncodersNLP2020,
  title = {Variety {{Of Encoders In NLP}}},
  author = {Bhavsar, Pratik},
  year = {2020},
  month = jun,
  journal = {Modern NLP},
  urldate = {2021-10-17},
  abstract = {Master feature engineering for text},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\E5T3LVZ7\\on-variety-of-encoding-text-8b7623969d1e.html}
}

@misc{BlindSquare,
  title = {{{BlindSquare}}},
  urldate = {2022-07-20},
  langid = {american},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\NC5G8WVT\\www.blindsquare.com.html}
}

@article{Braille2022,
  title = {{Braille}},
  year = {2022},
  month = jun,
  journal = {Wikip\'edia},
  urldate = {2022-09-01},
  abstract = {Le braille (prononc\'e en fran\c{c}ais : [bʁaj]) est un syst\`eme d'\'ecriture tactile \`a points saillants, \`a l'usage des personnes aveugles ou fortement malvoyantes. Le syst\`eme porte le nom de son inventeur, le Fran\c{c}ais Louis Braille (1809-1852) qui avait perdu la vue \`a la suite d'un accident. \'El\`eve \`a l'Institution royale des jeunes aveugles, il modifie et perfectionne le code Barbier. En 1829 para\^it le premier expos\'e de sa m\'ethode. Un document qui n'est pas \'ecrit en braille et qui n'est donc pas lisible par un aveugle est dit \guillemotleft{} en noir \guillemotright{} ou \guillemotleft{} noir \guillemotright{} (un livre en noir, par exemple).},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {french},
  annotation = {Page Version ID: 194524089},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\TM2N6KDQ\\Braille.html}
}

@article{braunWouldYouLie2020,
  title = {Would {{You Lie To Me Bot}}? {{Supporting Decision-Making Processes}} with {{Deceiving Virtual Agents}}},
  shorttitle = {Would {{You Lie To Me Bot}}?},
  author = {Braun, Daniel and Bhat, Manoj and Biesdorf, Andreas and Matthes, Florian},
  year = {2020},
  month = jan,
  journal = {Procedia Computer Science},
  series = {The 11th {{International Conference}} on {{Emerging Ubiquitous Systems}} and {{Pervasive Networks}} ({{EUSPN}} 2020) / {{The}} 10th {{International Conference}} on {{Current}} and {{Future Trends}} of {{Information}} and {{Communication Technologies}} in {{Healthcare}} ({{ICTH}} 2020) / {{Affiliated Workshops}}},
  volume = {177},
  pages = {587--592},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2020.10.083},
  urldate = {2021-09-02},
  abstract = {Although we learn as children that "you shall not lie", it is widely accepted that deceptions and lies are necessary, yet unloved, parts of human communication. They are part of the clay that holds together the bricks of our society. In human-computer interaction, however, deceiving machines are seen as a taboo. In this paper, we want to challenge this perception by describing a virtual agent that improves decision-making processes by deceiving decision-makers in a controlled manner. A first survey we conducted shows that such a bot could indeed lead to a more informed decision-making process.},
  langid = {english},
  keywords = {artificial intelligence,bots,decision-making,software architecture},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\BGCXLHQX\\Braun et al. - 2020 - Would You Lie To Me Bot Supporting Decision-Makin.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\5HE7CVJX\\S1877050920323541.html}
}

@article{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  journal = {arXiv:2005.14165 [cs]},
  eprint = {2005.14165},
  primaryclass = {cs},
  urldate = {2022-04-22},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: 40+32 pages},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\MNBGCTKG\\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\C48WBCZI\\2005.html}
}

@misc{BuildChatbotsChatbot,
  title = {Build {{Chatbots}} | {{Chatbot}} for {{Developers}}},
  journal = {Build Chatbots | Open-Source Chatbot Development Tools | Botpress},
  urldate = {2022-09-01},
  abstract = {Botpress is a modern developer stack to build enterprise and open-source chatbots. Learn about our conversational AI platform built for developers.},
  howpublished = {https://botpress.com},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\CKWUBQJW\\botpress.com.html}
}

@misc{BuildSoftwareBetter,
  title = {Build Software Better, Together},
  journal = {GitHub},
  urldate = {2022-01-17},
  abstract = {GitHub is where people build software. More than 73 million people use GitHub to discover, fork, and contribute to over 200 million projects.},
  howpublished = {https://github.com},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\DEE5UU2V\\new.html}
}

@misc{caladoWeNeedTalk2017,
  title = {We Need to Talk about {{Accessibility}} on {{Chatbots}}},
  author = {Calado, Caio},
  year = {2017},
  month = jan,
  journal = {Medium},
  urldate = {2021-09-02},
  abstract = {What happens when a blind person wants to use your chatbot?},
  howpublished = {https://uxdesign.cc/we-need-to-talk-about-accessibility-on-chatbots-98cf93c54963},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\BMFHCW8L\\we-need-to-talk-about-accessibility-on-chatbots-98cf93c54963.html}
}

@misc{camembertCamemBERT,
  title = {{{CamemBERT}}},
  author = {CamemBERT},
  journal = {CamemBERT},
  urldate = {2021-09-28},
  abstract = {A Tasty French Language Model},
  howpublished = {https://camembert-model.fr/},
  langid = {american},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\Q8WXYDZA\\camembert-model.fr.html}
}

@misc{CanneBlanche,
  title = {{Canne blanche}},
  journal = {Ligue Braille},
  urldate = {2022-07-26},
  howpublished = {https://www.braille.be/fr/services-et-aides-techniques/se-deplacer/canne-blanche},
  langid = {french},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\QM3SFII8\\canne-blanche.html}
}

@misc{CeciteDeficienceVisuelle2023,
  title = {{C\'ecit\'e et d\'eficience visuelle}},
  year = {2023},
  month = oct,
  journal = {Organisation mondiale de la Sant\'e},
  urldate = {2021-10-17},
  abstract = {La rubrique "Principaux rep\`eres" de l'OMS sur la c\'ecit\'e et la d\'eficience visuelle fournit des faits, des d\'efinitions, les causes, ainsi que des informations sur les personnes \`a risque et sur l'action men\'ee \`a l'\'echelle mondiale et par l'OMS dans ce domaine.},
  howpublished = {https://www.who.int/fr/news-room/fact-sheets/detail/blindness-and-visual-impairment},
  langid = {french}
}

@misc{Chatbotsexplained2021,
  title = {Chatbots-Explained},
  year = {2021},
  month = aug,
  urldate = {2021-09-21},
  abstract = {Learn what a chatbot is, how it improves the customer experience and the best practices for building them.},
  howpublished = {https://www.ibm.com/cloud/learn/chatbots-explained},
  langid = {american}
}

@misc{chenBERTJointIntent2019,
  title = {{{BERT}} for {{Joint Intent Classification}} and {{Slot Filling}}},
  author = {Chen, Qian and Zhuo, Zhu and Wang, Wen},
  year = {2019},
  month = feb,
  number = {arXiv:1902.10909},
  eprint = {1902.10909},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-09-16},
  abstract = {Intent classification and slot filling are two essential tasks for natural language understanding. They often suffer from small-scale human-labeled training data, resulting in poor generalization capability, especially for rare words. Recently a new language representation model, BERT (Bidirectional Encoder Representations from Transformers), facilitates pre-training deep bidirectional representations on large-scale unlabeled corpora, and has created state-of-the-art models for a wide variety of natural language processing tasks after simple fine-tuning. However, there has not been much effort on exploring BERT for natural language understanding. In this work, we propose a joint intent classification and slot filling model based on BERT. Experimental results demonstrate that our proposed model achieves significant improvement on intent classification accuracy, slot filling F1, and sentence-level semantic frame accuracy on several public benchmark datasets, compared to the attention-based recurrent neural network models and slot-gated models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: 4 pages, 1 figure},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\QEPPZ64R\\Chen et al. - 2019 - BERT for Joint Intent Classification and Slot Fill.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\55S56AEI\\1902.html}
}

@article{chenGroundingAnswersVisual2022,
  title = {Grounding {{Answers}} for {{Visual Questions Asked}} by {{Visually Impaired People}}},
  author = {Chen, Chongyan and Anjum, Samreen and Gurari, Danna},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.01993 [cs]},
  eprint = {2202.01993},
  primaryclass = {cs},
  urldate = {2022-02-23},
  abstract = {Visual question answering is the task of answering questions about images. We introduce the VizWiz-VQA-Grounding dataset, the first dataset that visually grounds answers to visual questions asked by people with visual impairments. We analyze our dataset and compare it with five VQA-Grounding datasets to demonstrate what makes it similar and different. We then evaluate the SOTA VQA and VQA-Grounding models and demonstrate that current SOTA algorithms often fail to identify the correct visual evidence where the answer is located. These models regularly struggle when the visual evidence occupies a small fraction of the image, for images that are higher quality, as well as for visual questions that require skills in text recognition. The dataset, evaluation server, and leaderboard all can be found at the following link: https://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\GNJJ3SM7\\Chen et al. - 2022 - Grounding Answers for Visual Questions Asked by Vi.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\MHM4RWQQ\\2202.html}
}

@misc{chrismccormickaiApplyingBERTQuestion2020,
  title = {Applying {{BERT}} to {{Question Answering}} ({{SQuAD}} v1.1)},
  author = {{ChrisMcCormickAI}},
  year = {2020},
  month = mar,
  urldate = {2021-09-29},
  abstract = {In this video I'll explain the details of how BERT is used to perform ``Question Answering''--specifically, how it's applied to SQuAD v1.1 (Stanford Question Answering Dataset). I'll also walk us through the following notebook, where we'll take a model that's already been fine-tuned on SQuAD, and apply it to our own questions and text: https://colab.research.google.com/dri... ==== Fine-Tuning ==== As a follow-up to this video, I've published another tutorial Notebook on fine-tuning BERT on SQuAD. It's available to purchase on my site here: https://bit.ly/3inyZDv ==== Updates ==== Sign up to hear about new content across my blog and channel: https://www.chrismccormick.ai/subscribe}
}

@misc{ClavierBrailleTous,
  title = {Clavier {{Braille}} - {{Tous}} Les Fabricants de Mat\'eriel M\'edical},
  urldate = {2022-07-22},
  howpublished = {https://www.medicalexpo.fr/fabricant-medical/clavier-braille-45576.html},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\FB8KGJSB\\clavier-braille-45576.html}
}

@misc{CLIP2022,
  title = {{{CLIP}}},
  year = {2022},
  month = mar,
  urldate = {2022-03-11},
  abstract = {Contrastive Language-Image Pretraining},
  copyright = {MIT},
  howpublished = {OpenAI}
}

@misc{codepalaceCommentCreerSysteme2021,
  title = {Comment Cr\'eer Un Syst\`eme de R\'eponse de Chat Bot Pr\'ecis Avec {{Python}} (2021)},
  author = {{Code Palace}},
  year = {2021},
  month = mar,
  urldate = {2022-01-14},
  abstract = {Dans ce tutorial, je vais vous montrer comment cr\'eer un syst\`eme de r\'eponse de chat bot pr\'ecis en moins de 15 minutes \`a l'aide de Python 3.8. Obtenez le projet ici: https://github.com/federicocotogno}
}

@misc{CommandesVocalesTensorFlow,
  title = {{commandes\_vocales | TensorFlow Datasets}},
  journal = {TensorFlow},
  urldate = {2022-02-16},
  howpublished = {https://www.tensorflow.org/datasets/catalog/speech\_commands?hl=fr},
  langid = {french}
}

@misc{Contents2022,
  title = {Contents},
  year = {2022},
  month = sep,
  urldate = {2022-09-02},
  abstract = {Ongoing research training transformer models at scale},
  howpublished = {NVIDIA Corporation}
}

@misc{corporationBuildChatbotMicrosoft,
  title = {Build a {{Chatbot}} | {{Microsoft Power Virtual Agents}}},
  author = {Corporation, Microsoft},
  urldate = {2021-12-31},
  abstract = {Get insider tips to building a chatbot, including best practices for chatbot development and criteria for picking the right chatbot software.},
  howpublished = {https://powervirtualagents.microsoft.com/en-us/build-a-chatbot/},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\5EAYQ43F\\build-a-chatbot.html}
}

@misc{DearOpenAIPlease2019,
  title = {Dear {{OpenAI}}: {{Please Open Source Your Language Model}}},
  shorttitle = {Dear {{OpenAI}}},
  year = {2019},
  month = feb,
  journal = {The Gradient},
  urldate = {2022-09-02},
  abstract = {While OpenAI is correct to be concerned about misuse, withholding the full model is both unnecessary for safety and detrimental to future progress in AI.},
  howpublished = {https://thegradient.pub/openai-please-open-source-your-language-model/},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\QDJX77IF\\openai-please-open-source-your-language-model.html}
}

@misc{DeepMindGatoMediocre,
  title = {{{DeepMind}}'s '{{Gato}}' Is Mediocre, so Why Did They Build It?},
  journal = {ZDNET},
  urldate = {2022-09-01},
  abstract = {DeepMind's program is a generalist, to test the notion that over time, greater computing power will win in AI.},
  howpublished = {https://www.zdnet.com/article/deepminds-gato-is-mediocre-so-why-did-they-build-it/},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\U9LZKUGH\\deepminds-gato-is-mediocre-so-why-did-they-build-it.html}
}

@misc{DeepMindNewAI,
  title = {{{DeepMind}}'s New {{AI}} System Can Perform over 600 Tasks},
  journal = {TechCrunch},
  urldate = {2022-09-01},
  abstract = {Alphabet-backed research lab DeepMind created an AI system, Gato, that can perform hundreds of tasks ranging from controlling a robot to analyzing text.},
  langid = {american},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\3B66FBII\\deepminds-new-ai-can-perform-over-600-tasks-from-playing-games-to-controlling-robots.html}
}

@misc{demoGitHubCopilotGPT3,
  title = {{{GitHub Copilot}} | {{GPT-3 Demo}}},
  author = {Demo, GPT-3},
  urldate = {2022-09-02},
  abstract = {AI-powered pair programming *GitHub Copilot uses OpenAI Codex instead of GPT-3* Trained on billions of lines of public code, GitHub Copilot puts the kn...},
  howpublished = {https://gpt3demo.com/apps/github-copilot},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\WZKXFXNQ\\github-copilot.html}
}

@misc{dengRetinaFaceSinglestageDense2019,
  title = {{{RetinaFace}}: {{Single-stage Dense Face Localisation}} in the {{Wild}}},
  shorttitle = {{{RetinaFace}}},
  author = {Deng, Jiankang and Guo, Jia and Zhou, Yuxiang and Yu, Jinke and Kotsia, Irene and Zafeiriou, Stefanos},
  year = {2019},
  month = may,
  number = {arXiv:1905.00641},
  eprint = {1905.00641},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.00641},
  urldate = {2023-04-04},
  abstract = {Though tremendous strides have been made in uncontrolled face detection, accurate and efficient face localisation in the wild remains an open challenge. This paper presents a robust single-stage face detector, named RetinaFace, which performs pixel-wise face localisation on various scales of faces by taking advantages of joint extra-supervised and self-supervised multi-task learning. Specifically, We make contributions in the following five aspects: (1) We manually annotate five facial landmarks on the WIDER FACE dataset and observe significant improvement in hard face detection with the assistance of this extra supervision signal. (2) We further add a self-supervised mesh decoder branch for predicting a pixel-wise 3D shape face information in parallel with the existing supervised branches. (3) On the WIDER FACE hard test set, RetinaFace outperforms the state of the art average precision (AP) by 1.1\% (achieving AP equal to 91.4\%). (4) On the IJB-C test set, RetinaFace enables state of the art methods (ArcFace) to improve their results in face verification (TAR=89.59\% for FAR=1e-6). (5) By employing light-weight backbone networks, RetinaFace can run real-time on a single CPU core for a VGA-resolution image. Extra annotations and code have been made available at: https://github.com/deepinsight/insightface/tree/master/RetinaFace.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\DQY8CHBI\\Deng et al. - 2019 - RetinaFace Single-stage Dense Face Localisation i.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\KSXYDJP9\\1905.html}
}

@misc{DesktopOperatingSystem,
  title = {Desktop {{Operating System Market Share Worldwide}}},
  journal = {StatCounter Global Stats},
  urldate = {2022-07-18},
  abstract = {This graph shows the market share of desktop operating systems worldwide based on over 10 billion monthly page views.},
  howpublished = {https://gs.statcounter.com/os-market-share/desktop/worldwide},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\BML9JMSB\\worldwide.html}
}

@misc{deyZaraPersonalityInduction2021,
  title = {Zara \textemdash{} {{Personality Induction}} and {{Adaptation}} by an {{Empathetic Virtual Agent}}},
  author = {Dey, Anik},
  year = {2021},
  month = jan,
  journal = {Anik Dey Portfolio},
  urldate = {2022-06-21},
  abstract = {Zara, or `Zara the Supergirl', is a virtual robot that analyses the MBTI personality of the user after a 5\textendash 10 minute conversation. It can\ldots},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\AWKDBSHL\\zara-personality-induction-and-adaptation-by-an-empathetic-virtual-agent-3a9a10f6114.html}
}

@misc{DistilBERT,
  title = {{{DistilBERT}}},
  urldate = {2021-09-17},
  abstract = {Overview: The DistilBERT model was proposed in the blog post Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT, and the ...},
  howpublished = {https://huggingface.co/transformers/model\_doc/model\_doc/distilbert.html},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\9RW3AAWZ\\distilbert.html}
}

@article{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  month = jun,
  journal = {arXiv:2010.11929 [cs]},
  eprint = {2010.11929},
  primaryclass = {cs},
  urldate = {2022-04-01},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\YYEQXSK8\\Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\GTZEMXYH\\2010.html}
}

@inproceedings{doyleWhatWeSee2021,
  title = {What {{Do We See}} in {{Them}}? {{Identifying Dimensions}} of {{Partner Models}} for {{Speech Interfaces Using}} a {{Psycholexical Approach}}},
  shorttitle = {What {{Do We See}} in {{Them}}?},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Doyle, Philip R and Clark, Leigh and Cowan, Benjamin R.},
  year = {2021},
  month = may,
  series = {{{CHI}} '21},
  pages = {1--14},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3411764.3445206},
  urldate = {2022-02-10},
  abstract = {Perceptions of system competence and communicative ability, termed partner models, play a significant role in speech interface interaction. Yet we do not know what the core dimensions of this concept are. Taking a psycholexical approach, our paper is the first to identify the key dimensions that define partner models in speech agent interaction. Through a repertory grid study (N=21), a review of key subjective questionnaires, an expert review of resulting word pairs and an online study of 356 users of speech interfaces, we identify three key dimensions that make up a users' partner model: 1) perceptions towards partner competence and dependability; 2) assessment of human-likeness; and 3) a system's perceived cognitive flexibility. We discuss the implications for partner modelling as a concept, emphasising the importance of salience and the dynamic nature of these perceptions.},
  isbn = {978-1-4503-8096-6},
  keywords = {human-machine dialogue,mental models,partner models,psycholexical,psychometrics,speech interfaces},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\LB2MPH32\\Doyle et al. - 2021 - What Do We See in Them Identifying Dimensions of .pdf}
}

@article{drozdowskiDemographicBiasBiometrics2020,
  title = {Demographic {{Bias}} in {{Biometrics}}: {{A Survey}} on an {{Emerging Challenge}}},
  shorttitle = {Demographic {{Bias}} in {{Biometrics}}},
  author = {Drozdowski, P. and Rathgeb, C. and Dantcheva, A. and Damer, N. and Busch, C.},
  year = {2020},
  month = jun,
  journal = {IEEE Transactions on Technology and Society},
  volume = {1},
  number = {2},
  eprint = {2003.02488},
  primaryclass = {cs},
  pages = {89--103},
  issn = {2637-6415},
  doi = {10.1109/TTS.2020.2992344},
  urldate = {2023-04-07},
  abstract = {Systems incorporating biometric technologies have become ubiquitous in personal, commercial, and governmental identity management applications. Both cooperative (e.g. access control) and non-cooperative (e.g. surveillance and forensics) systems have benefited from biometrics. Such systems rely on the uniqueness of certain biological or behavioural characteristics of human beings, which enable for individuals to be reliably recognised using automated algorithms. Recently, however, there has been a wave of public and academic concerns regarding the existence of systemic bias in automated decision systems (including biometrics). Most prominently, face recognition algorithms have often been labelled as "racist" or "biased" by the media, non-governmental organisations, and researchers alike. The main contributions of this article are: (1) an overview of the topic of algorithmic bias in the context of biometrics, (2) a comprehensive survey of the existing literature on biometric bias estimation and mitigation, (3) a discussion of the pertinent technical and social matters, and (4) an outline of the remaining challenges and future work items, both from technological and social points of view.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Computers and Society,Computer Science - Cryptography and Security},
  note = {Comment: 15 pages, 3 figures, 3 tables. Submitted to IEEE Transactions on Technology and Society. Update after first round of peer review},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\9HYUV3D6\\Drozdowski et al. - 2020 - Demographic Bias in Biometrics A Survey on an Eme.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\SB6YK5UX\\2003.html}
}

@misc{DSTC11,
  title = {{DSTC11}},
  urldate = {2022-10-26},
  abstract = {News},
  howpublished = {https://dstc11.dstc.community/},
  langid = {french},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\9IGKU4LR\\dstc11.dstc.community.html}
}

@misc{dunnStructuredInformationExtraction2022,
  title = {Structured Information Extraction from Complex Scientific Text with Fine-Tuned Large Language Models},
  author = {Dunn, Alexander and Dagdelen, John and Walker, Nicholas and Lee, Sanghoon and Rosen, Andrew S. and Ceder, Gerbrand and Persson, Kristin and Jain, Anubhav},
  year = {2022},
  month = dec,
  number = {arXiv:2212.05238},
  eprint = {2212.05238},
  primaryclass = {cond-mat},
  publisher = {{arXiv}},
  urldate = {2023-04-04},
  abstract = {Intelligently extracting and linking complex scientific information from unstructured text is a challenging endeavor particularly for those inexperienced with natural language processing. Here, we present a simple sequence-to-sequence approach to joint named entity recognition and relation extraction for complex hierarchical information in scientific text. The approach leverages a pre-trained large language model (LLM), GPT-3, that is fine-tuned on approximately 500 pairs of prompts (inputs) and completions (outputs). Information is extracted either from single sentences or across sentences in abstracts/passages, and the output can be returned as simple English sentences or a more structured format, such as a list of JSON objects. We demonstrate that LLMs trained in this way are capable of accurately extracting useful records of complex scientific knowledge for three representative tasks in materials chemistry: linking dopants with their host materials, cataloging metal-organic frameworks, and general chemistry/phase/morphology/application information extraction. This approach represents a simple, accessible, and highly-flexible route to obtaining large databases of structured knowledge extracted from unstructured text. An online demo is available at http://www.matscholar.com/info-extraction.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Condensed Matter - Materials Science,I.7.m},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\YETSJQZQ\\Dunn et al. - 2022 - Structured information extraction from complex sci.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\C6Z96TKM\\2212.html}
}

@article{duprePerformanceComparisonEight2020,
  title = {A Performance Comparison of Eight Commercially Available Automatic Classifiers for Facial Affect Recognition},
  author = {Dupr{\'e}, Damien and Krumhuber, Eva G. and K{\"u}ster, Dennis and McKeown, Gary J.},
  year = {2020},
  month = apr,
  journal = {PLOS ONE},
  volume = {15},
  number = {4},
  pages = {e0231968},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0231968},
  urldate = {2023-04-07},
  abstract = {In the wake of rapid advances in automatic affect analysis, commercial automatic classifiers for facial affect recognition have attracted considerable attention in recent years. While several options now exist to analyze dynamic video data, less is known about the relative performance of these classifiers, in particular when facial expressions are spontaneous rather than posed. In the present work, we tested eight out-of-the-box automatic classifiers, and compared their emotion recognition performance to that of human observers. A total of 937 videos were sampled from two large databases that conveyed the basic six emotions (happiness, sadness, anger, fear, surprise, and disgust) either in posed (BU-4DFE) or spontaneous (UT-Dallas) form. Results revealed a recognition advantage for human observers over automatic classification. Among the eight classifiers, there was considerable variance in recognition accuracy ranging from 48\% to 62\%. Subsequent analyses per type of expression revealed that performance by the two best performing classifiers approximated those of human observers, suggesting high agreement for posed expressions. However, classification accuracy was consistently lower (although above chance level) for spontaneous affective behavior. The findings indicate potential shortcomings of existing out-of-the-box classifiers for measuring emotions, and highlight the need for more spontaneous facial databases that can act as a benchmark in the training and testing of automatic emotion recognition systems. We further discuss some limitations of analyzing facial expressions that have been recorded in controlled environments.},
  langid = {english},
  keywords = {Computer software,Emotions,Face,Face recognition,Facial expressions,Fear,Happiness,Human performance},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\S63ASDWB\\Dupré et al. - 2020 - A performance comparison of eight commercially ava.pdf}
}

@misc{dylanavalverdeBriefHistoryChatbots2018,
  title = {A {{Brief History}} of {{Chatbots}}},
  author = {{dylanavalverde}},
  year = {2018},
  month = mar,
  journal = {Perception, Control, Cognition},
  urldate = {2021-10-11},
  abstract = {As BYU's graduate and undergraduate students work tirelessly to create EVE, we wanted to take a step back and look at the history of chatbots. This is not a comprehensive history of chatbots; rathe\ldots},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\LWINEX9C\\a-brief-history-of-chatbots.html}
}

@misc{eagleGPT2HardwareRequirements2021,
  type = {Forum Post},
  title = {{{GPT-2}}: ({{Hardware}}) Requirements for Fine-Tuning the {{774M}} Model},
  shorttitle = {{{GPT-2}}},
  author = {Eagle, Comfort},
  year = {2021},
  month = dec,
  journal = {Artificial Intelligence Stack Exchange},
  urldate = {2022-09-02},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\WDJVCPI3\\gpt-2-hardware-requirements-for-fine-tuning-the-774m-model.html}
}

@misc{Emojify,
  title = {Emojify},
  urldate = {2023-04-03},
  howpublished = {https://emojify.info/menu},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\BVLSIXUC\\menu.html}
}

@misc{EnvisionApp2022,
  title = {Envision {{App}}},
  year = {2022},
  journal = {envision},
  urldate = {2022-09-01},
  abstract = {Envision is free a smartphone app that articulates everyday visual information into speech. And, with this information, comes the feeling of independence and the perception of possibilities.},
  howpublished = {https://www.letsenvision.com/app},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\4AYES7FZ\\app.html}
}

@misc{Eqla,
  title = {{Eqla}},
  journal = {Eqla},
  urldate = {2022-09-01},
  abstract = {Changeons le quotidien des personnes aveugles et malvoyantes},
  langid = {french},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\PEM4T6PQ\\mieux-comprendre-la-deficience-visuelle.html}
}

@article{FacialRecognitionSystem2023,
  title = {Facial Recognition System},
  year = {2023},
  month = mar,
  journal = {Wikipedia},
  urldate = {2023-04-04},
  abstract = {A facial recognition system is a technology capable of matching a human face from a digital image or a video frame against a database of faces. Such a system is typically employed to authenticate users through ID verification services, and works by pinpointing and measuring facial features from a given image.Development began on similar systems in the 1960s, beginning as a form of computer application. Since their inception, facial recognition systems have seen wider uses in recent times on smartphones and in other forms of technology, such as robotics. Because computerized facial recognition involves the measurement of a human's physiological characteristics, facial recognition systems are categorized as biometrics. Although the accuracy of facial recognition systems as a biometric technology is lower than iris recognition and fingerprint recognition, it is widely adopted due to its contactless process. Facial recognition systems have been deployed in advanced human\textendash computer interaction, video surveillance and automatic indexing of images.Facial recognition systems are employed throughout the world today by governments and private companies. Their effectiveness varies, and some systems have previously been scrapped because of their ineffectiveness. The use of facial recognition systems has also raised controversy, with claims that the systems violate citizens' privacy, commonly make incorrect identifications, encourage gender norms and racial profiling, and do not protect important biometric data. The appearance of synthetic media such as deepfakes has also raised concerns about its security. These claims have led to the ban of facial recognition systems in several cities in the United States. Growing societal concerns led social networking company Meta Platforms to shut down its Facebook facial recognition system in 2021, deleting the face scan data of more than one billion users. The change represented one of the largest shifts in facial recognition usage in the technology's history.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1147316762},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\L96BTGAE\\Facial_recognition_system.html}
}

@misc{fendouaiAwesomeChatbotProjects2022,
  title = {Awesome {{Chatbot Projects}}},
  author = {{fendouai}},
  year = {2022},
  month = feb,
  urldate = {2022-02-08},
  abstract = {Awesome Chatbot Projects,Corpus,Papers,Tutorials.Chinese Chatbot ={$>$}:},
  copyright = {Apache-2.0},
  keywords = {awesome,chatbot,corpus,seq2seq,seq2seq-chatbot,seq2seq-model,tensorflow,tutorial}
}

@misc{FichesurlesfeuxsonoresPDFAccessiblePdf,
  title = {Fiche-Sur-Les-Feux-Sonores-{{PDF-Accessible}}.Pdf},
  urldate = {2022-07-22},
  abstract = {Fiches reprennant diff\'erentes informations sur le fonctionnement et l'utilisation des feux sonores.},
  howpublished = {https://aveuglesdefrance.org/app/uploads/2022/03/Fiche-sur-les-feux-sonores-PDF-Accessible.pdf},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\ABF8URDM\\Fiche-sur-les-feux-sonores-PDF-Accessible.pdf}
}

@misc{FondationAsileAveugles,
  title = {{La Fondation Asile des aveugles}},
  journal = {Fondation Asile des aveugles},
  urldate = {2023-01-20},
  abstract = {Nous poursuivons avec c\oe ur la mission de nos fondateurs au service de la sant\'e visuelle, depuis 175 ans.},
  howpublished = {https://www.ophtalmique.ch/la-fondation-asile-des-aveugles/},
  langid = {french}
}

@article{FontRasterization2022,
  title = {Font Rasterization},
  year = {2022},
  month = feb,
  journal = {Wikipedia},
  urldate = {2022-07-18},
  abstract = {Font rasterization  is the process of converting text from a vector description (as found in scalable fonts such as TrueType fonts) to a raster or bitmap description.  This often involves some anti-aliasing on screen text to make it smoother and easier to read.  It may also involve hinting\textemdash information embedded in the font data that optimizes rendering details for particular character sizes.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1070872834},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\EFDJ2GUL\\Font_rasterization.html}
}

@article{GatoDeepMind2022,
  title = {Gato ({{DeepMind}})},
  year = {2022},
  month = jun,
  journal = {Wikipedia},
  urldate = {2022-09-01},
  abstract = {Gato is a deep neural network for a range of complex tasks that exhibits multimodality. It can perform tasks such as engaging in a dialogue, playing video games, controlling a robot arm to stack blocks, and more. It was created by researchers at London-based AI firm DeepMind. It is a transformer, like GPT-3. According to MIT Technology Review, the system "learns multiple different tasks at the same time, which means it can switch between them without having to forget one skill before learning another" whereas "[t]he AI systems of today are called ``narrow,'' meaning they can only do a specific, restricted set of tasks such as generate text", and according to The Independent, it is a "'generalist agent' that can carry out a huge range of complex tasks, from stacking blocks to writing poetry". The technology has been described as "general purpose" artificial intelligence and a "step toward" artificial general intelligence.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1095006697},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\3YTWSSF5\\Gato_(DeepMind).html}
}

@misc{geekyfayeartEasy3DPrinted,
  title = {Easy {{3D Printed Braille}} to {{Add}} to {{Everything}}!},
  author = {{geekyfayeart}},
  journal = {Instructables},
  urldate = {2022-03-29},
  abstract = {Easy 3D Printed Braille to Add to Everything!: Over 43 million people worldwide are considered completely blind, and yet most of us remain woefully undereducated about braille and don't know how to include it in the signage we create! Braille may seem complicated, but I've broken it's creation d\ldots},
  howpublished = {https://www.instructables.com/Easy-3D-Printed-Braille-to-Add-to-Everything/},
  langid = {english}
}

@misc{GeneralistAgent,
  title = {A {{Generalist Agent}}},
  urldate = {2022-07-08},
  abstract = {Inspired by progress in large-scale language modelling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
  howpublished = {https://www.deepmind.com/publications/a-generalist-agent},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\EFL6BP6Y\\a-generalist-agent.html}
}

@misc{GetStartedAndroid2023,
  type = {User Guide},
  title = {Get Started on {{Android}} with {{TalkBack}} - {{Android}} Accessibility {{Help}}},
  year = {2023},
  urldate = {2023-04-04},
  howpublished = {https://support.google.com/accessibility/android/answer/6283677?hl=en-GB},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\ISQZJMZ9\\6283677.html}
}

@misc{GLUEBenchmark,
  title = {{{GLUE Benchmark}}},
  urldate = {2021-09-17},
  abstract = {The General Language Understanding Evaluation (GLUE) benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems},
  howpublished = {https://gluebenchmark.com/},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\73P9M4XZ\\gluebenchmark.com.html}
}

@misc{googledevelopersindiaConversationalAINew2020,
  title = {Conversational {{AI}}, the New {{UI}} ({{Google Cloud Community Day}} `19)},
  author = {{Google Developers India}},
  year = {2020},
  month = apr,
  urldate = {2022-09-22}
}

@misc{GPT3VsBERT2020,
  title = {{{GPT-3 Vs BERT For NLP Tasks}}},
  year = {2020},
  month = sep,
  journal = {Analytics India Magazine},
  urldate = {2022-04-25},
  abstract = {Both GPT-3 and BERT have been relatively new for the industry, but their SOTA performance has made them the winners in the NLP tasks.},
  howpublished = {https://analyticsindiamag.com/gpt-3-vs-bert-for-nlp-tasks/},
  langid = {american},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\RBAZJZ53\\gpt-3-vs-bert-for-nlp-tasks.html}
}

@misc{GPT42023,
  title = {{{GPT-4}}},
  year = {2023},
  month = mar,
  urldate = {2023-04-04},
  howpublished = {https://openai.com/product/gpt-4},
  langid = {american},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\FCKFISCJ\\gpt-4.html}
}

@article{guoVisualAttentionNetwork2022,
  title = {Visual {{Attention Network}}},
  author = {Guo, Meng-Hao and Lu, Cheng-Ze and Liu, Zheng-Ning and Cheng, Ming-Ming and Hu, Shi-Min},
  year = {2022},
  month = mar,
  journal = {arXiv:2202.09741 [cs]},
  eprint = {2202.09741},
  primaryclass = {cs},
  urldate = {2022-04-01},
  abstract = {While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision. (1) Treating images as 1D sequences neglects their 2D structures. (2) The quadratic complexity is too expensive for high-resolution images. (3) It only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel large kernel attention (LKA) module to enable self-adaptive and long-range correlations in self-attention while avoiding the above issues. We further introduce a novel neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple, VAN outperforms the state-of-the-art vision transformers and convolutional neural networks with a large margin in extensive experiments, including image classification, object detection, semantic segmentation, instance segmentation, etc. Code is available at https://github.com/Visual-Attention-Network.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Code is available at https://github.com/Visual-Attention-Network},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\24JDTR7Z\\Guo et al. - 2022 - Visual Attention Network.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\YCBYJTDY\\2202.html}
}

@inproceedings{guoVizLensRobustInteractive2016,
  title = {{{VizLens}}: {{A Robust}} and {{Interactive Screen Reader}} for {{Interfaces}} in the {{Real World}}},
  shorttitle = {{{VizLens}}},
  booktitle = {Proceedings of the 29th {{Annual Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Guo, Anhong and Chen, Xiang 'Anthony' and Qi, Haoran and White, Samuel and Ghosh, Suman and Asakawa, Chieko and Bigham, Jeffrey P.},
  year = {2016},
  month = oct,
  pages = {651--664},
  publisher = {{ACM}},
  address = {{Tokyo Japan}},
  doi = {10.1145/2984511.2984518},
  urldate = {2022-02-14},
  abstract = {The world is full of physical interfaces that are inaccessible to blind people, from microwaves and information kiosks to thermostats and checkout terminals. Blind people cannot in\- dependently use such devices without at least first learning their layout, and usually only after labeling them with sighted assistance. We introduce VizLens \textemdash an accessible mobile ap\- plication and supporting backend that can robustly and inter\- actively help blind people use nearly any interface they en\- counter. VizLens users capture a photo of an inaccessible interface and send it to multiple crowd workers, who work in parallel to quickly label and describe elements of the inter\- face to make subsequent computer vision easier. The VizLens application helps users recapture the interface in the field of the camera, and uses computer vision to interactively describe the part of the interface beneath their finger (updating 8 times per second). We show that VizLens provides accurate and usable real-time feedback in a study with 10 blind partici\- pants, and our crowdsourcing labeling workflow was fast (8 minutes), accurate (99.7\%), and cheap (\$1.15). We then ex\- plore extensions of VizLens that allow it to (i) adapt to state changes in dynamic interfaces, (ii) combine crowd labeling with OCR technology to handle dynamic displays, and (iii) benefit from head-mounted cameras. VizLens robustly solves a long-standing challenge in accessibility by deeply integrat\- ing crowdsourcing and computer vision, and foreshadows a future of increasingly powerful interactive applications that would be currently impossible with either alone.},
  isbn = {978-1-4503-4189-9},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\F35ZND2R\\Guo et al. - 2016 - VizLens A Robust and Interactive Screen Reader fo.pdf}
}

@article{guptaGeneralPurposeVision2022,
  title = {Towards {{General Purpose Vision Systems}}},
  author = {Gupta, Tanmay and Kamath, Amita and Kembhavi, Aniruddha and Hoiem, Derek},
  year = {2022},
  month = apr,
  journal = {arXiv:2104.00743 [cs]},
  eprint = {2104.00743},
  primaryclass = {cs},
  urldate = {2022-04-22},
  abstract = {Computer vision systems today are primarily N-purpose systems, designed and trained for a predefined set of tasks. Adapting such systems to new tasks is challenging and often requires non-trivial modifications to the network architecture (e.g. adding new output heads) or training process (e.g. adding new losses). To reduce the time and expertise required to develop new applications, we would like to create general purpose vision systems that can learn and perform a range of tasks without any modification to the architecture or learning process. In this paper, we propose GPV-1, a task-agnostic vision-language architecture that can learn and perform tasks that involve receiving an image and producing text and/or bounding boxes, including classification, localization, visual question answering, captioning, and more. We also propose evaluations of generality of architecture, skill-concept transfer, and learning efficiency that may inform future work on general purpose vision. Our experiments indicate GPV-1 is effective at multiple tasks, reuses some concept knowledge across tasks, can perform the Referring Expressions task zero-shot, and further improves upon the zero-shot performance using a few training samples.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: CVPR 2022 Oral; Project page: https://prior.allenai.org/projects/gpv},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\GHL7ZACH\\Gupta et al. - 2022 - Towards General Purpose Vision Systems.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\7V9ZTALD\\2104.html}
}

@misc{hagenTuringNLG17billionparameterLanguage2020,
  title = {Turing-{{NLG}}: {{A}} 17-Billion-Parameter Language Model by {{Microsoft}}},
  shorttitle = {Turing-{{NLG}}},
  author = {Hagen, Alexis},
  year = {2020},
  month = feb,
  journal = {Microsoft Research},
  urldate = {2022-09-02},
  abstract = {This figure was adapted from a similar image published in DistilBERT. Turing Natural Language Generation (T-NLG) is a 17 billion parameter language model by Microsoft that outperforms the state of the art on many downstream NLP tasks. We present a demo of the model, including its freeform generation, question answering, and summarization capabilities, to academics [\ldots ]},
  langid = {american},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\MSI76NTR\\turing-nlg-a-17-billion-parameter-language-model-by-microsoft.html}
}

@inproceedings{hakkani-turMultiDomainJointSemantic2016,
  title = {Multi-{{Domain Joint Semantic Frame Parsing Using Bi-Directional RNN-LSTM}}},
  booktitle = {Interspeech 2016},
  author = {{Hakkani-T{\"u}r}, Dilek and Tur, Gokhan and Celikyilmaz, Asli and Chen, Yun-Nung and Gao, Jianfeng and Deng, Li and Wang, Ye-Yi},
  year = {2016},
  month = sep,
  pages = {715--719},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2016-402},
  urldate = {2022-11-28},
  abstract = {Sequence-to-sequence deep learning has recently emerged as a new paradigm in supervised learning for spoken language understanding. However, most of the previous studies explored this framework for building single domain models for each task, such as slot filling or domain classification, comparing deep learning based approaches with conventional ones like conditional random fields. This paper proposes a holistic multi-domain, multi-task (i.e. slot filling, domain and intent detection) modeling approach to estimate complete semantic frames for all user utterances addressed to a conversational system, demonstrating the distinctive power of deep learning methods, namely bi-directional recurrent neural network (RNN) with long-short term memory (LSTM) cells (RNN-LSTM) to handle such complexity. The contributions of the presented work are three-fold: (i) we propose an RNN-LSTM architecture for joint modeling of slot filling, intent determination, and domain classification; (ii) we build a joint multi-domain model enabling multi-task deep learning where the data from each domain reinforces each other; (iii) we investigate alternative architectures for modeling lexical context in spoken language understanding. In addition to the simplicity of the single model framework, experimental results show the power of such an approach on Microsoft Cortana real user data over alternative methods based on single domain/task deep learning.},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\LHYBK47I\\Hakkani-Tür et al. - 2016 - Multi-Domain Joint Semantic Frame Parsing Using Bi.pdf}
}

@article{hashemihosseinabadMultipleAnswersQuestion2021,
  title = {Multiple Answers to a Question: A New Approach for Visual Question Answering},
  shorttitle = {Multiple Answers to a Question},
  author = {Hashemi~Hosseinabad, Sayedshayan and Safayani, Mehran and Mirzaei, Abdolreza},
  year = {2021},
  month = jan,
  journal = {The Visual Computer},
  volume = {37},
  number = {1},
  pages = {119--131},
  issn = {1432-2315},
  doi = {10.1007/s00371-019-01786-4},
  urldate = {2022-03-08},
  abstract = {With the advent of deep learning, multi-modal data have been of great interest. One of the multi-modal tasks which can be included in the computer vision domain is visual question answering (VQA). In VQA, a question and an image are entered into the model and the model tries to answer the question according to the image. To the best of our knowledge, the current techniques look at the image and only give one answer to the question asked. However, in some situations, there are several answers to the asked question. In this paper, we address this problem and define a new domain in the task of VQA as well as a new computationally efficient approach to cope with multiple-answer VQA. In this approach, we use a sliding window in an efficient manner to examine the answer to the question in different parts of the image. Due to the fact that so far no proper dataset is available for multiple-answer VQA, we provide a new dataset for evaluating our proposed model. The experiments express that our model uses 94\% less operation than other models, making it very suitable for real-time applications.},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\NJAZ5JNX\\Hashemi Hosseinabad et al. - 2021 - Multiple answers to a question a new approach for.pdf}
}

@misc{Home2021,
  title = {{Home}},
  year = {2021},
  month = sep,
  journal = {Semanticbots},
  urldate = {2021-09-02},
  howpublished = {https://www.semanticbots.com/},
  langid = {spanish}
}

@misc{Home2021a,
  title = {{Home}},
  year = {2021},
  month = sep,
  journal = {Sayobo},
  urldate = {2021-09-02},
  howpublished = {https://www.sayobo.io/},
  langid = {spanish}
}

@misc{HomeCVPR2022,
  title = {Home | {{CVPR}} 2022},
  urldate = {2022-05-24},
  howpublished = {https://cvpr2022.thecvf.com/},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\E2EJUJAI\\cvpr2022.thecvf.com.html}
}

@misc{HowBERTGPT2020,
  title = {How {{BERT}} and {{GPT}} Models Change the Game for {{NLP}}},
  year = {2020},
  month = dec,
  journal = {Watson Blog},
  urldate = {2022-04-25},
  abstract = {Our NLP series blog discusses the BERT and GPT models: what makes these models so powerful and how they can benefit your business.},
  copyright = {\textcopyright{} Copyright IBM Corp. 2022},
  howpublished = {https://www.ibm.com/blogs/watson/2020/12/how-bert-and-gpt-models-change-the-game-for-nlp/},
  langid = {american},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\BSES5HM7\\how-bert-and-gpt-models-change-the-game-for-nlp.html}
}

@misc{HowBuildChatbot,
  title = {How {{To Build}} a {{Chatbot With GPT-3}} or {{GPT-J}}},
  urldate = {2022-01-14},
  abstract = {GPT-3 and GPT-J give amazing results. They are the best AI models for chatbots as of this writing. Here are a couple of tips in order to help you build the best chatbot with GPT-3 or GPT-J. We also make a quick comparison with Blenderbot 2.},
  howpublished = {https://nlpcloud.io/how-to-build-chatbot-gpt-3-gpt-j.html},
  langid = {english}
}

@misc{hrilaboratoryLearningInteractionsNatural2018,
  title = {Learning {{Interactions}} from {{Natural Language Instruction}} and {{Demonstration}}},
  author = {{HRILaboratory}},
  year = {2018},
  month = aug,
  urldate = {2022-02-10}
}

@misc{HuggingFaceAI,
  title = {Hugging {{Face}} \textendash{} {{The AI}} Community Building the Future.},
  urldate = {2022-11-06},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/datasets},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\YRE6QT22\\datasets.html}
}

@misc{HuggingFaceAIa,
  title = {Hugging {{Face}} \textendash{} {{The AI}} Community Building the Future.},
  urldate = {2022-11-24},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/datasets},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\H3FH4AJH\\datasets.html}
}

@article{humeauPolyencodersTransformerArchitectures2020,
  title = {Poly-Encoders: {{Transformer Architectures}} and {{Pre-training Strategies}} for {{Fast}} and {{Accurate Multi-sentence Scoring}}},
  shorttitle = {Poly-Encoders},
  author = {Humeau, Samuel and Shuster, Kurt and Lachaux, Marie-Anne and Weston, Jason},
  year = {2020},
  month = mar,
  journal = {arXiv:1905.01969 [cs]},
  eprint = {1905.01969},
  primaryclass = {cs},
  urldate = {2021-09-30},
  abstract = {The use of deep pre-trained bidirectional transformers has led to remarkable progress in a number of applications (Devlin et al., 2018). For tasks that make pairwise comparisons between sequences, matching a given input with a corresponding label, two approaches are common: Cross-encoders performing full self-attention over the pair and Bi-encoders encoding the pair separately. The former often performs better, but is too slow for practical use. In this work, we develop a new transformer architecture, the Poly-encoder, that learns global rather than token level self-attention features. We perform a detailed comparison of all three approaches, including what pre-training and fine-tuning strategies work best. We show our models achieve state-of-the-art results on three existing tasks; that Poly-encoders are faster than Cross-encoders and more accurate than Bi-encoders; and that the best results are obtained by pre-training on large datasets similar to the downstream tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  note = {Comment: ICLR 2020},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\PQKQUCB9\\Humeau et al. - 2020 - Poly-encoders Transformer Architectures and Pre-t.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\7CAWWNQD\\1905.html}
}

@misc{HypeDeepMindNew,
  title = {The Hype around {{DeepMind}}'s New {{AI}} Model Misses What's Actually Cool about It},
  journal = {MIT Technology Review},
  urldate = {2022-09-01},
  abstract = {Some worry that the chatter about these tools is doing the whole field a disservice.},
  howpublished = {https://www.technologyreview.com/2022/05/23/1052627/deepmind-gato-ai-model-hype/},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\DE8THW3V\\deepmind-gato-ai-model-hype.html}
}

@misc{ibmtechnologyWhatChatbot2021,
  title = {What Is a {{Chatbot}}?},
  author = {{IBM Technology}},
  year = {2021},
  urldate = {2021-09-20},
  abstract = {Learn more about Chatbots \textrightarrow{} http://ibm.biz/chatbot-guide Learn more about AI \textrightarrow{} http://ibm.biz/learn-ai Learn more about Natural Language Processing \textrightarrow{} http://ibm.biz/nlp-guide Watch "What is Natural Language Processing (NLP)" lightboard video \textrightarrow{} https://youtu.be/fLvJ8VdHLA0 Check out IBM Watson Assistant \textrightarrow{} http://ibm.biz/ibm-watson-assistant Check out IBM Cloud Pak for Data \textrightarrow{} http://ibm.biz/cp-for-data Earn a badge and create your own FREE Chatbot \textrightarrow{} http://ibm.biz/build-your-own-chatbot By now most of us have interacted with a chatbot in one form or another, but exactly how do they work? Do chatbots only operate on websites, or are there other mediums that a chatbot can facilitate a conversation? And why would anyone want to use a chatbot? In this lightboard video, Morgan Carroll with IBM Cloud, answers these questions and many more as she walks through an example of Floral company using a chatbot and shows first hand what a chatbot is, how it works, and why you may want to use one for your business. Earn a badge with FREE interactive Kubernetes labs \textrightarrow{} https://ibm.biz/try-k8s-labs Get started on IBM Cloud at no cost \textrightarrow{} http://ibm.biz/start-a-free-cloud-acct Subscribe to the IBM Cloud channel to be notified when a new video drops \textrightarrow{} http://ibm.biz/subscribe-now \#Chatbots \#VirtualAssistants \#AI}
}

@misc{ibmtechnologyWhatNLPNatural2021,
  title = {What Is {{NLP}} ({{Natural Language Processing}})?},
  author = {{IBM Technology}},
  year = {2021},
  urldate = {2021-09-14},
  abstract = {Learn more about NLP with free guide \textrightarrow{} http://ibm.biz/guide-to-nlp  Learn how to build apps with NLP \textrightarrow{} http://ibm.biz/watson-nlp Watch "What is a Chatbot" lightboard video \textrightarrow{} https://youtu.be/o9-ObGgfpEk  Check out IBM Watson Natural Language Understanding \textrightarrow{} http://ibm.biz/watson-nl-understanding Every time you surf the internet you encounter a Natural Language Processing, or NLP, application. But what exactly is NLP and how does it work? In this lightboard video, Master Inventor with IBM, Martin Keen, visually explains what NLP is and why we need it, as well as how NLP takes unstructured human speech and converts it to structured data that a computer can understand. Chapters 0:00 - Intro 0:38 - Unstructured data 1:12 - Structured data 2:03 - Natural Language Understanding (NLU) \& Natural Language Generation (NLG) 2:36 - Machine Translation use case 3:40 - Virtual Assistance / Chat Bots use case 4:14 - Sentiment Analysis use case 4:44 - Spam Detection use case 5:44 - Tokenization 6:18 - Stemming \& Lemmatization 7:42 - Part of Speech Tagging 8:22 - Named Entity Recognition (NER) 9:08 - Summary Subscribe to the IBM Cloud channel to be notified when a new video drops \textrightarrow{} http://ibm.biz/subscribe-now  Get started on IBM Cloud at no cost \textrightarrow{} http://ibm.biz/create-your-free-acct \#NLP \#NaturalLangueProcessing \#AI}
}

@misc{InfographieSystemesExploitation,
  title = {{Infographie: Les syst\`emes d'exploitation les plus utilis\'es sur PC}},
  shorttitle = {{Infographie}},
  journal = {Statista Infographies},
  urldate = {2022-07-18},
  abstract = {Ce graphique montre les parts de march\'e des syst\`emes d'exploitation pour PC dans le monde (septembre 2021).},
  howpublished = {https://fr.statista.com/infographie/20455/parts-de-marche-des-systemes-exploitation-pour-ordinateurs-dans-le-monde/},
  langid = {french},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\7QG2HZ3G\\parts-de-marche-des-systemes-exploitation-pour-ordinateurs-dans-le-monde.html}
}

@misc{InfographieSystemesExploitationa,
  title = {{Infographie: Les syst\`emes d'exploitation les plus utilis\'es sur PC}},
  shorttitle = {{Infographie}},
  journal = {Statista Infographies},
  urldate = {2022-07-22},
  abstract = {Ce graphique montre les parts de march\'e des syst\`emes d'exploitation pour PC dans le monde (septembre 2021).},
  howpublished = {https://fr.statista.com/infographie/20455/parts-de-marche-des-systemes-exploitation-pour-ordinateurs-dans-le-monde/},
  langid = {french}
}

@misc{insiderChatbotCOVID19Virtual2020,
  title = {Chatbot {{COVID-19}}, a Virtual Assistant for People with Vision Disabilities},
  author = {Insider, Disability},
  year = {2020},
  month = may,
  journal = {Disability Insider},
  urldate = {2021-09-02},
  abstract = {The accessible chatbot on COVID-19 created by the SemanticBots~company~can be installed free of charge on any website of organizations, associations and companies to facilitate access to information about the pandemic to people with visual disabilities.},
  langid = {english}
}

@misc{IntroducingOurVirtual2023,
  title = {Introducing {{Our Virtual Volunteer Tool}} for {{People}} Who Are {{Blind}} or {{Have Low Vision}}, {{Powered}} by {{OpenAI}}'s {{GPT-4}}},
  year = {2023},
  month = mar,
  urldate = {2023-04-04},
  abstract = {We are thrilled to announce Be My Eyes Virtual Volunteer\texttrademark, the first-ever digital visual assistant powered by OpenAI's new GPT-4 language model.},
  howpublished = {https://www.bemyeyes.com/blog/introducing-be-my-eyes-virtual-volunteer},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\TS9G93S2\\introducing-be-my-eyes-virtual-volunteer.html}
}

@inproceedings{isyantoDesignImplementationIoTBased2020,
  title = {Design and {{Implementation}} of {{IoT-Based Smart Home Voice Commands}} for Disabled People Using {{Google Assistant}}},
  booktitle = {2020 {{International Conference}} on {{Smart Technology}} and {{Applications}} ({{ICoSTA}})},
  author = {Isyanto, Haris and Arifin, Ajib Setyo and Suryanegara, Muhammad},
  year = {2020},
  month = feb,
  pages = {1--6},
  doi = {10.1109/ICoSTA48221.2020.1570613925},
  abstract = {The development of the Internet of Things (IoT) technology has a positive impact on human life. A smart home is one of the IoT technology applications that facilitate human activities. The problem of physical abnormalities is a matter of limited activities for disabled people. This paper proposes a design of IoT-based smart home application with a remote control device that developed using voice commands for disabled people. Smart home control systems help disabled people to control their home electrical devices such as television (TV), lights, and fans using only voice commands without moving to turn on or turn off electrical equipment. The voice recognition on electrical equipment is using the Google Assistant's application on smartphones. The Google Assistant application will accept voice commands when the pronunciation is correct. Voice commands on IoT-based Smart Home are more simple to apply, without typing text messages. Users get convenience compared to using text. The signal strength of an Internet connection will create a useful performance device at the Response Time of Google Assistant, Response Time of System Processing, activating and deactivating Electrical Equipment. We expect that this device could be more useful to help disabled people interact with their environment by utilizing IoT technology facilities.},
  keywords = {disabled people,google assistants,smart home,the internet of things,voice commands}
}

@misc{JAWSFreedomScientific2023,
  title = {{{JAWS}}\textregistered{} \textendash{} {{Freedom Scientific}}},
  year = {2023},
  urldate = {2023-04-07},
  howpublished = {Freedom Scientific},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\8876S3B3\\jaws.html}
}

@misc{JAWSLogicielLecture,
  title = {{JAWS - Logiciel de lecture d'\'ecran avec retour vocale et braille}},
  journal = {Sensotec},
  urldate = {2022-07-25},
  abstract = {JAWS est le logiciel de lecture d'\'ecran le plus utilis\'e sous Windows. JAWS offre un acc\`es vocal complet \`a l'ordinateur pour personnes malvoyantes et aveugles. Un afficheur braille peut-\^etre connect\'e pour une lecture braille.},
  howpublished = {https://sensotec.be/fr/produit/jaws/},
  langid = {french},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\8XWTB65Z\\jaws.html}
}

@misc{jekelClassifyingOnlineDating2018,
  title = {Classifying {{Online Dating Profiles}} on {{Tinder}} Using {{FaceNet Facial Embeddings}}},
  author = {Jekel, Charles F. and Haftka, Raphael T.},
  year = {2018},
  month = mar,
  number = {arXiv:1803.04347},
  eprint = {1803.04347},
  primaryclass = {cs, eess, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1803.04347},
  urldate = {2023-04-04},
  abstract = {A method to produce personalized classification models to automatically review online dating profiles on Tinder is proposed, based on the user's historical preference. The method takes advantage of a FaceNet facial classification model to extract features which may be related to facial attractiveness. The embeddings from a FaceNet model were used as the features to describe an individual's face. A user reviewed 8,545 online dating profiles. For each reviewed online dating profile, a feature set was constructed from the profile images which contained just one face. Two approaches are presented to go from the set of features for each face, to a set of profile features. A simple logistic regression trained on the embeddings from just 20 profiles could obtain a 65\% validation accuracy. A point of diminishing marginal returns was identified to occur around 80 profiles, at which the model accuracy of 73\% would only improve marginally after reviewing a significant number of additional profiles.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Social and Information Networks,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  note = {Comment: 6 pages, 7 figures},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\LGQ3D3W6\\Jekel et Haftka - 2018 - Classifying Online Dating Profiles on Tinder using.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\MBN336FE\\1803.html}
}

@inproceedings{kaleObjectDetectionFace2022,
  title = {Object {{Detection}} and~{{Face Recognition Using Yolo}} and~{{Inception Model}}},
  booktitle = {Advanced {{Network Technologies}} and {{Intelligent Computing}}},
  author = {Kale, Yatharth V. and Shetty, Ashish U. and Patil, Yogeshwar A. and Patil, Rajeshwar A. and Medhane, Darshan V.},
  editor = {Woungang, Isaac and Dhurandher, Sanjay Kumar and Pattanaik, Kiran Kumar and Verma, Anshul and Verma, Pradeepika},
  year = {2022},
  series = {Communications in {{Computer}} and {{Information Science}}},
  pages = {274--287},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-96040-7_22},
  abstract = {The task of facial recognition involves recognising faces in images, while object detection entails determining the location of objects in images. To accomplish this goal, we have developed a model capable of detecting objects as well as recognizing faces. The YOLO (You Only Look Once) model was used to detect objects in the image. If a person is detected by the model, then a cropped image of the person's face is passed to the pre-trained Inception model with additional custom hidden layers. The Inception model is trained on a facial dataset of size 1821 which consists of 5 classes. The Siamese network identifies the person by referring to the database of known people. By adding Siamese network, the framework becomes more scalable and adaptable. The testing accuracy of person recognition is 93.75\%.},
  isbn = {978-3-030-96040-7},
  langid = {english},
  keywords = {Convolutional neural network,Face recognition,Inception,Object detection,Siamese,YOLO},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\PCZS498X\\Kale et al. - 2022 - Object Detection and Face Recognition Using Yolo a.pdf}
}

@book{kanadeComputerRecognitionHuman1977,
  title = {Computer Recognition of Human Faces},
  author = {Kanade, Takeo},
  year = {1977},
  publisher = {{Birkh\"auser Basel}},
  address = {{Basel}},
  doi = {10.1007/978-3-0348-5737-6},
  urldate = {2023-04-06},
  isbn = {978-3-7643-0957-2 978-3-0348-5737-6},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\X74JLI7T\\Kanade - 1977 - Computer recognition of human faces.pdf}
}

@misc{kargwalGatoStepGeneral,
  title = {Gato: {{A Step Towards General AI}} by {{DeepMind}}},
  shorttitle = {Gato},
  author = {Kargwal, Aryan},
  journal = {NimbleBox.ai},
  urldate = {2022-09-01},
  abstract = {DeepMind has just released a new AI system called Gato, a generalist agent. That's a big step forward in the AGI field.},
  howpublished = {https://nimblebox.ai/blog/gato-deepmind},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\IKHCA22W\\gato-deepmind.html}
}

@book{kaurBeMyEyes2017,
  title = {Be {{My Eyes}} : {{Android App}} for Visually Impaired People},
  shorttitle = {Be {{My Eyes}}},
  author = {Kaur, Parminder and Ganore, Mayuri and Doiphode, Rucha and Garud, Ashwini and Ghuge, Tejaswini},
  year = {2017},
  month = apr,
  doi = {10.13140/RG.2.2.12307.48164},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\IKQ2VA9C\\Kaur et al. - 2017 - Be My Eyes  Android App for visually impaired peo.pdf}
}

@article{kaurFacialrecognitionAlgorithmsLiterature2020,
  title = {Facial-Recognition Algorithms: {{A}} Literature Review},
  shorttitle = {Facial-Recognition Algorithms},
  author = {Kaur, Paramjit and Krishan, Kewal and Sharma, Suresh K. and Kanchan, Tanuj},
  year = {2020},
  month = apr,
  journal = {Medicine, Science and the Law},
  volume = {60},
  number = {2},
  pages = {131--139},
  publisher = {{SAGE Publications}},
  issn = {0025-8024},
  doi = {10.1177/0025802419893168},
  urldate = {2022-04-04},
  abstract = {The face is an important part of the human body, distinguishing individuals in large groups of people. Thus, because of its universality and uniqueness, it has become the most widely used and accepted biometric method. The domain of face recognition has gained the attention of many scientists, and hence it has become a standard benchmark in the area of human recognition. It has turned out to be the most deeply studied area in computer vision for more than four decades. It has a wide array of applications, including security monitoring, automated surveillance systems, victim and missing-person identification and so on. This review presents the broad range of methods used for face recognition and attempts to discuss their advantages and disadvantages. Initially, we present the basics of face-recognition technology, its standard workflow, background and problems, and the potential applications. Then, face-recognition methods with their advantages and limitations are discussed. The concluding section presents the possibilities and future implications for further advancing the field.},
  langid = {english},
  keywords = {appearance-based methods,biometrics,computer-based facial recognition,criminalistics,Forensic science,human face,knowledge-based methods}
}

@inproceedings{kaziakhmedovRealworldAttackMTCNN2019,
  title = {Real-World {{Attack}} on {{MTCNN Face Detection System}}},
  booktitle = {2019 {{International Multi-Conference}} on {{Engineering}}, {{Computer}} and {{Information Sciences}} ({{SIBIRCON}})},
  author = {Kaziakhmedov, Edgar and Kireev, Klim and Melnikov, Grigorii and Pautov, Mikhail and Petiushko, Aleksandr},
  year = {2019},
  month = oct,
  pages = {0422--0427},
  doi = {10.1109/SIBIRCON48586.2019.8958122},
  abstract = {Recent studies proved that deep learning approaches achieve remarkable results on face detection task. On the other hand, the advances gave rise to a new problem associated with the security of the deep convolutional neural network models unveiling potential risks of DCNNs based applications. Even minor input changes in the digital domain can result in the network being fooled. It was shown then that some deep learning-based face detectors are prone to adversarial attacks not only in a digital domain but also in the real world. In the paper, we investigate the security of the well-known cascade CNN face detection system - MTCNN and introduce an easily reproducible and a robust way to attack it. We propose different face attributes printed on an ordinary white and black printer and attached either to the medical face mask or to the face directly. Our approach is capable of breaking the MTCNN detector in a realworld scenario.},
  keywords = {adversarial attacks,face detection,MTCNN,physical domain},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\XRLYDAUW\\Kaziakhmedov et al. - 2019 - Real-world Attack on MTCNN Face Detection System.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\EZMCD5MJ\\8958122.html}
}

@book{khanBuildBetterChatbots2018,
  title = {Build {{Better Chatbots}}},
  author = {Khan, Rashid and Das, Anik},
  year = {2018},
  publisher = {{Apress}},
  address = {{Berkeley, CA}},
  doi = {10.1007/978-1-4842-3111-1},
  urldate = {2021-09-03},
  isbn = {978-1-4842-3110-4 978-1-4842-3111-1},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\ST6LPI3F\\Khan et Das - 2018 - Build Better Chatbots.pdf}
}

@misc{kharwalEndEndChatbot2023,
  title = {End to {{End Chatbot}} Using {{Python}} | {{Aman Kharwal}}},
  author = {Kharwal, Aman},
  year = {2023},
  month = mar,
  journal = {thecleverprogrammer},
  urldate = {2023-03-31},
  abstract = {In this article, I will take you through how to create an end-to-end chatbot using Python. End to End Chatbot using Python.},
  howpublished = {https://thecleverprogrammer.com/2023/03/27/end-to-end-chatbot-using-python/},
  langid = {american}
}

@misc{kudoSentencePieceSimpleLanguage2018,
  title = {{{SentencePiece}}: {{A}} Simple and Language Independent Subword Tokenizer and Detokenizer for {{Neural Text Processing}}},
  shorttitle = {{{SentencePiece}}},
  author = {Kudo, Taku and Richardson, John},
  year = {2018},
  month = aug,
  number = {arXiv:1808.06226},
  eprint = {1808.06226},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1808.06226},
  urldate = {2022-11-18},
  abstract = {This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: Accepted as a demo paper at EMNLP2018},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\4TVM8NND\\Kudo et Richardson - 2018 - SentencePiece A simple and language independent s.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\JL89K92K\\1808.html}
}

@misc{kudoSentencePieceSimpleLanguage2018a,
  title = {{{SentencePiece}}: {{A}} Simple and Language Independent Subword Tokenizer and Detokenizer for {{Neural Text Processing}}},
  shorttitle = {{{SentencePiece}}},
  author = {Kudo, Taku and Richardson, John},
  year = {2018},
  month = aug,
  number = {arXiv:1808.06226},
  eprint = {1808.06226},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1808.06226},
  urldate = {2022-11-26},
  abstract = {This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: Accepted as a demo paper at EMNLP2018},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\6VJLW66D\\Kudo et Richardson - 2018 - SentencePiece A simple and language independent s.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\RMMB7VBS\\1808.html}
}

@book{kumarUserGeneratedData2020,
  title = {User {{Generated Data}}: {{Achilles}}' Heel of {{BERT}}},
  shorttitle = {User {{Generated Data}}},
  author = {Kumar, Ankit and Makhija, Piyush and Gupta, Anuj},
  year = {2020},
  month = mar,
  abstract = {Pre-trained language models such as BERT are known to perform exceedingly well on various NLP tasks and have even established new State-Of-The-Art (SOTA) benchmarks for many of these tasks. Owing to its success on various tasks and benchmark datasets, industry practitioners have started to explore BERT to build applications solving industry use cases. These use cases are known to have much more noise in the data as compared to benchmark datasets. In this work we systematically show that when the data is noisy, there is a significant degradation in the performance of BERT. Specifically, we performed experiments using BERT on popular tasks such sentiment analysis and textual similarity. For this we work with three well known datasets - IMDB movie reviews, SST-2 and STS-B to measure the performance. Further, we examine the reason behind this performance drop and identify the shortcomings in the BERT pipeline.},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\HFUCYCGU\\Kumar et al. - 2020 - User Generated Data Achilles' heel of BERT.pdf}
}

@misc{kynapseKySHARENLPComment2020,
  title = {{{kySHARE}} \#6 - {{Le NLP}}, Comment \c{C}a Marche ?},
  author = {{Kynapse}},
  year = {2020},
  urldate = {2021-09-28},
  abstract = {Gaston BIZEL-BIZELLOT, consultant data \& ia vous explique comment fonctionne le Natural Language Processing et ses usages. En ces temps de confinement, dus \`a la crise de Covid-19, retrouvez chaque jour \`a 13h45 nos consultants en direct de leur salon pour comprendre un nouveau concept ! \#data \#nlp \#machinelearning}
}

@misc{larousseDefinitionsChatbotDictionnaire,
  title = {{D\'efinitions : chatbot - Dictionnaire de fran\c{c}ais Larousse}},
  shorttitle = {{D\'efinitions}},
  author = {Larousse, {\'E}ditions},
  urldate = {2021-10-17},
  abstract = {chatbot - D\'efinitions Fran\c{c}ais : Retrouvez la d\'efinition de chatbot... - synonymes, homonymes, difficult\'es, citations.},
  howpublished = {https://www.larousse.fr/dictionnaires/francais/chatbot/188506},
  langid = {french}
}

@misc{LaunchingSpeechCommands,
  title = {Launching the {{Speech Commands Dataset}}},
  journal = {Google AI Blog},
  urldate = {2022-02-07},
  abstract = {Posted by Pete Warden, Software Engineer, Google Brain Team At Google, we're often asked how to get started using deep learning for speech a...},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\A6KA5MR5\\launching-speech-commands-dataset.html}
}

@misc{LectureNaturalLanguage,
  title = {Lecture 7: {{Natural Language Processing}} ({{NLP}}), {{Part}} 1 | {{Lecture Videos}} | {{Machine Learning}} for {{Healthcare}} | {{Electrical Engineering}} and {{Computer Science}} | {{MIT OpenCourseWare}}},
  shorttitle = {Lecture 7},
  urldate = {2021-09-13},
  abstract = {This lecture and the next covers the role of Natural Language Processing in machine learning in healthcare. In this lecture, Prof. Szolovits covers methods which are not based on neural networks representations.},
  howpublished = {https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-s897-machine-learning-for-healthcare-spring-2019/lecture-videos/lecture-7-natural-language-processing-nlp-part-1/},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\DU5QPJ8S\\lecture-7-natural-language-processing-nlp-part-1.html}
}

@article{leeChatbots2020,
  title = {Chatbots},
  author = {Lee, Jang Ho and Yang, Hyejin and Shin, Dongkwang and Kim, Heyoung},
  year = {2020},
  month = aug,
  journal = {ELT Journal},
  volume = {74},
  number = {3},
  pages = {338--344},
  issn = {0951-0893},
  doi = {10.1093/elt/ccaa035},
  urldate = {2022-02-08},
  abstract = {In this series, we explore technology-related themes and topics. This series aims to discuss and demystify what may be new areas for some readers and to consider their relevance for English language teachers.},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\QZYC5KXJ\\5876230.html}
}

@article{liUnsupervisedVisionandLanguagePretraining2021,
  title = {Unsupervised {{Vision-and-Language Pre-training Without Parallel Images}} and {{Captions}}},
  author = {Li, Liunian Harold and You, Haoxuan and Wang, Zhecan and Zareian, Alireza and Chang, Shih-Fu and Chang, Kai-Wei},
  year = {2021},
  month = apr,
  journal = {arXiv:2010.12831 [cs]},
  eprint = {2010.12831},
  primaryclass = {cs},
  urldate = {2022-02-23},
  abstract = {Pre-trained contextual vision-and-language (V\&L) models have achieved impressive performance on various benchmarks. However, existing models require a large amount of parallel image-caption data for pre-training. Such data are costly to collect and require cumbersome curation. Inspired by unsupervised machine translation, we investigate if a strong V\&L representation model can be learned through unsupervised pre-training without image-caption corpora. In particular, we propose to conduct ``mask-and-predict'' pre-training on text-only and image-only corpora and introduce the object tags detected by an object recognition model as anchor points to bridge two modalities. We find that such a simple approach achieves performance close to a model pre-trained with aligned data, on four English V\&L benchmarks. Our work challenges the widely held notion that aligned data is necessary for V\&L pre-training, while significantly reducing the amount of supervision needed for V\&L models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: NAACL 2021 Camera Ready},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\GTED8M8R\\Li et al. - 2021 - Unsupervised Vision-and-Language Pre-training With.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\EAR2K7XE\\2010.html}
}

@incollection{liuSSDSingleShot2016,
  title = {{{SSD}}: {{Single Shot MultiBox Detector}}},
  shorttitle = {{{SSD}}},
  author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  year = {2016},
  volume = {9905},
  eprint = {1512.02325},
  primaryclass = {cs},
  pages = {21--37},
  doi = {10.1007/978-3-319-46448-0_2},
  urldate = {2023-04-06},
  abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For \$300\textbackslash times 300\$ input, SSD achieves 72.1\% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for \$500\textbackslash times 500\$ input, SSD achieves 75.1\% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: ECCV 2016},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\GCRALYAF\\Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\WCYUXC35\\1512.html}
}

@article{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021},
  month = aug,
  journal = {arXiv:2103.14030 [cs]},
  eprint = {2103.14030},
  primaryclass = {cs},
  urldate = {2022-04-01},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbackslash textbf\{S\}hifted \textbackslash textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at\textasciitilde\textbackslash url\{https://github.com/microsoft/Swin-Transformer\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\BZP3QZ77\\Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf;C\:\\Users\\Vincent\\Zotero\\storage\\5BCT59B7\\2103.html}
}

@article{liVisualBERTSimplePerformant2019,
  title = {{{VisualBERT}}: {{A Simple}} and {{Performant Baseline}} for {{Vision}} and {{Language}}},
  shorttitle = {{{VisualBERT}}},
  author = {Li, Liunian Harold and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.03557 [cs]},
  eprint = {1908.03557},
  primaryclass = {cs},
  urldate = {2022-04-01},
  abstract = {We propose VisualBERT, a simple and flexible framework for modeling a broad range of vision-and-language tasks. VisualBERT consists of a stack of Transformer layers that implicitly align elements of an input text and regions in an associated input image with self-attention. We further propose two visually-grounded language model objectives for pre-training VisualBERT on image caption data. Experiments on four vision-and-language tasks including VQA, VCR, NLVR2, and Flickr30K show that VisualBERT outperforms or rivals with state-of-the-art models while being significantly simpler. Further analysis demonstrates that VisualBERT can ground elements of language to image regions without any explicit supervision and is even sensitive to syntactic relationships, tracking, for example, associations between verbs and image regions corresponding to their arguments.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: Work in Progress},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\96SHFQBV\\Li et al. - 2019 - VisualBERT A Simple and Performant Baseline for V.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\7VVIIVT5\\1908.html}
}

@misc{LogicielRevueEcran,
  title = {{Logiciel de revue d'\'ecran JAWS 2022}},
  journal = {Cflou},
  urldate = {2022-07-26},
  abstract = {Logiciel de revue d'\'ecran JAWS permettant la lecture vocale de tout l'environnement Windows et de tous les logiciels. Le logiciel JAWS facilite l'acc\`es \`a l'informatique pour les personnes malvoyantes et non-voyantes. Disponible en version Familiale ou professionnelle avec activation GLI, Dongle ou en t\'el\'echargement. Possibilit\'e d'inclure les 2 prochaines mises \`a jour gratuitement avec l'option SMA.},
  howpublished = {https://www.cflou.com/logiciel-lecteur-d-ecran/1690-logiciel-de-revue-d-ecran-jaws-2019.html},
  langid = {french},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\T8XFTFT7\\1690-logiciel-de-revue-d-ecran-jaws-2019.html}
}

@misc{LogicielsInformatiquesPour,
  title = {Logiciels Informatiques Pour Malvoyants et Aveugles - {{Cflou}} - {{Cflou}}},
  urldate = {2022-07-22},
  howpublished = {https://www.cflou.com/125-logiciel-informatique-malvoyant}
}

@misc{Longformer2023,
  title = {Longformer},
  year = {2023},
  month = apr,
  urldate = {2023-04-05},
  abstract = {Longformer: The Long-Document Transformer},
  copyright = {Apache-2.0},
  howpublished = {AI2}
}

@misc{LookoutVisionAssistee2022,
  title = {Lookout - {{Vision}} Assist\'ee \textendash{} {{Applications}} Sur {{Google Play}}},
  year = {2022},
  month = may,
  urldate = {2023-04-04},
  abstract = {Un assistant qui aide les personnes avec une perte de vision \`a explorer le monde},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\D7PPAEVM\\details.html}
}

@article{luFaceDetectionRecognition2021,
  title = {Face {{Detection}} and {{Recognition Algorithm}} in {{Digital Image Based}} on {{Computer Vision Sensor}}},
  author = {Lu, Di and Yan, Limin},
  year = {2021},
  month = sep,
  journal = {Journal of Sensors},
  volume = {2021},
  pages = {e4796768},
  publisher = {{Hindawi}},
  issn = {1687-725X},
  doi = {10.1155/2021/4796768},
  urldate = {2022-04-04},
  abstract = {With the continuous innovation of network technology, various kinds of convenient network technologies have grown, and human dependence on network technology has gradually increased, which has resulted in the importance of network information security issues. With the continuous development of my country's industrialization, the application of sensors is becoming more and more extensive, for example, the security vulnerabilities and defects in the operating system itself. Traditional sensors can ``perceive'' a certain thing or signal, convert it into an electrical signal and record it, and then use a conversion circuit to output the electrical signal into a value or other display form that is conducive to observation. Nowadays, sensors have been further developed. Based on the original ``perception'' function, combined with computer technology, it integrates data storage, data processing, data communication, and other functions, so that it has analysis functions and can better display information. The technical level has reached a new level. Early intelligent recognition mainly used the uniqueness of finger and palm lines to scan and contrast, but due to some weather reasons or skin texture constraints caused by skin texture, these methods showed certain limitations. This paper proposes a new computer vision-based algorithm from face detection technology and face recognition technology. In the face detection technology, it is mainly introduced from the OpenCV method. Face recognition technology is improved in practical applications through the Seetaface method and YouTu method. At the same time, using the contrast experiment, the detection and recognition rates under the three different requirements of side face detection, occlusion detection, and facial exaggerated expression are compared, and the accuracy of each method is improved. The results show that each case is compared in each case. The advantages and disadvantages of the algorithm effectively verify the effectiveness of the method.},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\R3T33IAI\\Lu et Yan - 2021 - Face Detection and Recognition Algorithm in Digita.pdf}
}

@book{lugrinHandbookSociallyInteractive2021,
  title = {The {{Handbook}} on {{Socially Interactive Agents}}: 20 Years of {{Research}} on {{Embodied Conversational Agents}}, {{Intelligent Virtual Agents}}, and {{Social Robotics Volume}} 1: {{Methods}}, {{Behavior}}, {{Cognition}}},
  shorttitle = {The {{Handbook}} on {{Socially Interactive Agents}}},
  editor = {Lugrin, Birgit and Pelachaud, Catherine and Traum, David},
  year = {2021},
  month = sep,
  edition = {First},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3477322},
  urldate = {2022-09-01},
  isbn = {978-1-4503-8720-0},
  langid = {english}
}

@misc{LXMERT,
  title = {{{LXMERT}}},
  urldate = {2022-03-21},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/docs/transformers/model\_doc/lxmert},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\ZYIQ3XMT\\lxmert.html}
}

@article{macedoConversationalAgentMHealth2019,
  title = {Conversational {{Agent}} in {{mHealth}} to Empower People Managing the {{Parkinson}}'s {{Disease}}},
  author = {Macedo, Patr{\'i}cia and Pereira, Carla and Mota, Pedro and Silva, Daniela and Frade, Ana and Madeira, Rui Neves},
  year = {2019},
  month = jan,
  journal = {Procedia Computer Science},
  series = {The 10th {{International Conference}} on {{Emerging Ubiquitous Systems}} and {{Pervasive Networks}} ({{EUSPN-2019}}) / {{The}} 9th {{International Conference}} on {{Current}} and {{Future Trends}} of {{Information}} and {{Communication Technologies}} in {{Healthcare}} ({{ICTH-2019}}) / {{Affiliated Workshops}}},
  volume = {160},
  pages = {402--408},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2019.11.074},
  urldate = {2021-09-02},
  abstract = {ONParkinson is an integrated platform that aims to provide smart assistance towards the empowerment of the triad comprised of people with Parkinson's disease, their caregivers and healthcare professionals. Initial studies have shown the importance of providing patients and their caregivers with accessible and credible information to allow them to deal with clinical issues. Over the last two decades, a substantial body of evidence has shown the potential benefits of using embodied conversational agents for health-related purposes. This paper presents the design of a conversational agent, which is being integrated into the ONParkinson's mobile healthcare app, aiming to respond to questions about Parkinson's disease posed by people with Parkinson's disease and their caregivers. A detailed evaluation plan that comprises technological performance, user experience, and health research is also presented.},
  langid = {english},
  keywords = {Chatbot,Conversational agent,IBM Watson,Informal Caregivers,mHealth,Parkinson's Disease,Self-management},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\5ZNRZFEL\\Macedo et al. - 2019 - Conversational Agent in mHealth to empower people .pdf}
}

@misc{MachineLearningHealthcare,
  title = {Machine {{Learning}} for {{Healthcare}}},
  journal = {MIT OpenCourseWare},
  urldate = {2021-09-13},
  abstract = {This course introduces students to machine learning in healthcare, including the nature of clinical data and the use of machine learning for risk stratification, disease progression modeling, precision medicine, diagnosis, subtype discovery, and improving clinical workflows.},
  howpublished = {https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-s897-machine-learning-for-healthcare-spring-2019/},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\PDY3VELP\\6-s897-machine-learning-for-healthcare-spring-2019.html}
}

@misc{mannellyHowBuildGPT32021,
  title = {How {{To Build}} a {{GPT-3 Chatbot}} with {{Python}}},
  author = {Mannelly, John},
  year = {2021},
  month = may,
  journal = {Medium},
  urldate = {2022-01-14},
  abstract = {Learn With Jabe: Create a messaging chatbot using OpenAI's GPT-3},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\JA58GCXL\\how-to-build-a-gpt-3-chatbot-with-python-7b83e55805e6.html}
}

@inproceedings{mcintireMethodsChatbotDetection2010,
  title = {Methods for Chatbot Detection in Distributed Text-Based Communications},
  booktitle = {2010 {{International Symposium}} on {{Collaborative Technologies}} and {{Systems}}},
  author = {McIntire, John P. and McIntire, Lindsey K. and Havig, Paul R.},
  year = {2010},
  pages = {463--472},
  publisher = {{IEEE}},
  address = {{Chicago, IL, USA}},
  doi = {10.1109/CTS.2010.5478478},
  urldate = {2022-02-07},
  abstract = {At present there are many products/services available and there are large number of consumers that use those products/services. It is not physically possible for every service provider to help the consumers personally if they have any problem while using the service. So, many companies are starting to provide chatbots to automate the communication with the people using computers to interact with the people. Using Artificial Intelligence chatbot tries to mimic human during the conversation. Variety of techniques are involved in the design and development of chatbots. This paper shows the design and working of a few chatbots and the approaches available for their development.},
  isbn = {978-1-4244-6619-1},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\TU4A3XJM\\McIntire et al. - 2010 - Methods for chatbot detection in distributed text-.pdf}
}

@article{mcstayEmotionalAISoft2020,
  title = {Emotional {{AI}}, Soft Biometrics and the Surveillance of Emotional Life: {{An}} Unusual Consensus on Privacy},
  shorttitle = {Emotional {{AI}}, Soft Biometrics and the Surveillance of Emotional Life},
  author = {McStay, Andrew},
  year = {2020},
  month = jan,
  journal = {Big Data \& Society},
  volume = {7},
  number = {1},
  pages = {2053951720904386},
  publisher = {{SAGE Publications Ltd}},
  issn = {2053-9517},
  doi = {10.1177/2053951720904386},
  urldate = {2023-04-07},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\ZKXDESJ9\\McStay - 2020 - Emotional AI, soft biometrics and the surveillance.pdf}
}

@misc{MegatronTuringNaturalLanguage2022,
  title = {Megatron-{{Turing Natural Language Generation}}},
  year = {2022},
  month = mar,
  journal = {NVIDIA Developer},
  urldate = {2022-09-02},
  abstract = {Megatron-Turing Natural Language Generation Megatron-Turing Natural Language Generation model (MT-NLG), is the largest and the most powerful monolithic transformer English language model with 530 billion parameters. This 105-layer, transformer-based MT-NLG improves upon the prior state-of-the-art models in zero-, one-, and few-shot settings. It demonstrates unmatched accuracy in a broad set of natural language tasks such as, Completion prediction, Reading comprehension, Commonsense reasoning, Natural language inferences, Word sense disambiguation, etc.},
  howpublished = {https://developer.nvidia.com/megatron-turing-natural-language-generation},
  langid = {american},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\29GSAFNM\\megatron-turing-natural-language-generation.html}
}

@article{merdivanDialogueSystemsIntelligent2019,
  title = {Dialogue {{Systems}} for {{Intelligent Human Computer Interactions}}},
  author = {Merdivan, Erinc and Singh, Deepika and Hanke, Sten and Holzinger, Andreas},
  year = {2019},
  month = may,
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {The Proceedings of {{AmI}}, the 2018 {{European Conference}} on {{Ambient Intelligence}}.},
  volume = {343},
  pages = {57--71},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2019.04.010},
  urldate = {2021-09-02},
  abstract = {The most fundamental communication mechanism for interaction is dialogues involving speech, gesture, semantic and pragmatic knowledge. Various researches on dialogue management have been conducted focusing on standardized model for goal oriented applications using machine learning and deep learning models. The paper presents the overview on existing methods for dialogue manager training; their advantages and limitations. Furthermore, a new image-based method is used in Facebook bAbI Task 1 dataset in Out Of Vocabulary setting. The results show that using dialogue as an image performs well and helps dialogue manager in expanding out of vocabulary dialogue tasks in comparison to Memory Networks.},
  langid = {english},
  keywords = {chatbots,dialogue system,image-based method},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\673DDSYM\\Merdivan et al. - 2019 - Dialogue Systems for Intelligent Human Computer In.pdf}
}

@inproceedings{michaelyKeywordSpottingGoogle2017,
  title = {Keyword Spotting for {{Google}} Assistant Using Contextual Speech Recognition},
  booktitle = {2017 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  author = {Michaely, Assaf Hurwitz and Zhang, Xuedong and Simko, Gabor and Parada, Carolina and Aleksic, Petar},
  year = {2017},
  month = dec,
  pages = {272--278},
  doi = {10.1109/ASRU.2017.8268946},
  abstract = {We present a novel keyword spotting (KWS) system that uses contextual automatic speech recognition (ASR). For voice-activated devices, it is common that a KWS system is run on the device in order to quickly detect a trigger phrase (e.g. ``Ok Google''). After the trigger phrase is detected, the audio corresponding to the voice command that follows is streamed to the server. The audio is transcribed by the server-side ASR system and semantically processed to generate a response which is sent back to the device. Due to limited resources on the device, the device KWS system might introduce false accepts (FA) and false rejects (FR) that can cause an unsatisfactory user experience. We describe a system that uses server-side contextual ASR and trigger phrase non-terminals to improve overall KWS accuracy. We show that this approach can significantly reduce the FA rate (by 89\%) while minimally increasing the FR rate (by 0.2\%). Furthermore, we show that this system significantly improves the ASR quality, reducing Word Error Rate (WER) (by 10\% to 50\% relative), and allows the user to speak seamlessly, without pausing between the trigger phrase and the voice command.},
  keywords = {Adaptation models,Decoding,Error analysis,Google,keyword spotting,Servers,speech recognition,Speech recognition,Task analysis}
}

@misc{microsoftmechanicsQuEstceQui2023,
  title = {Qu'est-Ce Qui Ex\'ecute {{ChatGPT}}~? {{\`A}} l'int\'erieur Du Supercalculateur {{IA}} de {{Microsoft}}},
  shorttitle = {Qu'est-Ce Qui Ex\'ecute {{ChatGPT}}~?},
  author = {{Microsoft Mechanics}},
  year = {2023},
  month = may,
  urldate = {2023-05-27},
  abstract = {D\'ecouvrez de l'int\'erieur l'infrastructure de supercalculateur d'IA con\c{c}ue pour ex\'ecuter ChatGPT et d'autres grands mod\`eles de langage, et d\'ecouvrez comment l'exploiter pour vos charges de travail dans Azure, \`a n'importe quelle \'echelle.  Entrez dans les coulisses :  -Pour la fa\c{c}on dont nous avons collabor\'e avec NVIDIA pour fournir une infrastructure d'IA sp\'ecialement con\c{c}ue avec les GPU NVIDIA  -Comment les points de contr\^ole de Project Forge fonctionnent pour restaurer les \'etats des t\^aches si une longue t\^ache de formation \'echoue ou doit \^etre migr\'ee  -Comment nous avons utilis\'e le r\'eglage fin de LoRA pour mettre \`a jour une fraction du mod\`ele de base pour plus de d\'ebit de formation et des points de contr\^ole plus petits  -Comment la soci\'et\'e bas\'ee au Royaume-Uni, Wayve, utilise l'infrastructure de supercalculateur AI d'Azure pour les voitures autonomes  -Et comment Confidential Computing fonctionne avec Azure AI pour combiner des ensembles de donn\'ees sans partager d'informations personnellement identifiables pour des collaborations multipartites s\'ecuris\'ees.  Mark Russinovich, Azure CTO, rejoint Jeremy Chapman pour le d\'ecomposer.  {$\blackpointerright$} LIENS RAPIDES~:  00:00 - Pr\'esentation  01:15 - Innovation IA cr\'eant du mat\'eriel et des logiciels sp\'ecialis\'es  04:22 - Optimisation du mat\'eriel~: InfiniBand et NVIDIA  05:40 - Am\'elioration du d\'ebit  06:17 - Projet Forge  08:01 - D\'emo de points de contr\^ole de Project Forge  10:02 - R\'eglage fin de LoRA  11:29 - Utilisez l'infrastructure de supercalculateurs IA pour vos charges de travail  12:34 - Comment Wayve tire parti de l'infrastructure de supercalculateurs IA  \hspace{0pt}\hspace{0pt}13:47 - Fonctionnement de l'informatique confidentielle avec Azure AI  15:21 - Conclusion  {$\blackpointerright$} R\'ef\'erences de liens~:  Tirez parti des capacit\'es Azure AI pour vous-m\^eme sur https://aka.ms/AzureAIInfrastructure  {$\blackpointerright$} Vous n'\^etes pas familier avec Microsoft Mechanics ?  En tant que s\'erie de vid\'eos officielles de Microsoft pour l'informatique, vous pouvez regarder et partager du contenu pr\'ecieux et des d\'emonstrations de la technologie actuelle et \`a venir des personnes qui l'ont cr\'e\'ee chez Microsoft.  \textbullet{} Abonnez-vous \`a notre YouTube~: ~~~/~microsoftmechanic...~~  \textbullet{} Discutez avec d'autres professionnels de l'informatique, rejoignez-nous sur la communaut\'e technique Microsoft~: https://techcommunity.microsoft.com/t...  \textbullet{} Regardez ou \'ecoutez o\`u que vous soyez, abonnez-vous \`a notre podcast~: https://microsoftmechanics.libsyn.com...  {$\blackpointerright$} Continuez \`a acqu\'erir ces connaissances d'initi\'es, rejoignez-nous sur les r\'eseaux sociaux~:  \textbullet{} Suivez-nous sur Twitter~: https://twitter.com/MSFTMechanics  \textbullet{} Partagez vos connaissances sur LinkedIn~: https://www.linkedin.com/company/micr...  \textbullet{} Profitez-en sur Instagram~: https://www.instagram.com/msftmechanics/  \textbullet{} D\'etendez-vous avec nous sur TikTok~: https://www.tiktok.com/@msftmechanics  \#Azure \#OpenAI \#Supercomputer \#LLM}
}

@misc{mipsoftBlindSquare2023,
  title = {{BlindSquare}},
  author = {MIPsoft},
  year = {2023},
  month = mar,
  journal = {App~Store},
  urldate = {2022-07-22},
  abstract = {Blindsquare donne un sens \`a ce qui vous entoure. La seule chose que vous avez \`a faire est d'\'ecouter. Blindsquare est une nouvelle solution qui combine les derni\`eres technologies pour aider les non-voyants dans leur vie quotidienne. Blindsquare \`a \'et\'e d\'evelopp\'e en collaboration avec des non-voyants et\ldots},
  howpublished = {https://apps.apple.com/fr/app/blindsquare/id500557255},
  langid = {french},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\MB9K6TK5\\id500557255.html}
}

@misc{mitopencoursewareNaturalLanguageProcessing2020,
  title = {8. {{Natural Language Processing}} ({{NLP}}), {{Part}} 2},
  author = {{MIT OpenCourseWare}},
  year = {2020},
  month = oct,
  urldate = {2021-09-06},
  abstract = {MIT 6.S897 Machine Learning for Healthcare, Spring 2019 Instructor: Peter Szolovits View the complete course: https://ocw.mit.edu/6-S897S19 YouTube Playlist: https://www.youtube.com/playlist?list... Prof. Szolovits covers Natural Language Processing methods including those which are not based on neural networks representations and those techniques which employ neural network architectures. License: Creative Commons BY-NC-SA More information at https://ocw.mit.edu/terms More courses at https://ocw.mit.edu}
}

@misc{MobileOperatingSystem,
  title = {Mobile {{Operating System Market Share Worldwide}}},
  journal = {StatCounter Global Stats},
  urldate = {2022-07-14},
  abstract = {This graph shows the market share of mobile operating systems worldwide based on over 10 billion monthly page views.},
  howpublished = {https://gs.statcounter.com/os-market-share/mobile/worldwide},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\559BK78G\\worldwide.html}
}

@misc{mooreCannyApproachSpoken2019,
  title = {A '{{Canny}}' {{Approach}} to {{Spoken Language Interfaces}}},
  author = {Moore, Roger K.},
  year = {2019},
  month = aug,
  number = {arXiv:1908.08131},
  eprint = {1908.08131},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-09-01},
  abstract = {Voice-enabled artefacts such as Amazon Echo are very popular, but there appears to be a 'habitability gap' whereby users fail to engage with the full capabilities of the device. This position paper draws a parallel with the 'uncanny valley' effect, thereby proposing a solution based on aligning the visual, vocal, behavioural and cognitive affordances of future voice-enabled devices.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Human-Computer Interaction},
  note = {Comment: Presented at the CHI 2019 Workshop on Mapping Theoretical and Methodological Perspectives for Understanding Speech Interface Interactions, 4-9 May 2019, Glasgow, UK},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\YB3J7IR8\\Moore - 2019 - A 'Canny' Approach to Spoken Language Interfaces.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\536PTVNT\\1908.html}
}

@misc{naczykEqla2020,
  title = {{Eqla}},
  author = {Naczyk, Rafal},
  year = {2020},
  month = oct,
  journal = {Eqla},
  urldate = {2022-09-02},
  abstract = {Changeons le quotidien des personnes aveugles et malvoyantes},
  howpublished = {https://eqla.be},
  langid = {french},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\VE82D8GN\\grande-enquete-2020-malvoyance-cecite.html}
}

@misc{narayananGPT4ProfessionalBenchmarks2023,
  type = {Substack Newsletter},
  title = {{{GPT-4}} and Professional Benchmarks: The Wrong Answer to the Wrong Question},
  shorttitle = {{{GPT-4}} and Professional Benchmarks},
  author = {Narayanan, Arvind and Kapoor, Sayash},
  year = {2023},
  month = mar,
  journal = {AI Snake Oil},
  urldate = {2023-03-23},
  abstract = {OpenAI may have tested on the training data. Besides, human benchmarks are meaningless for bots.},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\C77HAVFD\\gpt-4-and-professional-benchmarks.html}
}

@misc{NaturalLanguageInteraction,
  title = {Natural {{Language Interaction Style}} | Ipl.Org},
  urldate = {2022-02-10},
  howpublished = {https://www.ipl.org/essay/Communication-Style-Of-Interaction-FJZQUL9ERG},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\SGZ4M29H\\Communication-Style-Of-Interaction-FJZQUL9ERG.html}
}

@article{NaturallanguageUserInterface2021,
  title = {Natural-Language User Interface},
  year = {2021},
  month = nov,
  journal = {Wikipedia},
  urldate = {2022-02-10},
  abstract = {Natural-language user interface (LUI or NLUI) is a type of computer human interface where linguistic phenomena such as verbs, phrases and clauses act as UI controls for creating, selecting and modifying data in software applications. In interface design, natural-language interfaces are sought after for their speed and ease of use, but most suffer the challenges to understanding wide varieties of ambiguous input. Natural-language interfaces are an active area of study in the field of natural-language processing and computational linguistics. An intuitive general natural-language interface is one of the active goals of the Semantic Web. Text interfaces are "natural" to varying degrees. Many formal (un-natural) programming languages incorporate idioms of natural human language. Likewise, a traditional keyword search engine could be described as a "shallow" natural-language user interface.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1053627679},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\DQD55XSK\\Natural-language_user_interface.html}
}

@misc{NebullvmAppsAccelerate,
  title = {Nebullvm/Apps/Accelerate/Chatllama at Main {$\cdot$} Nebuly-Ai/Nebullvm},
  journal = {GitHub},
  urldate = {2023-03-31},
  abstract = {Plug and play modules to optimize the performances of your AI systems 🚀 - nebullvm/apps/accelerate/chatllama at main {$\cdot$} nebuly-ai/nebullvm},
  howpublished = {https://github.com/nebuly-ai/nebullvm},
  langid = {english}
}

@misc{neuralnineIntelligentAIChatbot2020,
  title = {Intelligent {{AI Chatbot}} in {{Python}}},
  author = {{NeuralNine}},
  year = {2020},
  month = dec,
  urldate = {2022-01-13},
  abstract = {In today's video, we are going to build an intelligent AI chatbot using neural networks and natural language processing in Python. {$\mdsmblksquare\mdsmblksquare\mdsmblksquare\mdsmblksquare\mdsmblksquare\mdsmblksquare\mdsmblksquare\mdsmblksquare\mdsmblksquare\mdsmblksquare\mdsmblksquare\mdsmblksquare\mdsmblksquare\mdsmblksquare\mdsmblksquare\mdsmblksquare\mdsmblksquare$} 📚 Programming Books \& Merch 📚 💻 The Algorithm Bible Book: https://www.neuralnine.com/books/ 🐍 The Python Bible Book: https://www.neuralnine.com/books/ 👕 Programming Merch: https://www.neuralnine.com/shop 🌐 Social Media \& Contact 🌐  📱 Website: https://www.neuralnine.com/ 📷 Instagram: https://www.instagram.com/neuralnine 🐦 Twitter: https://twitter.com/neuralnine 🤵 LinkedIn: https://www.linkedin.com/company/neur... 📁 GitHub: https://github.com/NeuralNine  🎵 Outro Music From: https://www.bensound.com/ Timestamps (0:00) Intro (0:18) General Structure (2:16) Setting Up Intents (6:48) Load Training Data (13:35) Prepare Training Data (19:40) Build Neural Network (24:10) Train Neural Network (24:35) Build Chatbot (33:34) Demonstration (35:08) Outro}
}

@misc{NosSolutionsDigitales,
  title = {Nos Solutions Digitales},
  urldate = {2022-02-15},
  abstract = {Nous d\'eveloppons des solutions de mobilit\'e digitales afin d'am\'eliorer l'autonomie des personnes en situation de handicap et de faciliter leur quotidien},
  howpublished = {https://ezymob.fr/nos-solutions},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\9DVAPZLK\\nos-solutions.html}
}

@article{NoTitleFound,
  title = {[{{No}} Title Found]},
  journal = {International Journal of Innovative Science and Research Technology},
  issn = {24562165},
  abstract = {This paper consists survey of some efficient face detection algorithms. Automatic face detection is a very complex problem in image processing and many methods and algorithms have been proposed for the same. Detection of faces is done efficiently from images, video and live video streaming using face detection algorithms.There are many techniques for face detection. This survey paper includes a general review on different face detection techniques. We have tried to explain a few particularly important algorithms like Cascade Classifier, Dlib CNN, Dlib HOG, MTCNN for face detection with their working, characteristics, advantages and disadvantages.},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\CXHP4995\\[No title found].pdf}
}

@misc{NVAccess2017,
  title = {{{NV Access}}},
  year = {2017},
  month = jul,
  urldate = {2023-04-07},
  abstract = {Visit the post for more.},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\4KPV3JVU\\support-us.html}
}

@misc{NVDA2022,
  title = {{{NVDA}}},
  year = {2022},
  month = may,
  urldate = {2022-05-28},
  abstract = {NVDA, the free and open source Screen Reader for Microsoft Windows},
  howpublished = {NV Access}
}

@misc{NVDA2022a,
  title = {{{NVDA}}},
  year = {2022},
  month = jul,
  urldate = {2022-07-22},
  abstract = {NVDA, the free and open source Screen Reader for Microsoft Windows},
  howpublished = {NV Access},
  keywords = {accessibility,blind,screen-reader}
}

@article{okonkwoChatbotsApplicationsEducation2021,
  title = {Chatbots Applications in Education: {{A}} Systematic Review},
  shorttitle = {Chatbots Applications in Education},
  author = {Okonkwo, Chinedu Wilfred and {Ade-Ibijola}, Abejide},
  year = {2021},
  month = jan,
  journal = {Computers and Education: Artificial Intelligence},
  volume = {2},
  pages = {100033},
  issn = {2666-920X},
  doi = {10.1016/j.caeai.2021.100033},
  urldate = {2022-02-07},
  abstract = {The introduction of Artificial Intelligence technology enables the integration of Chatbot systems into various aspects of education. This technology is increasingly being used for educational purposes. Chatbot technology has the potential to provide quick and personalised services to everyone in the sector, including institutional employees and students. This paper presents a systematic review of previous studies on the use of Chatbots in education. A systematic review approach was used to analyse 53 articles from recognised digital databases. The review results provide a comprehensive understanding of prior research related to the use of Chatbots in education, including information on existing studies, benefits, and challenges, as well as future research areas on the implementation of Chatbot technology in the field of education. The implications of the findings were discussed, and suggestions were made.},
  langid = {english},
  keywords = {Artificial intelligence,Benefits of chatbots,Challenges of chatbots,Chatbots in education,Systematic review},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\L29F63PT\\Okonkwo et Ade-Ibijola - 2021 - Chatbots applications in education A systematic r.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\A4K4Z23P\\S2666920X21000278.html}
}

@article{oliveiraMultiAgentInteractionAssist2022,
  title = {Multi-{{Agent Interaction}} to {{Assist Visually-Impaired}} and {{Elderly People}}},
  author = {Oliveira, Juliana Damasio and Engelmann, Debora C. and Kniest, Davi and Vieira, Renata and Bordini, Rafael H.},
  year = {2022},
  month = jan,
  journal = {International Journal of Environmental Research and Public Health},
  volume = {19},
  number = {15},
  pages = {8945},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1660-4601},
  doi = {10.3390/ijerph19158945},
  urldate = {2022-09-08},
  abstract = {A voice-controlled smart home system based on conversational agents can address the specific needs of older people, proactively providing support, compensating for cognitive decline, and coping with solitude, among other features. In particular, Multi-Agent Systems (MAS) platforms provide considerable support for complex adaptive systems that are naturally distributed and situated in dynamic environments, such as Ambient intelligence (AmI) applications. Such autonomous intelligent agents are capable of independent reasoning and joint analysis of complex situations to support high-level interaction with humans, besides providing typical characteristics of MAS, such as cooperation and coordinated action. In this context, we developed an approach using a MAS previously evaluated for visually impaired users, where most of the system's functionalities are also helpful for the elderly. Our methodology is based on the four steps of the interactive design process. As a result, we determined that our approach has elements that allow for natural interaction with users, and we identified and discussed improvements and new features for future work. We believe that our findings can point to directions for building AmI systems that are capable of more natural interaction with users.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {daily living assistance,elderly care,multi-agent systems,visual impairment assistance},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\TXVKVK8U\\Oliveira et al. - 2022 - Multi-Agent Interaction to Assist Visually-Impaire.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\H9EQQ5M2\\htm.html}
}

@misc{OOrion2023,
  title = {{OOrion}},
  year = {2023},
  month = mar,
  journal = {App~Store},
  urldate = {2023-04-04},
  abstract = {OOrion est une application destin\textbackslash 'ee aux d\textbackslash 'eficients visuels.  Vous ne trouvez pas vos cl\textbackslash 'es ? Vous cherchez une prise \textbackslash 'electrique ? Une chaise libre ? Vous souhaitez simplement vous faire une id\textbackslash 'ee des diff\textbackslash 'erents objets pr\textbackslash 'esents autour de vous ? Ou encore trouver du texte ? OOrion est l\textbackslash `a pour vous aider\textbackslash ldots},
  howpublished = {https://apps.apple.com/fr/app/oorion/id1567957213},
  langid = {french},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\L3FH6UBW\\id1567957213.html}
}

@misc{openaiGPT4TechnicalReport2023,
  title = {{{GPT-4 Technical Report}}},
  author = {OpenAI},
  year = {2023},
  month = mar,
  number = {arXiv:2303.08774},
  eprint = {2303.08774},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.08774},
  urldate = {2023-04-05},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  note = {Comment: 100 pages},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\B2HU8WCE\\OpenAI - 2023 - GPT-4 Technical Report.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\F3QA837K\\2303.html}
}

@misc{OpenCVDetectByAudioMain,
  title = {Open-{{CV}}/{{DetectByAudio}} at Main {$\cdot$} Feitgemel/{{Open-CV}}},
  journal = {GitHub},
  urldate = {2023-03-31},
  abstract = {Open CV Python code examples. Contribute to feitgemel/Open-CV development by creating an account on GitHub.},
  howpublished = {https://github.com/feitgemel/Open-CV},
  langid = {english}
}

@misc{ouyangTrainingLanguageModels2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  year = {2022},
  month = mar,
  number = {arXiv:2203.02155},
  eprint = {2203.02155},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.02155},
  urldate = {2023-04-04},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\7ACDG7MI\\Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\WCQUX5IY\\2203.html}
}

@misc{PapersCodeATIS,
  title = {Papers with {{Code}} - {{ATIS Dataset}}},
  urldate = {2022-10-27},
  abstract = {The ATIS (Airline Travel Information Systems) is a dataset consisting of audio recordings and corresponding manual transcripts about humans asking for flight information on automated airline travel inquiry systems. The data consists of 17 unique intent categories. The original split contains 4478, 500 and 893 intent-labeled reference utterances in train, development and test set respectively.},
  howpublished = {https://paperswithcode.com/dataset/atis},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\6Y2ZKWTQ\\atis.html}
}

@misc{PapersCodeBimodel,
  title = {Papers with {{Code}} - {{A Bi-model}} Based {{RNN Semantic Frame Parsing Model}} for {{Intent Detection}} and {{Slot Filling}}},
  urldate = {2022-11-27},
  abstract = {🏆 SOTA for Slot Filling on THU-Bi-Hand (F1-score metric)},
  howpublished = {https://paperswithcode.com/paper/a-bi-model-based-rnn-semantic-frame-parsing},
  langid = {english}
}

@misc{PapersCodeGoogle,
  title = {Papers with {{Code}} - {{Google Speech Commands Benchmark}} ({{Keyword Spotting}})},
  urldate = {2022-03-01},
  abstract = {The current state-of-the-art on Google Speech Commands is TripletLoss-res15. See a full comparison of 35 papers with code.},
  howpublished = {https://paperswithcode.com/sota/keyword-spotting-on-google-speech-commands},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\AK5NQ9WW\\keyword-spotting-on-google-speech-commands.html}
}

@misc{PapersCodeGPT2,
  title = {Papers with {{Code}} - {{GPT-2 Explained}}},
  urldate = {2022-09-02},
  abstract = {GPT-2 is a Transformer architecture that was notable for its size (1.5 billion parameters) on its release. The model is pretrained on a WebText dataset - text from 45 million website links. It largely follows the previous GPT architecture with some modifications: Layer normalization is moved to the input of each sub-block, similar to a pre-activation residual network and an additional layer normalization was added after the final self-attention block.  A modified initialization which accounts for the accumulation on the residual path with model depth is used. Weights of residual layers are scaled at initialization by a factor of \$1/\textbackslash sqrt\{N\}\$ where \$N\$ is the number of residual layers.  The vocabulary is expanded to 50,257. The context size is expanded from 512 to 1024 tokens and a larger batch size of 512 is used.},
  howpublished = {https://paperswithcode.com/method/gpt-2},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\FJ7R4U3U\\gpt-2.html}
}

@misc{PapersCodeSNIPS,
  title = {Papers with {{Code}} - {{SNIPS Dataset}}},
  urldate = {2022-11-25},
  abstract = {The SNIPS Natural Language Understanding benchmark is a dataset of over 16,000 crowdsourced queries distributed among 7 user intents of various complexity: SearchCreativeWork (e.g. Find me the I, Robot television show), GetWeather (e.g. Is it windy in Boston, MA right now?), BookRestaurant (e.g. I want to book a highly rated restaurant in Paris tomorrow night), PlayMusic (e.g. Play the last track from Beyonc\'e off Spotify), AddToPlaylist (e.g. Add Diamonds to my roadtrip playlist), RateBook (e.g. Give 6 stars to Of Mice and Men), SearchScreeningEvent (e.g. Check the showtimes for Wonder Woman in Paris). The training set contains of 13,084 utterances, the validation set and the test set contain 700 utterances each, with 100 queries per intent.},
  howpublished = {https://paperswithcode.com/dataset/snips},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\GA44UVQZ\\snips.html}
}

@misc{PapersCodeT5,
  title = {Papers with {{Code}} - {{T5 Explained}}},
  urldate = {2022-09-02},
  abstract = {T5, or Text-to-Text Transfer Transformer, is a Transformer based architecture that uses a text-to-text approach. Every task \textendash{} including translation, question answering, and classification \textendash{} is cast as feeding the model text as input and training it to generate some target text. This allows for the use of the same model, loss function, hyperparameters, etc. across our diverse set of tasks. The changes compared to BERT include: adding a causal decoder to the bidirectional architecture. replacing the fill-in-the-blank cloze task with a mix of alternative pre-training tasks.},
  howpublished = {https://paperswithcode.com/method/t5},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\NIFEQKCI\\t5.html}
}

@misc{PapersCodeTransformerCapsule,
  title = {Papers with {{Code}} - {{Transformer-Capsule Model}} for {{Intent Detection}}},
  urldate = {2022-11-27},
  abstract = {🏆 SOTA for Intent Detection on ATIS (Accuracy metric)},
  howpublished = {https://paperswithcode.com/paper/transformer-capsule-model-for-intent},
  langid = {english}
}

@misc{PapersCodeVisual,
  title = {Papers with {{Code}} - {{Visual Question Answering}}},
  urldate = {2022-03-22},
  abstract = {**Visual Question Answering** is a semantic task that aims to answer questions based on an image. Image Source: [visualqa.org](https://visualqa.org/)},
  howpublished = {https://paperswithcode.com/task/visual-question-answering},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\E6VWATXI\\visual-question-answering.html}
}

@inproceedings{parkhiDeepFaceRecognition2015,
  title = {Deep {{Face Recognition}}},
  booktitle = {Procedings of the {{British Machine Vision Conference}} 2015},
  author = {Parkhi, Omkar M. and Vedaldi, Andrea and Zisserman, Andrew},
  year = {2015},
  pages = {41.1-41.12},
  publisher = {{British Machine Vision Association}},
  address = {{Swansea}},
  doi = {10.5244/C.29.41},
  urldate = {2023-04-06},
  abstract = {The goal of this paper is face recognition \textendash{} from either a single photograph or from a set of faces tracked in a video. Recent progress in this area has been due to two factors: (i) end to end learning for the task using a convolutional neural network (CNN), and (ii) the availability of very large scale training datasets.},
  isbn = {978-1-901725-53-7},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\56IDNFBJ\\Parkhi et al. - 2015 - Deep Face Recognition.pdf}
}

@misc{parkJointBERT2022,
  title = {{{JointBERT}}},
  author = {Park, Jangwon},
  year = {2022},
  month = nov,
  urldate = {2022-11-28},
  abstract = {Pytorch implementation of JointBERT: "BERT for Joint Intent Classification and Slot Filling"},
  copyright = {Apache-2.0},
  keywords = {bert,intent-classification,joint-bert,pytorch,slot-filling,slu,transformers}
}

@inproceedings{patadiaReviewVQADatasets2021,
  title = {Review of {{VQA}} : {{Datasets}} and {{Approaches}}},
  shorttitle = {Review of {{VQA}}},
  booktitle = {2021 {{International Conference}} on {{Advances}} in {{Computing}}, {{Communication}}, and {{Control}} ({{ICAC3}})},
  author = {Patadia, Devika and Kejriwal, Shivam and Shah, Richa and Katre, Neha},
  year = {2021},
  month = dec,
  pages = {1--6},
  doi = {10.1109/ICAC353642.2021.9697283},
  abstract = {Visual Question Answering (VQA) is a fairly recent problem that has amassed much attention from two highly significant research fields by integrating natural language processing and computer vision techniques. It is a multimodal task in which an algorithm must answer questions about images. To begin, the paper will evaluate and analyze the various existing datasets for VQA tasks and explore their limitations and specialties. The paper then reviews various papers for novel approaches to the VQA problem. Finally, it explores the field's future scope and prospective possibilities.},
  keywords = {Artificial Intelligence,Computational modeling,Computer vision,Computer Vision,Knowledge based systems,Learning (artificial intelligence),Natural Language Processing,Process control,Production,Visual Question Answering,Visualization},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\UVQQES93\\9697283.html}
}

@article{paulEfficientGroundingAbstract2016,
  title = {Efficient {{Grounding}} of {{Abstract Spatial Concepts}} for {{Natural Language Interaction}} with {{Robot Manipulators}}},
  author = {Paul, Rohan and Arkin, Jacob and Roy, Nicholas and M. Howard, Thomas},
  year = {2016},
  journal = {Other repository},
  publisher = {{Robotics: Science and Systems Foundation}},
  urldate = {2022-02-21},
  abstract = {Our goal is to develop models that allow a robot to understand natural language instructions in the context of its world representation. Contemporary models learn possible correspondences between parsed instructions and candidate groundings that include objects, regions and motion constraints. However, these models cannot reason about abstract concepts expressed in an instruction like, ``pick up the middle block in the row of five blocks''. In this work, we introduce a probabilistic model that incorporates an expressive space of abstract spatial concepts as well as notions of cardinality and ordinality. The graph is structured according to the parse structure of language  and introduces a factorisation over abstract concepts correlated with concrete constituents. Inference in the model is posed as an approximate search procedure that leverages partitioning of the joint in terms of concrete and abstract factors. The algorithm  first estimates a set of probable concrete constituents that constrains the search procedure to a reduced space of abstract concepts, pruning away improbable portions of the exponentiallylarge search space. Empirical evaluation demonstrates accurate grounding of abstract concepts embedded in complex natural language instructions commanding a robot manipulator. The proposed inference method leads to significant efficiency gains compared to the baseline, with minimal trade-off in accuracy.},
  copyright = {Creative Commons Attribution-Noncommercial-Share Alike},
  isbn = {9780992374723},
  langid = {english},
  annotation = {Accepted: 2018-06-19T19:45:35Z},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\T6QJFCHB\\Paul et al. - 2016 - Efficient Grounding of Abstract Spatial Concepts f.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\GRJZLMZW\\116438.html}
}

@article{paulEfficientGroundingAbstract2018,
  title = {Efficient Grounding of Abstract Spatial Concepts for Natural Language Interaction with Robot Platforms},
  author = {Paul, Rohan and Arkin, Jacob and Aksaray, Derya and Roy, Nicholas and Howard, Thomas M.},
  year = {2018},
  month = sep,
  journal = {The International Journal of Robotics Research},
  volume = {37},
  number = {10},
  pages = {1269--1299},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0278-3649},
  doi = {10.1177/0278364918777627},
  urldate = {2022-02-21},
  abstract = {Our goal is to develop models that allow a robot to efficiently understand or ``ground'' natural language instructions in the context of its world representation. Contemporary approaches estimate correspondences between language instructions and possible groundings such as objects, regions, and goals for actions that the robot should execute. However, these approaches typically reason in relatively small domains and do not model abstract spatial concepts such as as ``rows,'' ``columns,'' or ``groups'' of objects and, hence, are unable to interpret an instruction such as ``pick up the middle block in the row of five blocks.'' In this paper, we introduce two new models for efficient natural language understanding of robot instructions. The first model, which we call the adaptive distributed correspondence graph (ADCG), is a probabilistic model for interpreting abstract concepts that require hierarchical reasoning over constituent concrete entities as well as notions of cardinality and ordinality. Abstract grounding variables form a Markov boundary over concrete groundings, effectively de-correlating them from the remaining variables in the graph. This structure reduces the complexity of model training and inference. Inference in the model is posed as an approximate search procedure that orders factor computation such that the estimated probable concrete groundings focus the search for abstract concepts towards likely hypothesis, pruning away improbable portions of the exponentially large space of abstractions. Further, we address the issue of scalability to complex domains and introduce a hierarchical extension to a second model termed the hierarchical adaptive distributed correspondence graph (HADCG). The model utilizes the abstractions in the ADCG but infers a coarse symbolic structure from the utterance and the environment model and then performs fine-grained inference over the reduced graphical model, further improving the efficiency of inference. Empirical evaluation demonstrates accurate grounding of abstract concepts embedded in complex natural language instructions commanding a robotic torso and a mobile robot. Further, the proposed approximate inference method allows significant efficiency gains compared with the baseline, with minimal trade-off in accuracy.},
  langid = {english},
  keywords = {abstract spatial concepts,Human-Robot interaction,language grounding,robot learning}
}

@misc{pavlovAnswerGPT2Hardware2021,
  title = {Answer to "{{GPT-2}}: ({{Hardware}}) Requirements for Fine-Tuning the {{774M}} Model"},
  shorttitle = {Answer to "{{GPT-2}}},
  author = {Pavlov, Dan},
  year = {2021},
  month = apr,
  journal = {Artificial Intelligence Stack Exchange},
  urldate = {2022-09-02},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\UHXAT8YW\\gpt-2-hardware-requirements-for-fine-tuning-the-774m-model.html}
}

@article{PDFALICEChatbot2021,
  title = {({{PDF}}) {{ALICE}} Chatbot: {{Trials}} and Outputs},
  shorttitle = {({{PDF}}) {{ALICE}} Chatbot},
  year = {2021},
  month = sep,
  journal = {ResearchGate},
  doi = {10.13053/cys-19-4-2326},
  urldate = {2021-09-03},
  abstract = {PDF | A chatbot is a conversational agent that interacts with users using natural language. Multi chatbots are available to serve in different domains.... | Find, read and cite all the research you need on ResearchGate},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\G58X3VHY\\2021 - (PDF) ALICE chatbot Trials and outputs.pdf}
}

@article{pentlandFaceRecognitionUsing,
  title = {Face {{Recognition Using Eigenfaces}}},
  author = {Pentland, Alex P},
  abstract = {We present an approach t o the detection and identification of human faces and describe a working, near-real-time face recognition system which tracks a subject's head and then recognizes the person by comparing characteristics of the face t o those of known individuals. Our approach treats face recognition as a two-dimensional recognition problem, taking advantage of the fact that faces are are normally upright and thus may be described by a small set of 2-D characteristic views. Face images are projected onto a feature space (``face space'') that best encodes the variation among known face images. T h e face space is defined by the ``eigenfaces'', which are the eigenvectors of the set of faces; they do not necessarily correspond to isolated features such as eyes, ears, and noses. T h e framework provides the ability to learn t o recognize new faces in an unsupervised manner.},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\RQS97GXX\\Pentland - Face Recognition Using Eigenfaces.pdf}
}

@misc{PeopleLensUsingAI2022,
  title = {{{PeopleLens}}: {{Using AI}} to Support Social Interaction between Children Who Are Blind and Their Peers},
  shorttitle = {{{PeopleLens}}},
  year = {2022},
  month = mar,
  journal = {Microsoft Research},
  urldate = {2022-03-18},
  abstract = {For children born blind, social interaction can be particularly challenging. A child may have difficulty aiming their voice at the person they're talking to and put their head on their desk instead. Linguistically advanced young people may struggle with maintaining a topic of conversation, talking only about something of interest to them. Most noticeably, many [\ldots ]},
  langid = {american},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\FP2J3YHK\\peoplelens-using-ai-to-support-social-interaction-between-children-who-are-blind-and-their-peer.html}
}

@misc{perronWillChatbotsEver2022,
  title = {Will Chatbots Ever Live up to the Hype?},
  author = {Perron, Sylvain},
  year = {2022},
  month = apr,
  journal = {Stack Overflow Blog},
  urldate = {2022-04-15},
  abstract = {Chatbots held so much promise as a software tool. But near-future NLP advances will bring them more in line with their hype.},
  howpublished = {https://stackoverflow.blog/2022/04/13/will-chatbots-ever-live-up-to-the-hype/},
  langid = {american}
}

@misc{petersDeepContextualizedWord2018,
  title = {Deep Contextualized Word Representations},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  month = mar,
  number = {arXiv:1802.05365},
  eprint = {1802.05365},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-09-02},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: NAACL 2018. Originally posted to openreview 27 Oct 2017. v2 updated for NAACL camera ready},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\IDPJ8VY2\\Peters et al. - 2018 - Deep contextualized word representations.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\M8FJPSCS\\1802.html}
}

@misc{ph.dAdvancingBERTBigBird2021,
  title = {Advancing over {{BERT}} ? {{BigBird}}, {{ConvBERT}}, {{DynaBERT}} ...},
  shorttitle = {Advancing over {{BERT}} ?},
  author = {Ph.D, Suleiman Khan},
  year = {2021},
  month = oct,
  journal = {Medium},
  urldate = {2021-10-17},
  abstract = {BERT and its variants have stormed the NLP landscape in 2021 as well. There were over 20 articles advancing BERT or Transformers in NeurIPS2020 and ICLR2020 each and the trend is continuing! In this\ldots},
  howpublished = {https://towardsdatascience.com/advancing-over-bert-bigbird-convbert-dynabert-bca78a45629c},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\LDXCJA5B\\advancing-over-bert-bigbird-convbert-dynabert-bca78a45629c.html}
}

@misc{ph.dBERTRoBERTaDistilBERT2021,
  title = {{{BERT}}, {{RoBERTa}}, {{DistilBERT}}, {{XLNet}} \textemdash{} Which One to Use?},
  author = {Ph.D, Suleiman Khan},
  year = {2021},
  month = may,
  journal = {Medium},
  urldate = {2021-10-17},
  abstract = {Google's BERT and recent transformer-based methods have taken the NLP landscape by a storm, outperforming the state-of-the-art on several\ldots},
  howpublished = {https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\VBDY5YI8\\bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8.html}
}

@misc{PNYNVIDIAA100,
  title = {{{PNY NVIDIA A100}} 80 {{GB}} - {{Carte}} Graphique pro {{PNY}} Sur {{LDLC}}},
  urldate = {2023-03-31},
  howpublished = {https://www.ldlc.com/fr-be/fiche/PB00462651.html}
}

@misc{Preface,
  title = {Preface},
  urldate = {2022-01-13},
  howpublished = {https://www.nltk.org/book/ch00.html},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\DIMKKL4N\\ch00.html}
}

@misc{pyohioNaturalLanguageProcessing2018,
  title = {Natural {{Language Processing}} in {{Python}}},
  author = {{PyOhio}},
  year = {2018},
  urldate = {2021-09-15},
  abstract = {Alice Zhao https://pyohio.org/2018/schedule/pres... Natural language processing (NLP) is an exciting branch of artificial intelligence (AI) that allows machines to break down and understand human language. As a data scientist, I often use NLP techniques to interpret text data that I'm working with for my analysis. During this tutorial, I plan to walk through text pre-processing techniques, machine learning techniques and Python libraries for NLP.    Text pre-processing techniques include tokenization, text normalization and data cleaning. Once in a standard format, various machine learning techniques can be applied to better understand the data. This includes using popular modeling techniques to classify emails as spam or not, or to score the sentiment of a tweet on Twitter. Newer, more complex techniques can also be used such as topic modeling, word embeddings or text generation with deep learning.    We will walk through an example in Jupyter Notebook that goes through all of the steps of a text analysis project, using several NLP libraries in Python including NLTK, TextBlob, spaCy and gensim along with the standard machine learning libraries including pandas and scikit-learn.    \#\# Setup Instructions  [ https://github.com/adashofdata/nlp-in...](https://github.com/adashofdata/nlp-in...) ===  https://pyohio.org    A FREE annual conference for anyone interested in Python in and around Ohio, the entire Midwest, maybe even the whole world.}
}

@article{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = feb,
  journal = {arXiv:2103.00020 [cs]},
  eprint = {2103.00020},
  primaryclass = {cs},
  urldate = {2022-04-01},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\HD4F694H\\Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\DPI7RPP5\\2103.html}
}

@misc{raffelExploringLimitsTransfer2020,
  title = {Exploring the {{Limits}} of {{Transfer Learning}} with a {{Unified Text-to-Text Transformer}}},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  year = {2020},
  month = jul,
  number = {arXiv:1910.10683},
  eprint = {1910.10683},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-09-02},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Final version as published in JMLR},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\I66CEDR6\\Raffel et al. - 2020 - Exploring the Limits of Transfer Learning with a U.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\SM4S8Q5F\\1910.html}
}

@misc{raffelExploringLimitsTransfer2020a,
  title = {Exploring the {{Limits}} of {{Transfer Learning}} with a {{Unified Text-to-Text Transformer}}},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  year = {2020},
  month = jul,
  number = {arXiv:1910.10683},
  eprint = {1910.10683},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-09-02},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Final version as published in JMLR},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\UALC4P86\\Raffel et al. - 2020 - Exploring the Limits of Transfer Learning with a U.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\YTZ84SAS\\1910.html}
}

@article{ranftlVisionTransformersDense2021,
  title = {Vision {{Transformers}} for {{Dense Prediction}}},
  author = {Ranftl, Ren{\'e} and Bochkovskiy, Alexey and Koltun, Vladlen},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.13413 [cs]},
  eprint = {2103.13413},
  primaryclass = {cs},
  urldate = {2022-04-01},
  abstract = {We introduce dense vision transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense vision transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28\% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense vision transformers set a new state of the art on ADE20K with 49.02\% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. Our models are available at https://github.com/intel-isl/DPT.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 15 pages},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\RNDABLMD\\Ranftl et al. - 2021 - Vision Transformers for Dense Prediction.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\IV8N3CP5\\2103.html}
}

@misc{reedGeneralistAgent2022,
  title = {A {{Generalist Agent}}},
  author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and {Barth-Maron}, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and {de Freitas}, Nando},
  year = {2022},
  month = may,
  number = {arXiv:2205.06175},
  eprint = {2205.06175},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-09-01},
  abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\8ERNRWLR\\Reed et al. - 2022 - A Generalist Agent.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\C7A9IT7W\\2205.html}
}

@misc{RobustRealtimeObject,
  title = {Robust {{Real-time Object Detection}}},
  urldate = {2023-04-06},
  howpublished = {https://www.hpl.hp.com/techreports/Compaq-DEC/CRL-2001-1.html},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\YQ9ZV8EC\\CRL-2001-1.html}
}

@article{rocaMicroserviceChatbotArchitecture2020,
  title = {Microservice Chatbot Architecture for Chronic Patient Support},
  author = {Roca, Surya and Sancho, Jorge and Garc{\'i}a, Jos{\'e} and Alesanco, {\'A}lvaro},
  year = {2020},
  month = feb,
  journal = {Journal of Biomedical Informatics},
  volume = {102},
  pages = {103305},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2019.103305},
  urldate = {2021-09-02},
  abstract = {Chatbots are able to provide support to patients suffering from very different conditions. Patients with chronic diseases or comorbidities could benefit the most from chatbots which can keep track of their condition, provide specific information, encourage adherence to medication, etc. To perform these functions, chatbots need a suitable underlying software architecture. In this paper, we introduce a chatbot architecture for chronic patient support grounded on three pillars: scalability by means of microservices, standard data sharing models through HL7 FHIR and standard conversation modeling using AIML. We also propose an innovative automation mechanism to convert FHIR resources into AIML files, thus facilitating the interaction and data gathering of medical and personal information that ends up in patient health records. To align the way people interact with each other using messaging platforms with the chatbot architecture, we propose these very same channels for the chatbot-patient interaction, paying special attention to security and privacy issues. Finally, we present a monitored-data study performed in different chronic diseases, and we present a prototype implementation tailored for one specific chronic disease, psoriasis, showing how this new architecture allows the change, the addition or the improvement of different parts of the chatbot in a dynamic and flexible way, providing a substantial improvement in the development of chatbots used as virtual assistants for chronic patients.},
  langid = {english},
  keywords = {Artificial Intelligence Markup Language (AIML),Chronic patient support,Fast Healthcare Interoperability Resources (FHIR),Medical chatbot,Messaging platforms,Microservice architecture},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\Q42NZFHI\\Roca et al. - 2020 - Microservice chatbot architecture for chronic pati.pdf}
}

@article{rudinStopExplainingBlack2019,
  title = {Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead},
  author = {Rudin, Cynthia},
  year = {2019},
  month = may,
  journal = {Nature Machine Intelligence},
  volume = {1},
  number = {5},
  pages = {206--215},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0048-x},
  urldate = {2023-05-27},
  abstract = {Black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. The way forward is to design models that are inherently interpretable. This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision.},
  copyright = {2019 Springer Nature Limited},
  langid = {english},
  keywords = {Computer science,Criminology,Science,Statistics,technology and society},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\9LREPULA\\Rudin - 2019 - Stop explaining black box machine learning models .pdf}
}

@misc{runningUsingDeepSpeedMegatron2021,
  title = {Using {{DeepSpeed}} and {{Megatron}} to {{Train Megatron-Turing NLG 530B}}, the {{World}}'s {{Largest}} and {{Most Powerful Generative Language Model}}},
  author = {Running, Jeff},
  year = {2021},
  month = oct,
  journal = {Microsoft Research},
  urldate = {2022-09-01},
  abstract = {We are excited to introduce the DeepSpeed- and Megatron-powered Megatron-Turing Natural Language Generation model (MT-NLG), the largest and the most powerful monolithic transformer language model trained to date, with 530 billion parameters. It is the result of a research collaboration between Microsoft and NVIDIA to further parallelize and optimize the training of very large AI [\ldots ]},
  langid = {american},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\FCM6ZJGX\\using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powe.html}
}

@misc{sanhBestMostCurrent2020,
  title = {🌻 {{The Best}} and {{Most Current}} of {{Modern Natural Language Processing}}},
  author = {Sanh, Victor},
  year = {2020},
  month = aug,
  journal = {HuggingFace},
  urldate = {2022-01-25},
  abstract = {Which papers can I read to catch up with the latest trends in modern Natural Language Processing?},
  langid = {english}
}

@inproceedings{schroffFaceNetUnifiedEmbedding2015,
  title = {{{FaceNet}}: {{A Unified Embedding}} for {{Face Recognition}} and {{Clustering}}},
  shorttitle = {{{FaceNet}}},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  year = {2015},
  month = jun,
  eprint = {1503.03832},
  primaryclass = {cs},
  pages = {815--823},
  doi = {10.1109/CVPR.2015.7298682},
  urldate = {2023-04-06},
  abstract = {Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63\%. On YouTube Faces DB it achieves 95.12\%. Our system cuts the error rate in comparison to the best published result by 30\% on both datasets. We also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Also published, in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2015},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\FNHRF2BV\\Schroff et al. - 2015 - FaceNet A Unified Embedding for Face Recognition .pdf;C\:\\Users\\Vincent\\Zotero\\storage\\8ZABRYI7\\1503.html}
}

@misc{SeeingAIApp2022,
  title = {Seeing {{AI App}} from {{Microsoft}}},
  year = {2022},
  urldate = {2022-07-15},
  abstract = {Seeing AI app helps people with vision impairment convert visual info into audio. Download for free in English, Dutch, German, French, Japanese, and Spanish.},
  howpublished = {https://www.microsoft.com/en-us/ai/seeing-ai},
  langid = {american},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\KLRWWUDB\\seeing-ai.html}
}

@misc{sentdexDeepLearningChatbot2020,
  title = {Deep {{Learning Chatbot R}}\&{{D}}},
  author = {{sentdex}},
  year = {2020},
  month = mar,
  urldate = {2022-01-13},
  abstract = {Me toying around with the scored outputs of 20-something models, trying to figure out how to find the best answers.  Neural Networks from Scratch: https://nnfs.io Chat response samples: https://github.com/Sentdex/chatbotrnd... Channel membership: https://www.youtube.com/channel/UCfzl... Discord: https://discord.gg/sentdex Support the content: https://pythonprogramming.net/support... Twitter: https://twitter.com/sentdex Instagram: https://instagram.com/sentdex Facebook: https://www.facebook.com/pythonprogra... Twitch: https://www.twitch.tv/sentdex}
}

@misc{SequenceLearningNLP,
  title = {Sequence {{Learning}} and {{NLP}} with {{Neural Networks}}\textemdash{{Wolfram Language Documentation}}},
  urldate = {2022-11-27},
  howpublished = {https://reference.wolfram.com/language/tutorial/NeuralNetworksSequenceLearning.html\#1013067167}
}

@misc{serengilDeepface2023,
  title = {Deepface},
  author = {Serengil, Sefik Ilkin},
  year = {2023},
  month = mar,
  urldate = {2023-03-31},
  abstract = {A Lightweight Face Recognition and Facial Attribute Analysis (Age, Gender, Emotion and Race) Library for Python},
  copyright = {MIT},
  keywords = {age-prediction,arcface,deep-learning,deepface,deepid,emotion-recognition,face-analysis,face-recognition,facenet,facial-expression-recognition,facial-recognition,gender-prediction,machine-learning,openface,python,race-classification,vgg-face}
}

@misc{serengilDeepface2023a,
  title = {Deepface},
  author = {Serengil, Sefik Ilkin},
  year = {2023},
  month = apr,
  urldate = {2023-04-06},
  abstract = {A Lightweight Face Recognition and Facial Attribute Analysis (Age, Gender, Emotion and Race) Library for Python},
  copyright = {MIT},
  keywords = {age-prediction,arcface,deep-learning,deepface,deepid,emotion-recognition,face-analysis,face-recognition,facenet,facial-expression-recognition,facial-recognition,gender-prediction,machine-learning,openface,python,race-classification,vgg-face}
}

@misc{serengilDeepFaceDetection2022,
  title = {Deep {{Face Detection}} with {{Mediapipe}}},
  author = {Serengil, Sefik},
  year = {2022},
  month = jan,
  journal = {Sefik Ilkin Serengil},
  urldate = {2023-04-06},
  abstract = {Mediapipe is a Google powered ML solution. In this post, we'll use mediapipe for both face detection and facial landmark detection.},
  langid = {american},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\EQGE3FM7\\deep-face-detection-with-mediapipe.html}
}

@misc{serengilFacialExpressionRecognition2018,
  title = {Facial {{Expression Recognition}} with {{Keras}}},
  author = {Serengil, Sefik},
  year = {2018},
  month = jan,
  journal = {Sefik Ilkin Serengil},
  urldate = {2023-04-06},
  abstract = {Kaggle announced facial expression recognition challenge in 2013.~Researchers are expected to create models to detect 7 different emotions from human being faces. However, recent studies are far away from the excellent results even today.~That's why, this topic is still satisfying subject.},
  langid = {american},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\SIIZWS6Z\\facial-expression-recognition-with-keras.html}
}

@misc{ServiceFormationChiens,
  title = {Service de Formation de Chiens Guides | {{Amis}} Des {{Aveugles}}},
  urldate = {2022-07-26},
  howpublished = {https://www.amisdesaveugles.org/formation-des-chiens-guides.html},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\HPVEU4XK\\formation-des-chiens-guides.html}
}

@inproceedings{sharmaFARECCNNBased2016,
  title = {{{FAREC}} \textemdash{} {{CNN}} Based Efficient Face Recognition Technique Using {{Dlib}}},
  booktitle = {2016 {{International Conference}} on {{Advanced Communication Control}} and {{Computing Technologies}} ({{ICACCCT}})},
  author = {Sharma, S. and Shanmugasundaram, Karthikeyan and Ramasamy, Sathees Kumar},
  year = {2016},
  month = may,
  pages = {192--195},
  doi = {10.1109/ICACCCT.2016.7831628},
  abstract = {Despite of advancement in face recognition, it has received much more attention in last few decades in the field of research and in commercial markets this project proposes an efficient technique for face recognition system based on Deep Learning using Convolutional Neural Network (CNN) with Dlib face alignment. The paper describes the process involved in the face recognition like face alignment and feature extraction. The paper also emphasizes the importance of the face alignment, thus the accuracy and False Acceptance Rate (FAR) is observed by using proposed technique. The computational analysis shows the better performance than other state-of-art approaches. The work has been done on Face Recognition Grand challenge (FRGC) dataset and giving accuracy of 96\% with FAR of 0.1.},
  keywords = {Biological neural networks,CNN,Deep learning,Dlib,Face,Face detection,Face recognition,Face Recognition,Feature extraction,FRGC,Neurons},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\846GB828\\7831628.html}
}

@article{shumElizaXiaoIceChallenges2018,
  title = {From {{Eliza}} to {{XiaoIce}}: {{Challenges}} and {{Opportunities}} with {{Social Chatbots}}},
  shorttitle = {From {{Eliza}} to {{XiaoIce}}},
  author = {Shum, Heung-Yeung and He, Xiaodong and Li, Di},
  year = {2018},
  month = feb,
  journal = {arXiv:1801.01957 [cs]},
  eprint = {1801.01957},
  primaryclass = {cs},
  urldate = {2021-09-03},
  abstract = {Conversational systems have come a long way since their inception in the 1960s. After decades of research and development, we've seen progress from Eliza and Parry in the 60's and 70's, to task-completion systems as in the DARPA Communicator program in the 2000s, to intelligent personal assistants such as Siri in the 2010s, to today's social chatbots like XiaoIce. Social chatbots' appeal lies not only in their ability to respond to users' diverse requests, but also in being able to establish an emotional connection with users. The latter is done by satisfying users' need for communication, affection, as well as social belonging. To further the advancement and adoption of social chatbots, their design must focus on user engagement and take both intellectual quotient (IQ) and emotional quotient (EQ) into account. Users should want to engage with a social chatbot; as such, we define the success metric for social chatbots as conversation-turns per session (CPS). Using XiaoIce as an illustrative example, we discuss key technologies in building social chatbots from core chat to visual awareness to skills. We also show how XiaoIce can dynamically recognize emotion and engage the user throughout long conversations with appropriate interpersonal responses. As we become the first generation of humans ever living with AI, we have a responsibility to design social chatbots to be both useful and empathetic, so they will become ubiquitous and help society as a whole.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\DV6MAY8J\\Shum et al. - 2018 - From Eliza to XiaoIce Challenges and Opportunitie.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\EEGVA82J\\1801.html}
}

@inproceedings{singhVQAModelsThat2019,
  title = {Towards {{VQA Models That Can Read}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  year = {2019},
  month = jun,
  pages = {8309--8318},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00851},
  urldate = {2022-03-29},
  abstract = {Studies have shown that a dominant class of questions asked by visually impaired users on images of their surroundings involves reading text in the image. But today's VQA models can not read! Our paper takes a first step towards addressing this problem. First, we introduce a new ``TextVQA'' dataset to facilitate progress on this important problem. Existing datasets either have a small proportion of questions about text (e.g., the VQA dataset) or are too small (e.g., the VizWiz dataset). TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Second, we introduce a novel model architecture that reads text in the image, reasons about it in the context of the image and the question, and predicts an answer which might be a deduction based on the text and the image or composed of the strings found in the image. Consequently, we call our approach Look, Read, Reason \& Answer (LoRRA). We show that LoRRA outperforms existing state-of-the-art VQA models on our TextVQA dataset. We find that the gap between human performance and machine performance is significantly larger on TextVQA than on VQA 2.0, suggesting that TextVQA is well-suited to benchmark progress along directions complementary to VQA 2.0.},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\W8PPA6L4\\Singh et al. - 2019 - Towards VQA Models That Can Read.pdf}
}

@misc{sitnflashRacialDiscriminationFace2020,
  title = {Racial {{Discrimination}} in {{Face Recognition Technology}}},
  author = {SITNFlash},
  year = {2020},
  month = oct,
  journal = {Science in the News},
  urldate = {2023-04-04},
  abstract = {The application of face recognition technology in the criminal justice system threatens to perpetuate racial inequality.},
  langid = {american},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\I6AP6HZH\\racial-discrimination-in-face-recognition-technology.html}
}

@article{sladeMultimodalSensingIntuitive,
  title = {Multimodal Sensing and Intuitive Steering Assistance Improve Navigation and Mobility for People with Impaired Vision},
  author = {Slade, Patrick and Tambe, Arjun and Kochenderfer, Mykel J.},
  journal = {Science Robotics},
  volume = {6},
  number = {59},
  pages = {eabg6594},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/scirobotics.abg6594},
  urldate = {2021-10-26},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\IRQQ5GA6\\Slade et al. - Multimodal sensing and intuitive steering assistan.pdf}
}

@misc{SLSResearchInitiatives,
  title = {{{SLS}} :: {{Research Initiatives}} :: {{Home}}},
  urldate = {2023-02-07},
  howpublished = {https://groups.csail.mit.edu/sls/downloads/}
}

@misc{SpokenDialogSystem,
  title = {Spoken {{Dialog System}} - an Overview | {{ScienceDirect Topics}}},
  urldate = {2022-11-29},
  howpublished = {https://www.sciencedirect.com/topics/computer-science/spoken-dialog-system},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\A46D56MH\\spoken-dialog-system.html}
}

@misc{spwDallesPodotactilesSecurotheque,
  title = {{Dalles podotactiles - S\'ecuroth\`eque}},
  author = {SPW},
  journal = {S\'ecurotheque},
  urldate = {2022-07-26},
  howpublished = {http://securotheque.wallonie.be/cms/render/live/fr/sites/securotheque/contents/articles/E-amenagements-usagers-et-vehicules/pmr/article-3002.html},
  langid = {french},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\22QDPAW8\\les-dalles-podotactiles-pour-les-pmr.html}
}

@article{srivastavaVisualQuestionAnswering2020,
  title = {Visual {{Question Answering}} Using {{Deep Learning}}: {{A Survey}} and {{Performance Analysis}}},
  shorttitle = {Visual {{Question Answering}} Using {{Deep Learning}}},
  author = {Srivastava, Yash and Murali, Vaishnav and Dubey, Shiv Ram and Mukherjee, Snehasis},
  year = {2020},
  month = dec,
  journal = {arXiv:1909.01860 [cs]},
  eprint = {1909.01860},
  primaryclass = {cs},
  urldate = {2022-02-22},
  abstract = {The Visual Question Answering (VQA) task combines challenges for processing data with both Visual and Linguistic processing, to answer basic `common sense' questions about given images. Given an image and a question in natural language, the VQA system tries to find the correct answer to it using visual elements of the image and inference gathered from textual questions. In this survey, we cover and discuss the recent datasets released in the VQA domain dealing with various types of question-formats and robustness of the machine-learning models. Next, we discuss about new deep learning models that have shown promising results over the VQA datasets. At the end, we present and discuss some of the results computed by us over the vanilla VQA model, Stacked Attention Network and the VQA Challenge 2017 winner model. We also provide the detailed analysis along with the challenges and future research directions.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
  note = {Comment: Accepted in Fifth IAPR International Conference on Computer Vision and Image Processing (CVIP), 2020},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\NNC5BFY4\\Srivastava et al. - 2020 - Visual Question Answering using Deep Learning A S.pdf}
}

@misc{StateoftheartOpenSource,
  title = {{A state-of-the-art open source chatbot}},
  urldate = {2021-09-30},
  abstract = {Today we're announcing that Facebook AI has built and open-sourced BlenderBot, the largest-ever open-domain chatbot. It outperforms others in terms of engagement and also feels more human, according to human evaluators.},
  howpublished = {https://ai.facebook.com/blog/state-of-the-art-open-source-chatbot/},
  langid = {dutch},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\38U225G6\\state-of-the-art-open-source-chatbot.html}
}

@book{StatisticalLanguageSpeech2018,
  title = {Statistical Language and Speech Processing},
  year = {2018},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{New York, NY}},
  isbn = {978-3-030-00809-3},
  note = {Main interest are in the NLU part with NER related work beginning at page 105 and ellipsis handling at page 140.}
}

@article{sutoyoDesigningEmotionallyRealistic2019,
  title = {Designing an {{Emotionally Realistic Chatbot Framework}} to {{Enhance Its Believability}} with {{AIML}} and {{Information States}}},
  author = {Sutoyo, Rhio and Chowanda, Andry and Kurniati, Agnes and Wongso, Rini},
  year = {2019},
  month = jan,
  journal = {Procedia Computer Science},
  series = {The 4th {{International Conference}} on {{Computer Science}} and {{Computational Intelligence}} ({{ICCSCI}} 2019) : {{Enabling Collaboration}} to {{Escalate Impact}} of {{Research Results}} for {{Society}}},
  volume = {157},
  pages = {621--628},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2019.08.226},
  urldate = {2021-09-02},
  abstract = {Chatbot program has been empirically proven help to improve the engagement with users. Moreover, the implementation of a chat-bot program in the industry helps the company to reduce their operational costs in engaging with their customers and employees. There are still quite a number of problems existed in order to build a human-like chatbot program. Understanding a natural conversation and replying back to the conversation the interlocutors, keeping the conversation flowing naturally is a cumbersome task for a computer. This research aims to design an emotionally realistic chatbot system to enhance the believability of the chatbot using Artificial Intelligence Markup Language (AIML) and Information State. The results show that there is a statistically significant improvement to the chatbot believability in the system that has emotions variables induced compare to the one without emotions. Moreover, 63,33\% of the respondents perceived Aero and Iris as two different individuals. The future work of this research is to deploy and have an exploration of the chatbot system to other cases.},
  langid = {english},
  keywords = {AIML,Believability,Chatbot,Emotions Model,Information State},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\IULK2Y7N\\Sutoyo et al. - 2019 - Designing an Emotionally Realistic Chatbot Framewo.pdf}
}

@article{tangDeepResidualLearning2018,
  title = {Deep {{Residual Learning}} for {{Small-Footprint Keyword Spotting}}},
  author = {Tang, Raphael and Lin, Jimmy},
  year = {2018},
  month = sep,
  journal = {arXiv:1710.10361 [cs]},
  eprint = {1710.10361},
  primaryclass = {cs},
  urldate = {2022-02-10},
  abstract = {We explore the application of deep residual learning and dilated convolutions to the keyword spotting task, using the recently-released Google Speech Commands Dataset as our benchmark. Our best residual network (ResNet) implementation significantly outperforms Google's previous convolutional neural networks in terms of accuracy. By varying model depth and width, we can achieve compact models that also outperform previous small-footprint variants. To our knowledge, we are the first to examine these approaches for keyword spotting, and our results establish an open-source state-of-the-art reference to support the development of future speech-based interfaces.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: Published in ICASSP 2018},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\I4E3FK5S\\Tang et Lin - 2018 - Deep Residual Learning for Small-Footprint Keyword.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\CPPGB7XE\\1710.html}
}

@article{tebenkovMachineLearningAlgorithms2021,
  title = {Machine Learning Algorithms for Teaching {{AI}} Chat Bots},
  author = {Tebenkov, Evgeny and Prokhorov, Igor},
  year = {2021},
  month = jan,
  journal = {Procedia Computer Science},
  series = {2020 {{Annual International Conference}} on {{Brain-Inspired Cognitive Architectures}} for {{Artificial Intelligence}}: {{Eleventh Annual Meeting}} of the {{BICA Society}}},
  volume = {190},
  pages = {735--744},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2021.06.086},
  urldate = {2021-09-02},
  abstract = {Machine learning is a method of data analysis, which allows the analytical system to learn in the course of solving many similar problems. Machine learning is based on the idea that analytical systems can learn how to identify patterns and make decisions with minimal human involvement. The history of already completed dialogues between users is used to train chat bots for automated communication with interlocutors. There are many machine learning algorithms, and this article describes the most popular of them and their use for teaching chat bots.},
  langid = {english},
  keywords = {algorithms of learning chat bots,artificial intelligence,chat bots,learning algorithm,Machine learning,machine learning algorithm.},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\3M4KX485\\Tebenkov et Prokhorov - 2021 - Machine learning algorithms for teaching AI chat b.pdf}
}

@misc{TesseractOCR2023,
  title = {Tesseract {{OCR}}},
  year = {2023},
  month = apr,
  urldate = {2023-04-05},
  abstract = {Tesseract Open Source OCR Engine (main repository)},
  copyright = {Apache-2.0},
  howpublished = {tesseract-ocr},
  keywords = {hacktoberfest,lstm,machine-learning,ocr,ocr-engine,tesseract,tesseract-ocr}
}

@article{TestTuring2022,
  title = {{Test de Turing}},
  year = {2022},
  month = may,
  journal = {Wikip\'edia},
  urldate = {2022-09-01},
  abstract = {Le test de Turing est une proposition de test d'intelligence artificielle fond\'ee sur la facult\'e d'une machine \`a imiter la conversation humaine. D\'ecrit par Alan Turing en 1950 dans sa publication Computing Machinery and Intelligence, ce test consiste \`a mettre un humain en confrontation verbale \`a l'aveugle avec un ordinateur et un autre humain. Si la personne qui engage les conversations n'est pas capable de dire lequel de ses interlocuteurs est un ordinateur, on peut consid\'erer que le logiciel de l'ordinateur a pass\'e avec succ\`es le test. Cela sous-entend que l'ordinateur et l'humain essaieront d'avoir une apparence s\'emantique humaine.  Pour conserver la simplicit\'e et l'universalit\'e du test, la conversation est limit\'ee \`a des messages textuels entre les protagonistes.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {french},
  annotation = {Page Version ID: 193900020}
}

@misc{thibaultneveuEtatArtNLP2020,
  title = {L'\'etat de l'art Du {{NLP}} - {{Julien Chaumond}}, {{CTO Hugging Face}} - {{Podcast IA}}},
  author = {{Thibault Neveu}},
  year = {2020},
  month = jan,
  urldate = {2021-09-17},
  abstract = {Hugging Face https://huggingface.co/ Visual Behavior http://visualbehavior.ai/ Podcast IA sur l'\'etat de l'art du NLP avec Julien Chaumond, CTO Hugging Face. On parle des applications du NLP, des Transformers ainsi que du niveau d'intelligence des mod\`eles.  Cours NLP https://www.youtube.com/playlist?list... https://www.youtube.com/playlist?list... Write With Transformer : https://transformer.huggingface.co/do... Attention is all you need https://arxiv.org/abs/1706.03762 Discord de la communaut\'e : https://discord.gg/8Fyzc8V About me: Visual Behavior : http://visualbehavior.ai/ Github: https://github.com/thibo73800 Medium: https://medium.com/@thibo73800 Twitter: https://twitter.com/ThiboNeveu}
}

@misc{ToutExpliqueDalles,
  title = {{Tout s'explique : les dalles podotactiles}},
  shorttitle = {{Tout s'explique}},
  journal = {Atingo},
  urldate = {2022-07-26},
  abstract = {Tout s'explique : les dalles podotactiles Tout s'explique : les dalles podotactiles (paru dans Aires Libres n\textdegree 5 \textendash{} mai 2009 \textendash{} pdf 644 ko) Chacun de nous a d\'ej\`a crois\'e ces dalles en relief que l'on retrouve de part et d'autre des passages pi\'etons, aux bords des quais de gare, sur les arr\^ets de bus, [\ldots ]},
  howpublished = {https://atingo.be/documentation/tout-sexplique-les-dalles-podotactiles/},
  langid = {french},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\75PXYHI8\\tout-sexplique-les-dalles-podotactiles.html}
}

@misc{TransformerNovelNeural,
  title = {Transformer: {{A Novel Neural Network Architecture}} for {{Language Understanding}}},
  shorttitle = {Transformer},
  journal = {Google AI Blog},
  urldate = {2022-03-01},
  abstract = {Posted by Jakob Uszkoreit, Software Engineer, Natural Language Understanding   Neural networks, in particular recurrent neural networks  (RN...},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\NB4SC6L9\\transformer-novel-neural-network.html}
}

@misc{TroubleshootingMostCommon,
  title = {Troubleshooting the Most Common {{Assistant}} Issues - {{Google Assistant Community}}},
  urldate = {2022-07-14},
  howpublished = {https://support.google.com/assistant/thread/213297/troubleshooting-the-most-common-assistant-issues?hl=en},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\4HWYPB97\\troubleshooting-the-most-common-assistant-issues.html}
}

@inproceedings{turWhatLeftBe2010,
  title = {What Is Left to Be Understood in {{ATIS}}?},
  booktitle = {2010 {{IEEE Spoken Language Technology Workshop}}},
  author = {Tur, Gokhan and {Hakkani-T{\"u}r}, Dilek and Heck, Larry},
  year = {2010},
  month = dec,
  pages = {19--24},
  doi = {10.1109/SLT.2010.5700816},
  abstract = {One of the main data resources used in many studies over the past two decades for spoken language understanding (SLU) research in spoken dialog systems is the airline travel information system (ATIS) corpus. Two primary tasks in SLU are intent determination (ID) and slot filling (SF). Recent studies reported error rates below 5\% for both of these tasks employing discriminative machine learning techniques with the ATIS test set. While these low error rates may suggest that this task is close to being solved, further analysis reveals the continued utility of ATIS as a research corpus. In this paper, our goal is not experimenting with domain specific techniques or features which can help with the remaining SLU errors, but instead exploring methods to realize this utility via extensive error analysis. We conclude that even with such low error rates, ATIS test set still includes many unseen example categories and sequences, hence requires more data. Better yet, new annotated larger data sets from more complex tasks with realistic utterances can avoid over-tuning in terms of modeling and feature design. We believe that advancements in SLU can be achieved by having more naturally spoken data sets and employing more linguistically motivated features while preserving robustness due to speech recognition noise and variance due to natural language.},
  keywords = {ATIS,Cities and towns,discriminative training,Error analysis,Feature extraction,Semantics,Speech recognition,spoken language understanding,Training,Training data}
}

@misc{UnderstandingFacialRecognition2021,
  title = {Understanding {{Facial Recognition Algorithms}}},
  year = {2021},
  month = mar,
  journal = {RecFaces},
  urldate = {2022-04-04},
  abstract = {An overview of the most efficient facial recognition algorithms. Find out about each method's key features and recent developments in face recognition research.},
  howpublished = {https://recfaces.com/articles/facial-recognition-algorithms},
  langid = {american}
}

@misc{UniversityOxfordDiscover2021,
  title = {{University of Oxford - Discover \#OxfordAI | Facebook}},
  year = {2021},
  month = sep,
  urldate = {2021-09-02},
  abstract = {Discover how \#OxfordAI is helping to build assistive technologies for visually-impaired people, in the form of a visual chatbot... http://po.st/OxfordAI},
  langid = {dutch},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\8QIF9EXJ\\1059265057584608.html}
}

@article{v.AlgorithmInspectionChatbot2020,
  title = {Algorithm {{Inspection}} for {{Chatbot Performance Evaluation}}},
  author = {V., Vijayaraghavan and Cooper, Jack Brian and J., Rian Leevinson},
  year = {2020},
  month = jan,
  journal = {Procedia Computer Science},
  series = {Third {{International Conference}} on {{Computing}} and {{Network Communications}} ({{CoCoNet}}'19)},
  volume = {171},
  pages = {2267--2274},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2020.04.245},
  urldate = {2021-09-02},
  abstract = {With the usage of Chatbots growing at an unprecedented rate, it is imperative that they are thoroughly tested, verified and validated. This is done to ensure that Chatbots do not fail during operation. Chatbot failures are undesirable and they often occur when the bot is provided with ambiguous or illegible input. The Chatbots have to be thoroughly tested to ensure that they do not fail under any circumstance and have mechanisms to deal with such scenarios. Although various methods are in use to test Chatbots, algorithm testing is a promising solution to the problem. This involves the use of techniques such as cross-validation, grammar and parsing, verification and validation and statistical parsing. This paper aims to explore the prominent types of chatbot testing methods with detailed emphasis on algorithm testing techniques.},
  langid = {english},
  keywords = {algorithm testing,ambiguity,Chatbot testing,machine learning,NLP,validation,verification},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\3JM5YJR5\\V. et al. - 2020 - Algorithm Inspection for Chatbot Performance Evalu.pdf}
}

@misc{VariousDocumentsRelated2012,
  title = {Various Documents Related to {{Tesseract OCR}}},
  year = {2012},
  journal = {docs},
  urldate = {2023-04-07},
  abstract = {Various documents related to Tesseract OCR},
  howpublished = {https://tesseract-ocr.github.io/docs/},
  langid = {american},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\732XM6QL\\docs.html}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-01-24},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\74HLCSMK\\Vaswani et al. - 2017 - Attention is All you Need.pdf}
}

@inproceedings{velikovichSemanticLatticeProcessing2018,
  title = {Semantic {{Lattice Processing}} in {{Contextual Automatic Speech Recognition}} for {{Google Assistant}}},
  booktitle = {Interspeech 2018},
  author = {Velikovich, Leonid and Williams, Ian and Scheiner, Justin and Aleksic, Petar and Moreno, Pedro and Riley, Michael},
  year = {2018},
  month = sep,
  pages = {2222--2226},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2018-2453},
  urldate = {2022-02-07},
  abstract = {Recent interest in intelligent assistants has increased demand for Automatic Speech Recognition (ASR) systems that can utilize contextual information to adapt to the user's preferences or the current device state. For example, a user might be more likely to refer to their favorite songs when giving a ``music playing'' command or request to watch a movie starring a particular favorite actor when giving a ``movie playing'' command. Similarly, when a device is in a ``music playing'' state, a user is more likely to give volume control commands.},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\WHTMTXWM\\Velikovich et al. - 2018 - Semantic Lattice Processing in Contextual Automati.pdf}
}

@article{violaRobustRealTimeFace2004,
  title = {Robust {{Real-Time Face Detection}}},
  author = {Viola, Paul and Jones, Michael J.},
  year = {2004},
  month = may,
  journal = {International Journal of Computer Vision},
  volume = {57},
  number = {2},
  pages = {137--154},
  issn = {1573-1405},
  doi = {10.1023/B:VISI.0000013087.49260.fb},
  urldate = {2023-04-06},
  abstract = {This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the ``Integral Image'' which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a ``cascade'' which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second.},
  langid = {english},
  keywords = {boosting,face detection,human sensing},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\VJHKT8N5\\Viola et Jones - 2004 - Robust Real-Time Face Detection.pdf}
}

@article{virgilpetrescuFaceRecognitionBiometric2019,
  title = {Face {{Recognition}} as a {{Biometric Application}}},
  author = {Virgil Petrescu, Relly Victoria},
  year = {2019},
  month = jan,
  journal = {Journal of Mechatronics and Robotics},
  volume = {3},
  number = {1},
  pages = {237--257},
  issn = {2617-0345},
  doi = {10.3844/jmrsp.2019.237.257},
  urldate = {2023-04-06},
  abstract = {A facial recognition system is a technology capable of identifying or verifying a person from a digital image or a video frame from a video source. There are multiple methods in which facial recognition systems work, but in general, they work by comparing selected facial features from the given image with faces within a database. It is also described as a Biometric Artificial Intelligence-based application that can uniquely identify a person by analyzing patterns based on the person's facial textures and shape. While initially a form of computer application, it has seen wider uses in recent times on mobile platforms and in other forms of technology, such as robotics. It is typically used as access control in security systems and can be compared to other biometrics such as fingerprint or eye iris recognition systems. Although the accuracy of facial recognition system as biometric technology is lower than iris recognition and fingerprint recognition, it is widely adopted due to its contactless and noninvasive process. Recently, it has also become popular as a commercial identification and marketing tool. Other applications include advanced humancomputer interaction, video surveillance, automatic indexing of images and video database, among others. The use of facial recognition has recently become a very debatable subject and has been criticized more and more because it was considered an unethical tool used to spy on the public. The reason for such criticism is, however, largely due to the lack of information and regulation of this technology. Used proportionally and responsibly, facial recognition can and should be beneficial. It has the capacity to do much more to increase security in the future-from street crime to airport security. Armed war crime has dominated UK titles throughout the year. Recent statistics indicate that the number of people who benefited from the emergency assistance due to armed attacks increased by almost 40\% compared to the two years ago, while the number of children under 18 years of age with stab wounds is increasing by 86 \% in only four years. Face recognition has become a normal activity in many airports around the world. Many people today have a so-called biometric passport that allows them to go faster to the gate without having to be controlled. The facial recognition used in this way has significantly reduced waiting times for passport control but also has the ability to increase security in and around airports. Face recognition thus allows officers to identify an individual more quickly and accurately than the human eye. While some critics may worry about technology-related confidentiality issues, airports have shown that the use of facial recognition has improved security as well as speeding up processes such as check-in and in the future, even procedures boarding. When used correctly and proportionally, facial recognition can help protect the public and improve national security on multiple fronts. Similarly, advanced technology can recognize a person seen on CCTV security systems at the crime scene, justifying a person's stop and search. The ability to check in real time whether a person is on the list of criminal investigations adds an added advantage to the decision-making process before stopping and searching, thus lowering the probability of discrimination.},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\4FS7YBAP\\Virgil Petrescu - 2019 - Face Recognition as a Biometric Application.pdf}
}

@misc{visionforwardstechconnectOCRAppsPeople2019,
  title = {{{OCR Apps}} for {{People}} with a {{Visual Impairment Discussion}}},
  author = {{Vision Forward's Tech Connect}},
  year = {2019},
  month = dec,
  urldate = {2022-09-09}
}

@misc{VisualQuestionAnswering2022,
  title = {Visual {{Question Answering}} Using {{CLIP}}},
  year = {2022},
  month = feb,
  journal = {Ashwin's},
  urldate = {2022-03-20},
  abstract = {\#Visual Question Answering using CLIP},
  howpublished = {https://ashwinpathak20.github.io/cs7641ml/2022/02/blog-post-1/},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\XC42WN9R\\blog-post-1.html}
}

@misc{wijayaCreatingSimpleRuleBased2021,
  title = {Creating a {{Simple Rule-Based Chatbot}} with {{Python}}},
  author = {Wijaya, Cornellius Yudha},
  year = {2021},
  month = oct,
  journal = {Geek Culture},
  urldate = {2022-01-15},
  abstract = {Learn how to make the chatbot},
  langid = {english}
}

@misc{Williams,
  title = {Williams},
  urldate = {2022-02-21},
  howpublished = {https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9300/9332},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\Z84UP39S\\9332.html}
}

@misc{workshopBLOOM176BParameterOpenAccess2022,
  title = {{{BLOOM}}: {{A 176B-Parameter Open-Access Multilingual Language Model}}},
  shorttitle = {{{BLOOM}}},
  author = {Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c c}ois and Gall{\'e}, Matthias and Tow, Jonathan and Rush, Alexander M. and Biderman, Stella and Webson, Albert and Ammanamanchi, Pawan Sasanka and Wang, Thomas and Sagot, Beno{\^i}t and Muennighoff, Niklas and {del Moral}, Albert Villanova and Ruwase, Olatunji and Bawden, Rachel and Bekman, Stas and {McMillan-Major}, Angelina and Beltagy, Iz and Nguyen, Huu and Saulnier, Lucile and Tan, Samson and Suarez, Pedro Ortiz and Sanh, Victor and Lauren{\c c}on, Hugo and Jernite, Yacine and Launay, Julien and Mitchell, Margaret and Raffel, Colin and Gokaslan, Aaron and Simhi, Adi and Soroa, Aitor and Aji, Alham Fikri and Alfassy, Amit and Rogers, Anna and Nitzav, Ariel Kreisberg and Xu, Canwen and Mou, Chenghao and Emezue, Chris and Klamm, Christopher and Leong, Colin and {van Strien}, Daniel and Adelani, David Ifeoluwa and Radev, Dragomir and Ponferrada, Eduardo Gonz{\'a}lez and Levkovizh, Efrat and Kim, Ethan and Natan, Eyal Bar and De Toni, Francesco and Dupont, G{\'e}rard and Kruszewski, Germ{\'a}n and Pistilli, Giada and Elsahar, Hady and Benyamina, Hamza and Tran, Hieu and Yu, Ian and Abdulmumin, Idris and Johnson, Isaac and {Gonzalez-Dios}, Itziar and {de la Rosa}, Javier and Chim, Jenny and Dodge, Jesse and Zhu, Jian and Chang, Jonathan and Frohberg, J{\"o}rg and Tobing, Joseph and Bhattacharjee, Joydeep and Almubarak, Khalid and Chen, Kimbo and Lo, Kyle and Von Werra, Leandro and Weber, Leon and Phan, Long and {allal}, Loubna Ben and Tanguy, Ludovic and Dey, Manan and Mu{\~n}oz, Manuel Romero and Masoud, Maraim and Grandury, Mar{\'i}a and {\v S}a{\v s}ko, Mario and Huang, Max and Coavoux, Maximin and Singh, Mayank and Jiang, Mike Tian-Jian and Vu, Minh Chien and Jauhar, Mohammad A. and Ghaleb, Mustafa and Subramani, Nishant and Kassner, Nora and Khamis, Nurulaqilla and Nguyen, Olivier and Espejel, Omar and {de Gibert}, Ona and Villegas, Paulo and Henderson, Peter and Colombo, Pierre and Amuok, Priscilla and Lhoest, Quentin and Harliman, Rheza and Bommasani, Rishi and L{\'o}pez, Roberto Luis and Ribeiro, Rui and Osei, Salomey and Pyysalo, Sampo and Nagel, Sebastian and Bose, Shamik and Muhammad, Shamsuddeen Hassan and Sharma, Shanya and Longpre, Shayne and Nikpoor, Somaieh and Silberberg, Stanislav and Pai, Suhas and Zink, Sydney and Torrent, Tiago Timponi and Schick, Timo and Thrush, Tristan and Danchev, Valentin and Nikoulina, Vassilina and Laippala, Veronika and Lepercq, Violette and Prabhu, Vrinda and Alyafeai, Zaid and Talat, Zeerak and Raja, Arun and Heinzerling, Benjamin and Si, Chenglei and Ta{\c s}ar, Davut Emre and Salesky, Elizabeth and Mielke, Sabrina J. and Lee, Wilson Y. and Sharma, Abheesht and Santilli, Andrea and Chaffin, Antoine and Stiegler, Arnaud and Datta, Debajyoti and Szczechla, Eliza and Chhablani, Gunjan and Wang, Han and Pandey, Harshit and Strobelt, Hendrik and Fries, Jason Alan and Rozen, Jos and Gao, Leo and Sutawika, Lintang and Bari, M. Saiful and {Al-shaibani}, Maged S. and Manica, Matteo and Nayak, Nihal and Teehan, Ryan and Albanie, Samuel and Shen, Sheng and {Ben-David}, Srulik and Bach, Stephen H. and Kim, Taewoon and Bers, Tali and Fevry, Thibault and Neeraj, Trishala and Thakker, Urmish and Raunak, Vikas and Tang, Xiangru and Yong, Zheng-Xin and Sun, Zhiqing and Brody, Shaked and Uri, Yallow and Tojarieh, Hadar and Roberts, Adam and Chung, Hyung Won and Tae, Jaesung and Phang, Jason and Press, Ofir and Li, Conglong and Narayanan, Deepak and Bourfoune, Hatim and Casper, Jared and Rasley, Jeff and Ryabinin, Max and Mishra, Mayank and Zhang, Minjia and Shoeybi, Mohammad and Peyrounette, Myriam and Patry, Nicolas and Tazi, Nouamane and Sanseviero, Omar and {von Platen}, Patrick and Cornette, Pierre and Lavall{\'e}e, Pierre Fran{\c c}ois and Lacroix, R{\'e}mi and Rajbhandari, Samyam and Gandhi, Sanchit and Smith, Shaden and Requena, St{\'e}phane and Patil, Suraj and Dettmers, Tim and Baruwa, Ahmed and Singh, Amanpreet and Cheveleva, Anastasia and Ligozat, Anne-Laure and Subramonian, Arjun and N{\'e}v{\'e}ol, Aur{\'e}lie and Lovering, Charles and Garrette, Dan and Tunuguntla, Deepak and Reiter, Ehud and Taktasheva, Ekaterina and Voloshina, Ekaterina and Bogdanov, Eli and Winata, Genta Indra and Schoelkopf, Hailey and Kalo, Jan-Christoph and Novikova, Jekaterina and Forde, Jessica Zosa and Clive, Jordan and Kasai, Jungo and Kawamura, Ken and Hazan, Liam and Carpuat, Marine and Clinciu, Miruna and Kim, Najoung and Cheng, Newton and Serikov, Oleg and Antverg, Omer and {van der Wal}, Oskar and Zhang, Rui and Zhang, Ruochen and Gehrmann, Sebastian and Mirkin, Shachar and Pais, Shani and Shavrina, Tatiana and Scialom, Thomas and Yun, Tian and Limisiewicz, Tomasz and Rieser, Verena and Protasov, Vitaly and Mikhailov, Vladislav and Pruksachatkun, Yada and Belinkov, Yonatan and Bamberger, Zachary and Kasner, Zden{\v e}k and Rueda, Alice and Pestana, Amanda and Feizpour, Amir and Khan, Ammar and Faranak, Amy and Santos, Ana and Hevia, Anthony and Unldreaj, Antigona and Aghagol, Arash and Abdollahi, Arezoo and Tammour, Aycha and HajiHosseini, Azadeh and Behroozi, Bahareh and Ajibade, Benjamin and Saxena, Bharat and Ferrandis, Carlos Mu{\~n}oz and Contractor, Danish and Lansky, David and David, Davis and Kiela, Douwe and Nguyen, Duong A. and Tan, Edward and Baylor, Emi and Ozoani, Ezinwanne and Mirza, Fatima and Ononiwu, Frankline and Rezanejad, Habib and Jones, Hessie and Bhattacharya, Indrani and Solaiman, Irene and Sedenko, Irina and Nejadgholi, Isar and Passmore, Jesse and Seltzer, Josh and Sanz, Julio Bonis and Dutra, Livia and Samagaio, Mairon and Elbadri, Maraim and Mieskes, Margot and Gerchick, Marissa and Akinlolu, Martha and McKenna, Michael and Qiu, Mike and Ghauri, Muhammed and Burynok, Mykola and Abrar, Nafis and Rajani, Nazneen and Elkott, Nour and Fahmy, Nour and Samuel, Olanrewaju and An, Ran and Kromann, Rasmus and Hao, Ryan and Alizadeh, Samira and Shubber, Sarmad and Wang, Silas and Roy, Sourav and Viguier, Sylvain and Le, Thanh and Oyebade, Tobi and Le, Trieu and Yang, Yoyo and Nguyen, Zach and Kashyap, Abhinav Ramesh and Palasciano, Alfredo and Callahan, Alison and Shukla, Anima and {Miranda-Escalada}, Antonio and Singh, Ayush and Beilharz, Benjamin and Wang, Bo and Brito, Caio and Zhou, Chenxi and Jain, Chirag and Xu, Chuxin and Fourrier, Cl{\'e}mentine and Peri{\~n}{\'a}n, Daniel Le{\'o}n and Molano, Daniel and Yu, Dian and Manjavacas, Enrique and Barth, Fabio and Fuhrimann, Florian and Altay, Gabriel and Bayrak, Giyaseddin and Burns, Gully and Vrabec, Helena U. and Bello, Imane and Dash, Ishani and Kang, Jihyun and Giorgi, John and Golde, Jonas and Posada, Jose David and Sivaraman, Karthik Rangasai and Bulchandani, Lokesh and Liu, Lu and Shinzato, Luisa and {de Bykhovetz}, Madeleine Hahn and Takeuchi, Maiko and P{\`a}mies, Marc and Castillo, Maria A. and Nezhurina, Marianna and S{\"a}nger, Mario and Samwald, Matthias and Cullan, Michael and Weinberg, Michael and De Wolf, Michiel and Mihaljcic, Mina and Liu, Minna and Freidank, Moritz and Kang, Myungsun and Seelam, Natasha and Dahlberg, Nathan and Broad, Nicholas Michio and Muellner, Nikolaus and Fung, Pascale and Haller, Patrick and Chandrasekhar, Ramya and Eisenberg, Renata and Martin, Robert and Canalli, Rodrigo and Su, Rosaline and Su, Ruisi and Cahyawijaya, Samuel and Garda, Samuele and Deshmukh, Shlok S. and Mishra, Shubhanshu and Kiblawi, Sid and Ott, Simon and {Sang-aroonsiri}, Sinee and Kumar, Srishti and Schweter, Stefan and Bharati, Sushil and Laud, Tanmay and Gigant, Th{\'e}o and Kainuma, Tomoya and Kusa, Wojciech and Labrak, Yanis and Bajaj, Yash Shailesh and Venkatraman, Yash and Xu, Yifan and Xu, Yingxin and Xu, Yu and Tan, Zhe and Xie, Zhongli and Ye, Zifan and Bras, Mathilde and Belkada, Younes and Wolf, Thomas},
  year = {2022},
  month = dec,
  number = {arXiv:2211.05100},
  eprint = {2211.05100},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-01-19},
  abstract = {Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\BGLIBDLH\\Workshop et al. - 2022 - BLOOM A 176B-Parameter Open-Access Multilingual L.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\QL7L5GVT\\2211.html}
}

@misc{workshopBLOOM176BParameterOpenAccess2023,
  title = {{{BLOOM}}: {{A 176B-Parameter Open-Access Multilingual Language Model}}},
  shorttitle = {{{BLOOM}}},
  author = {Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c c}ois and Gall{\'e}, Matthias and Tow, Jonathan and Rush, Alexander M. and Biderman, Stella and Webson, Albert and Ammanamanchi, Pawan Sasanka and Wang, Thomas and Sagot, Beno{\^i}t and Muennighoff, Niklas and {del Moral}, Albert Villanova and Ruwase, Olatunji and Bawden, Rachel and Bekman, Stas and {McMillan-Major}, Angelina and Beltagy, Iz and Nguyen, Huu and Saulnier, Lucile and Tan, Samson and Suarez, Pedro Ortiz and Sanh, Victor and Lauren{\c c}on, Hugo and Jernite, Yacine and Launay, Julien and Mitchell, Margaret and Raffel, Colin and Gokaslan, Aaron and Simhi, Adi and Soroa, Aitor and Aji, Alham Fikri and Alfassy, Amit and Rogers, Anna and Nitzav, Ariel Kreisberg and Xu, Canwen and Mou, Chenghao and Emezue, Chris and Klamm, Christopher and Leong, Colin and {van Strien}, Daniel and Adelani, David Ifeoluwa and Radev, Dragomir and Ponferrada, Eduardo Gonz{\'a}lez and Levkovizh, Efrat and Kim, Ethan and Natan, Eyal Bar and De Toni, Francesco and Dupont, G{\'e}rard and Kruszewski, Germ{\'a}n and Pistilli, Giada and Elsahar, Hady and Benyamina, Hamza and Tran, Hieu and Yu, Ian and Abdulmumin, Idris and Johnson, Isaac and {Gonzalez-Dios}, Itziar and {de la Rosa}, Javier and Chim, Jenny and Dodge, Jesse and Zhu, Jian and Chang, Jonathan and Frohberg, J{\"o}rg and Tobing, Joseph and Bhattacharjee, Joydeep and Almubarak, Khalid and Chen, Kimbo and Lo, Kyle and Von Werra, Leandro and Weber, Leon and Phan, Long and {allal}, Loubna Ben and Tanguy, Ludovic and Dey, Manan and Mu{\~n}oz, Manuel Romero and Masoud, Maraim and Grandury, Mar{\'i}a and {\v S}a{\v s}ko, Mario and Huang, Max and Coavoux, Maximin and Singh, Mayank and Jiang, Mike Tian-Jian and Vu, Minh Chien and Jauhar, Mohammad A. and Ghaleb, Mustafa and Subramani, Nishant and Kassner, Nora and Khamis, Nurulaqilla and Nguyen, Olivier and Espejel, Omar and {de Gibert}, Ona and Villegas, Paulo and Henderson, Peter and Colombo, Pierre and Amuok, Priscilla and Lhoest, Quentin and Harliman, Rheza and Bommasani, Rishi and L{\'o}pez, Roberto Luis and Ribeiro, Rui and Osei, Salomey and Pyysalo, Sampo and Nagel, Sebastian and Bose, Shamik and Muhammad, Shamsuddeen Hassan and Sharma, Shanya and Longpre, Shayne and Nikpoor, Somaieh and Silberberg, Stanislav and Pai, Suhas and Zink, Sydney and Torrent, Tiago Timponi and Schick, Timo and Thrush, Tristan and Danchev, Valentin and Nikoulina, Vassilina and Laippala, Veronika and Lepercq, Violette and Prabhu, Vrinda and Alyafeai, Zaid and Talat, Zeerak and Raja, Arun and Heinzerling, Benjamin and Si, Chenglei and Ta{\c s}ar, Davut Emre and Salesky, Elizabeth and Mielke, Sabrina J. and Lee, Wilson Y. and Sharma, Abheesht and Santilli, Andrea and Chaffin, Antoine and Stiegler, Arnaud and Datta, Debajyoti and Szczechla, Eliza and Chhablani, Gunjan and Wang, Han and Pandey, Harshit and Strobelt, Hendrik and Fries, Jason Alan and Rozen, Jos and Gao, Leo and Sutawika, Lintang and Bari, M. Saiful and {Al-shaibani}, Maged S. and Manica, Matteo and Nayak, Nihal and Teehan, Ryan and Albanie, Samuel and Shen, Sheng and {Ben-David}, Srulik and Bach, Stephen H. and Kim, Taewoon and Bers, Tali and Fevry, Thibault and Neeraj, Trishala and Thakker, Urmish and Raunak, Vikas and Tang, Xiangru and Yong, Zheng-Xin and Sun, Zhiqing and Brody, Shaked and Uri, Yallow and Tojarieh, Hadar and Roberts, Adam and Chung, Hyung Won and Tae, Jaesung and Phang, Jason and Press, Ofir and Li, Conglong and Narayanan, Deepak and Bourfoune, Hatim and Casper, Jared and Rasley, Jeff and Ryabinin, Max and Mishra, Mayank and Zhang, Minjia and Shoeybi, Mohammad and Peyrounette, Myriam and Patry, Nicolas and Tazi, Nouamane and Sanseviero, Omar and {von Platen}, Patrick and Cornette, Pierre and Lavall{\'e}e, Pierre Fran{\c c}ois and Lacroix, R{\'e}mi and Rajbhandari, Samyam and Gandhi, Sanchit and Smith, Shaden and Requena, St{\'e}phane and Patil, Suraj and Dettmers, Tim and Baruwa, Ahmed and Singh, Amanpreet and Cheveleva, Anastasia and Ligozat, Anne-Laure and Subramonian, Arjun and N{\'e}v{\'e}ol, Aur{\'e}lie and Lovering, Charles and Garrette, Dan and Tunuguntla, Deepak and Reiter, Ehud and Taktasheva, Ekaterina and Voloshina, Ekaterina and Bogdanov, Eli and Winata, Genta Indra and Schoelkopf, Hailey and Kalo, Jan-Christoph and Novikova, Jekaterina and Forde, Jessica Zosa and Clive, Jordan and Kasai, Jungo and Kawamura, Ken and Hazan, Liam and Carpuat, Marine and Clinciu, Miruna and Kim, Najoung and Cheng, Newton and Serikov, Oleg and Antverg, Omer and {van der Wal}, Oskar and Zhang, Rui and Zhang, Ruochen and Gehrmann, Sebastian and Mirkin, Shachar and Pais, Shani and Shavrina, Tatiana and Scialom, Thomas and Yun, Tian and Limisiewicz, Tomasz and Rieser, Verena and Protasov, Vitaly and Mikhailov, Vladislav and Pruksachatkun, Yada and Belinkov, Yonatan and Bamberger, Zachary and Kasner, Zden{\v e}k and Rueda, Alice and Pestana, Amanda and Feizpour, Amir and Khan, Ammar and Faranak, Amy and Santos, Ana and Hevia, Anthony and Unldreaj, Antigona and Aghagol, Arash and Abdollahi, Arezoo and Tammour, Aycha and HajiHosseini, Azadeh and Behroozi, Bahareh and Ajibade, Benjamin and Saxena, Bharat and Ferrandis, Carlos Mu{\~n}oz and Contractor, Danish and Lansky, David and David, Davis and Kiela, Douwe and Nguyen, Duong A. and Tan, Edward and Baylor, Emi and Ozoani, Ezinwanne and Mirza, Fatima and Ononiwu, Frankline and Rezanejad, Habib and Jones, Hessie and Bhattacharya, Indrani and Solaiman, Irene and Sedenko, Irina and Nejadgholi, Isar and Passmore, Jesse and Seltzer, Josh and Sanz, Julio Bonis and Dutra, Livia and Samagaio, Mairon and Elbadri, Maraim and Mieskes, Margot and Gerchick, Marissa and Akinlolu, Martha and McKenna, Michael and Qiu, Mike and Ghauri, Muhammed and Burynok, Mykola and Abrar, Nafis and Rajani, Nazneen and Elkott, Nour and Fahmy, Nour and Samuel, Olanrewaju and An, Ran and Kromann, Rasmus and Hao, Ryan and Alizadeh, Samira and Shubber, Sarmad and Wang, Silas and Roy, Sourav and Viguier, Sylvain and Le, Thanh and Oyebade, Tobi and Le, Trieu and Yang, Yoyo and Nguyen, Zach and Kashyap, Abhinav Ramesh and Palasciano, Alfredo and Callahan, Alison and Shukla, Anima and {Miranda-Escalada}, Antonio and Singh, Ayush and Beilharz, Benjamin and Wang, Bo and Brito, Caio and Zhou, Chenxi and Jain, Chirag and Xu, Chuxin and Fourrier, Cl{\'e}mentine and Peri{\~n}{\'a}n, Daniel Le{\'o}n and Molano, Daniel and Yu, Dian and Manjavacas, Enrique and Barth, Fabio and Fuhrimann, Florian and Altay, Gabriel and Bayrak, Giyaseddin and Burns, Gully and Vrabec, Helena U. and Bello, Imane and Dash, Ishani and Kang, Jihyun and Giorgi, John and Golde, Jonas and Posada, Jose David and Sivaraman, Karthik Rangasai and Bulchandani, Lokesh and Liu, Lu and Shinzato, Luisa and {de Bykhovetz}, Madeleine Hahn and Takeuchi, Maiko and P{\`a}mies, Marc and Castillo, Maria A. and Nezhurina, Marianna and S{\"a}nger, Mario and Samwald, Matthias and Cullan, Michael and Weinberg, Michael and De Wolf, Michiel and Mihaljcic, Mina and Liu, Minna and Freidank, Moritz and Kang, Myungsun and Seelam, Natasha and Dahlberg, Nathan and Broad, Nicholas Michio and Muellner, Nikolaus and Fung, Pascale and Haller, Patrick and Chandrasekhar, Ramya and Eisenberg, Renata and Martin, Robert and Canalli, Rodrigo and Su, Rosaline and Su, Ruisi and Cahyawijaya, Samuel and Garda, Samuele and Deshmukh, Shlok S. and Mishra, Shubhanshu and Kiblawi, Sid and Ott, Simon and {Sang-aroonsiri}, Sinee and Kumar, Srishti and Schweter, Stefan and Bharati, Sushil and Laud, Tanmay and Gigant, Th{\'e}o and Kainuma, Tomoya and Kusa, Wojciech and Labrak, Yanis and Bajaj, Yash Shailesh and Venkatraman, Yash and Xu, Yifan and Xu, Yingxin and Xu, Yu and Tan, Zhe and Xie, Zhongli and Ye, Zifan and Bras, Mathilde and Belkada, Younes and Wolf, Thomas},
  year = {2023},
  month = mar,
  number = {arXiv:2211.05100},
  eprint = {2211.05100},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.05100},
  urldate = {2023-04-07},
  abstract = {Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\UQIBPRCI\\Workshop et al. - 2023 - BLOOM A 176B-Parameter Open-Access Multilingual L.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\36REHMGU\\2211.html}
}

@misc{WriteTransformer,
  title = {Write {{With Transformer}}},
  urldate = {2021-09-17},
  abstract = {See how a modern neural network auto-completes your text},
  howpublished = {https://transformer.huggingface.co},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\XDID9EHJ\\distil-gpt2.html}
}

@misc{XiaoiceChatbotMicrosoft,
  title = {{Xiaoice, le chatbot de Microsoft dot\'e d'intelligence \'emotionnelle, s\'eduit des millions d'hommes c\'elibataires en Chine, il enregistre \'egalement leurs d\'esirs et leurs \'emotions les plus intimes}},
  journal = {Developpez.com},
  urldate = {2021-10-17},
  abstract = {En 2016, Microsoft a mis en service son chatbot Tay (une application de conversation bas\'ee sur des algorithmes d'intelligence artificielle) afin de donner aux utilisateurs l'occasion d'interagir avec le bot sur divers sujets. Apr\`es son lancement, il n'a fallu que quelques heures pour que le chatbot perde la boule et se mette \`a publier toutes sortes de messages violents, racistes et offensants. L'histoire de Tay a donc tourn\'e court. Microsoft qui n'\'etait pas pr\'epar\'ee \`a un tel sc\'enario l'a mis hor...},
  howpublished = {https://www.developpez.com/actu/311325/Xiaoice-le-chatbot-de-Microsoft-dote-d-intelligence-emotionnelle-seduit-des-millions-d-hommes-celibataires-en-Chine-il-enregistre-egalement-leurs-desirs-et-leurs-emotions-les-plus-intimes/},
  langid = {french}
}

@misc{XiaoIceChatbotPour2021,
  title = {{XiaoIce : le chatbot pour rompre la solitude qui cartonne en Chine}},
  shorttitle = {{XiaoIce}},
  year = {2021},
  month = aug,
  journal = {intelligence-artificielle.com},
  urldate = {2021-10-17},
  abstract = {XiaoIce propose \`a ses millions d'utilisateurs d'interagir avec un chatbot qui tient des conversations empathiques et r\'ealistes.},
  howpublished = {https://intelligence-artificielle.com/xiaoice-chatbot-interactif/},
  langid = {french}
}

@misc{xuBaizeOpenSourceChat2023,
  title = {Baize: {{An Open-Source Chat Model}} with {{Parameter-Efficient Tuning}} on {{Self-Chat Data}}},
  shorttitle = {Baize},
  author = {Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  year = {2023},
  month = apr,
  number = {arXiv:2304.01196},
  eprint = {2304.01196},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-21},
  abstract = {Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. The Baize models and data are released for research purposes only at https://github.com/project-baize/baize. An online demo is also available at https://huggingface.co/spaces/project-baize/baize-lora-7B.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\2PYIPRTP\\Xu et al. - 2023 - Baize An Open-Source Chat Model with Parameter-Ef.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\5HX69DT5\\2304.html}
}

@article{xuLayoutLMv2MultimodalPretraining2022,
  title = {{{LayoutLMv2}}: {{Multi-modal Pre-training}} for {{Visually-Rich Document Understanding}}},
  shorttitle = {{{LayoutLMv2}}},
  author = {Xu, Yang and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Wei, Furu and Wang, Guoxin and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Che, Wanxiang and Zhang, Min and Zhou, Lidong},
  year = {2022},
  month = jan,
  journal = {arXiv:2012.14740 [cs]},
  eprint = {2012.14740},
  primaryclass = {cs},
  urldate = {2022-03-29},
  abstract = {Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 \$\textbackslash to\$ 0.8420), CORD (0.9493 \$\textbackslash to\$ 0.9601), SROIE (0.9524 \$\textbackslash to\$ 0.9781), Kleister-NDA (0.8340 \$\textbackslash to\$ 0.8520), RVL-CDIP (0.9443 \$\textbackslash to\$ 0.9564), and DocVQA (0.7295 \$\textbackslash to\$ 0.8672). We made our model and code publicly available at \textbackslash url\{https://aka.ms/layoutlmv2\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: ACL 2021 main conference},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\X2EJ2GG7\\Xu et al. - 2022 - LayoutLMv2 Multi-modal Pre-training for Visually-.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\MMIRPDX4\\2012.html}
}

@article{xuLayoutLMv2MultimodalPretraining2022a,
  title = {{{LayoutLMv2}}: {{Multi-modal Pre-training}} for {{Visually-Rich Document Understanding}}},
  shorttitle = {{{LayoutLMv2}}},
  author = {Xu, Yang and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Wei, Furu and Wang, Guoxin and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Che, Wanxiang and Zhang, Min and Zhou, Lidong},
  year = {2022},
  month = jan,
  journal = {arXiv:2012.14740 [cs]},
  eprint = {2012.14740},
  primaryclass = {cs},
  urldate = {2022-04-01},
  abstract = {Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 \$\textbackslash to\$ 0.8420), CORD (0.9493 \$\textbackslash to\$ 0.9601), SROIE (0.9524 \$\textbackslash to\$ 0.9781), Kleister-NDA (0.8340 \$\textbackslash to\$ 0.8520), RVL-CDIP (0.9443 \$\textbackslash to\$ 0.9564), and DocVQA (0.7295 \$\textbackslash to\$ 0.8672). We made our model and code publicly available at \textbackslash url\{https://aka.ms/layoutlmv2\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: ACL 2021 main conference},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\X7N2F46V\\Xu et al. - 2022 - LayoutLMv2 Multi-modal Pre-training for Visually-.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\SMQTYZWR\\2012.html}
}

@misc{yangHarnessingPowerLLMs2023,
  title = {Harnessing the {{Power}} of {{LLMs}} in {{Practice}}: {{A Survey}} on {{ChatGPT}} and {{Beyond}}},
  shorttitle = {Harnessing the {{Power}} of {{LLMs}} in {{Practice}}},
  author = {Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Xiaotian and Feng, Qizhang and Jiang, Haoming and Yin, Bing and Hu, Xia},
  year = {2023},
  month = apr,
  number = {arXiv:2304.13712},
  eprint = {2304.13712},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-08},
  abstract = {This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at \textbackslash url\{https://github.com/Mooler0410/LLMsPracticalGuide\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\A2EHQ28H\\Yang et al. - 2023 - Harnessing the Power of LLMs in Practice A Survey.pdf;C\:\\Users\\Vincent\\Zotero\\storage\\JJQCKSZH\\2304.html}
}

@inproceedings{yangRealtimeFaceDetection2018,
  title = {Real-Time Face Detection Based on {{YOLO}}},
  booktitle = {2018 1st {{IEEE International Conference}} on {{Knowledge Innovation}} and {{Invention}} ({{ICKII}})},
  author = {Yang, Wang and Jiachun, Zheng},
  year = {2018},
  month = jul,
  pages = {221--224},
  doi = {10.1109/ICKII.2018.8569109},
  abstract = {As a target detection system, YOLO has a fast detection speed and is suitable for target detection in real-time environment. Compared with other similar target detection systems, it has better detection accuracy and faster detection time. This paper is based on YOLO network and applied to face detection. In this paper, YOLO target detection system is applied to face detection. Experimental results show that the face detection method based on YOLO has stronger robustness and faster detection speed. Still in a complex environment can guarantee the high detection accuracy. At the same time, the detection speed can meet real-time detection requirements.},
  keywords = {dimensional clustering,Face,face detection,Face detection,Feature extraction,Object detection,Real-time systems,target detection,Technological innovation,Training,YOLO},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\GHRQPZYZ\\8569109.html}
}

@misc{yannickilcherOpenAICLIPConnectingText2021,
  title = {{{OpenAI CLIP}}: {{ConnectingText}} and {{Images}} ({{Paper Explained}})},
  shorttitle = {{{OpenAI CLIP}}},
  author = {{Yannic Kilcher}},
  year = {2021},
  month = jan,
  urldate = {2022-03-11},
  abstract = {\#ai \#openai \#technology Paper Title: Learning Transferable Visual Models From Natural Language Supervision CLIP trains on 400 million images scraped from the web, along with text descriptions to learn a model that can connect the two modalities. The core idea is a contrastive objective combined with a large batch size. The resulting model can be turned into arbitrary zero-shot classifiers for new image \& text tasks. OUTLINE: 0:00 - Introduction 3:15 - Overview 4:40 - Connecting Images \& Text 9:00 - Building Zero-Shot Classifiers 14:40 - CLIP Contrastive Training Objective 22:25 - Encoder Choices 25:00 - Zero-Shot CLIP vs Linear ResNet-50 31:50 - Zero-Shot vs Few-Shot 35:35 - Scaling Properties 36:35 - Comparison on different tasks 37:40 - Robustness to Data Shift 44:20 - Broader Impact Section 47:00 - Conclusion \& Comments Paper: https://cdn.openai.com/papers/Learnin... Blog: https://openai.com/blog/clip/ Code: https://github.com/openai/CLIP Abstract: State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. Authors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever Links: TabNine Code Completion (Referral): http://bit.ly/tabnine-yannick YouTube: https://www.youtube.com/c/yannickilcher Twitter: https://twitter.com/ykilcher Discord: https://discord.gg/4H8xxDF BitChute: https://www.bitchute.com/channel/yann... Minds: https://www.minds.com/ykilcher Parler: https://parler.com/profile/YannicKilcher LinkedIn: https://www.linkedin.com/in/yannic-ki... BiliBili: https://space.bilibili.com/1824646584 If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https://www.subscribestar.com/yannick... Patreon: https://www.patreon.com/yannickilcher Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n}
}

@misc{YOLOv4ObjectDetector,
  title = {{{YOLO-v4 Object Detector}}},
  urldate = {2021-09-06},
  abstract = {Object detection is useful for understanding what's in an image, describing both what is in an image and where those objects are found. In\ldots},
  howpublished = {https://reckoning.dev},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\VG9YBYKL\\yolo-v4.html}
}

@article{zhangInformationFusionVisual2019,
  title = {Information Fusion in Visual Question Answering: {{A Survey}}},
  shorttitle = {Information Fusion in Visual Question Answering},
  author = {Zhang, Dongxiang and Cao, Rui and Wu, Sai},
  year = {2019},
  month = dec,
  journal = {Information Fusion},
  volume = {52},
  pages = {268--280},
  issn = {15662535},
  doi = {10.1016/j.inffus.2019.03.005},
  urldate = {2022-03-08},
  langid = {english},
  file = {C\:\\Users\\Vincent\\Zotero\\storage\\USRTZYNQ\\Zhang et al. - 2019 - Information fusion in visual question answering A.pdf}
}

@misc{zhuSlotFillingIntent2022,
  title = {Slot Filling and Intent Detection Tasks of Spoken Language Understanding},
  author = {Zhu, Su},
  year = {2022},
  month = nov,
  urldate = {2022-11-27},
  abstract = {slot filling, intent detection, joint training, ATIS \& SNIPS datasets, the Facebook's multilingual dataset, MIT corpus, E-commerce Shopping Assistant (ECSA) dataset, CoNLL2003 NER, ELMo, BERT, XLNet},
  copyright = {Apache-2.0},
  keywords = {atis-dataset,bert-bilstm-crf,crf,encoder-decoder,intent-detection,named-entity-recognition,sequence-labeling,slot-filling,snips-dataset,spoken-language,xlnet}
}

@misc{ZoomTextScreenMagnifier,
  title = {{{ZoomText Screen Magnifier}} and {{Screen Reader}}},
  journal = {zoomtext.com},
  urldate = {2022-07-26},
  abstract = {ZoomText is the world's leading magnification and screen reading software for the visually impaired.},
  howpublished = {https://www.zoomtext.com/},
  langid = {american}
}
